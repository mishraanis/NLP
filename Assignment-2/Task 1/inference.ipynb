{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(losses, embedding):\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(x=range(1, len(losses) + 1), y=losses, label='Loss')\n",
    "    plt.title(f'Loss - {embedding}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    # plt.show()\n",
    "    plt.savefig(f'Plots/loss_{embedding}.png')\n",
    "\n",
    "def plot_f1s(f1_scores, embedding):\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(x=range(1, len(f1_scores) + 1), y=f1_scores, label='F1 Score')\n",
    "    plt.title(f'F1 Score - {embedding}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    # plt.show()\n",
    "    plt.savefig(f'Plots/f1_{embedding}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'Part 2/Non Trainable Embeddings'\n",
    "models = os.listdir(base_path)\n",
    "\n",
    "for model in models:\n",
    "    final_path = base_path + '/' + model\n",
    "    train_losses = pickle.load(open(final_path + '/train_loss.pkl', 'rb'))\n",
    "    val_losses = pickle.load(open(final_path + '/val_loss.pkl', 'rb'))\n",
    "    train_f1s = pickle.load(open(final_path + '/train_f1.pkl', 'rb'))\n",
    "    val_f1s = pickle.load(open(final_path + '/val_f1.pkl', 'rb'))\n",
    "\n",
    "    plot_losses(train_losses, model)\n",
    "    plot_f1s(train_f1s, model)\n",
    "    plot_losses(val_losses, model)\n",
    "    plot_f1s(val_f1s, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting BiLSTM-CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'Part 3/Non Trainable Embeddings'\n",
    "models = os.listdir(base_path)\n",
    "\n",
    "for model in models:\n",
    "    final_path = base_path + '/' + model\n",
    "    train_losses = pickle.load(open(final_path + '/train_loss.pkl', 'rb'))\n",
    "    train_losses = [float(loss.item()) for loss in train_losses]\n",
    "\n",
    "    val_losses = pickle.load(open(final_path + '/val_loss.pkl', 'rb'))\n",
    "    val_losses = [float(loss.item()) for loss in val_losses]\n",
    "\n",
    "    train_f1s = pickle.load(open(final_path + '/train_f1.pkl', 'rb'))\n",
    "    train_f1s = [float(f1.item()) for f1 in train_f1s]\n",
    "\n",
    "    val_f1s = pickle.load(open(final_path + '/val_f1.pkl', 'rb'))\n",
    "    val_f1s = [float(f1.item()) for f1 in val_f1s]\n",
    "\n",
    "    plot_losses(train_losses, f'BiLSTM CRF + {model}')\n",
    "    plot_f1s(train_f1s, model)\n",
    "    plot_losses(val_losses, model)\n",
    "    plot_f1s(val_f1s, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = json.load(open('Dataset/NER_test.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()\n",
    "\n",
    "word_to_idx = pickle.load(open('Utils/word_to_idx.pkl', 'rb'))\n",
    "tag_to_ix = pickle.load(open('Utils/tag_to_ix.pkl', 'rb'))\n",
    "\n",
    "loss_function = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, target_size, embedding_mat, start_tag, end_tag, tag_to_ix, device='cpu'):\n",
    "        super(RNN_model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_mat)).to(device)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim).to(device)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size).to(device)\n",
    "        self.start_tag = start_tag\n",
    "        self.end_tag = end_tag\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.target_size = target_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        rnn_out, _ = self.rnn(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(rnn_out.view(len(sentence), -1))\n",
    "        tag_scores = nn.functional.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "    \n",
    "class LSTM_model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, target_size, embedding_mat, start_tag, end_tag, tag_to_ix, device='cpu'):\n",
    "        super(LSTM_model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_mat)).to(device)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim).to(device)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size).to(device)\n",
    "        self.start_tag = start_tag\n",
    "        self.end_tag = end_tag\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.target_size = target_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = nn.functional.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "    \n",
    "class GRU_model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, target_size, embedding_mat, start_tag, end_tag, tag_to_ix, device='cpu'):\n",
    "        super(GRU_model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_mat)).to(device)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim).to(device)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size).to(device)\n",
    "        self.start_tag = start_tag\n",
    "        self.end_tag = end_tag\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.target_size = target_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        gru_out, _ = self.gru(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(gru_out.view(len(sentence), -1))\n",
    "        tag_scores = nn.functional.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, target_size, embedding_mat, start_tag, end_tag, tag_to_ix, batch_size=1, device='cpu'):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.target_size = target_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.start_tag = start_tag\n",
    "        self.end_tag = end_tag\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_mat, freeze=False).to(device)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True).to(device)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size).to(device)\n",
    "\n",
    "        self.transitions_to = nn.Parameter(torch.randn(target_size, target_size)).to(device)\n",
    "        self.transitions_to.data[start_tag, :] = -10000\n",
    "        self.transitions_to.data[:, end_tag] = -10000\n",
    "\n",
    "        # self.transitions_from = nn.Parameter(torch.randn(target_size, target_size))\n",
    "        # self.transitions_from.data[:, start_tag] = -10000\n",
    "        # self.transitions_from.data[end_tag, :] = -10000\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2).to(self.device),\n",
    "                torch.randn(2, 1, self.hidden_dim // 2).to(self.device))\n",
    "\n",
    "    def get_lstm_features(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.embedding(sentence).view(len(sentence), 1, -1)\n",
    "        #convert embeds to torch float32\n",
    "        embeds = embeds.float()\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "\n",
    "\n",
    "    def _forward_algo(self, lstm_features):\n",
    "\n",
    "        scores = torch.full((1, self.target_size), -10000.).to(self.device)\n",
    "        scores[0][self.start_tag] = 0.\n",
    "\n",
    "        forward_var = scores\n",
    "\n",
    "        for feat in lstm_features:\n",
    "            next_tag_var = self.transitions_to + feat.view(-1, 1).expand(-1, self.target_size) + forward_var.expand(self.target_size, -1)\n",
    "            max_score = next_tag_var.max(dim=1).values.view(-1, 1)\n",
    "            next_tag_var = next_tag_var - max_score\n",
    "            forward_var = (max_score + torch.logsumexp(next_tag_var, dim=1).view(-1, 1)).view(1, -1)\n",
    "            \n",
    "        terminal_var = forward_var + (self.transitions_to[self.end_tag]).view(1, -1)\n",
    "        alpha = terminal_var\n",
    "        max_score = alpha.max()\n",
    "        alpha = max_score + torch.logsumexp(alpha - max_score, dim=1)\n",
    "        return alpha\n",
    "    \n",
    "\n",
    "    def _score_sentence(self, lstm_features, tags):\n",
    "        score = torch.zeros(1).to(self.device)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix['START_TAG']], dtype=torch.long).to(self.device), tags]).to(self.device)\n",
    "        for i, feat in enumerate(lstm_features):\n",
    "            score += self.transitions_to[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "                \n",
    "        score += self.transitions_to[self.tag_to_ix['END_TAG'], tags[-1]]\n",
    "        return score\n",
    "    \n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        lstm_feats = self.get_lstm_features(sentence)\n",
    "        forward_score = self._forward_algo(lstm_feats)\n",
    "        gold_score = self._score_sentence(lstm_feats, tags)\n",
    "        return forward_score - gold_score\n",
    "    \n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.target_size), -10000.).to(self.device)\n",
    "        init_vvars[0][self.start_tag] = 0\n",
    "\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = [] \n",
    "            viterbivars_t = [] \n",
    "\n",
    "            next_tag_var = self.transitions_to + forward_var.expand(self.target_size, -1)\n",
    "            best_tag_id = torch.argmax(next_tag_var, dim=1)\n",
    "            bptrs_t = best_tag_id\n",
    "            viterbivars_t = next_tag_var[range(len(best_tag_id)), best_tag_id].view(1, -1)\n",
    "            \n",
    "            forward_var = (viterbivars_t + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        \n",
    "        terminal_var = forward_var + self.transitions_to[self.end_tag]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "       \n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id.item())\n",
    "        \n",
    "        start = best_path.pop()\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "        \n",
    "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self.get_lstm_features(sentence)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, model_name):\n",
    "    test_loss = 0\n",
    "    test_f1 = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for case in tqdm(test_data):\n",
    "            sentence = prepare_sequence(test_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(test_data[case]['labels'], tag_to_ix)\n",
    "\n",
    "            # Make predictions\n",
    "            tag_scores = model(sentence)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Convert predictions and targets to numpy arrays\n",
    "            predictions = torch.argmax(tag_scores, dim=1).detach().numpy()\n",
    "            targets = targets.detach().numpy()\n",
    "\n",
    "            # Collect predictions and targets for later evaluation\n",
    "            all_predictions.extend(predictions)\n",
    "            all_targets.extend(targets)\n",
    "\n",
    "            # Calculate F1 score\n",
    "            test_f1 += f1_score(targets, predictions, average='macro')\n",
    "\n",
    "    # Calculate average test loss and F1 score\n",
    "    avg_test_loss = test_loss / len(test_data)\n",
    "    avg_test_f1 = test_f1 / len(test_data)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(all_targets, all_predictions)    \n",
    "    return {'Embedding': model_name, 'Test Macro-F1': avg_test_f1, 'Test Accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_BiLSTM(model, model_name):\n",
    "    test_loss = 0\n",
    "    test_f1 = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for case in tqdm(test_data):\n",
    "            sentence = prepare_sequence(test_data[case]['text'].split(' '), word_to_idx)\n",
    "            tags = prepare_sequence(test_data[case]['labels'], tag_to_ix)\n",
    "            # sentence = sentence.to(device)\n",
    "            # tags = tags.to(device)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = model.neg_log_likelihood(sentence, tags)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Make predictions\n",
    "            _, preds = model(sentence)\n",
    "            # preds = preds.cpu().numpy()\n",
    "            # tags = tags.cpu().numpy()\n",
    "\n",
    "            # Collect predictions and targets for later evaluation\n",
    "            all_predictions.extend(preds)\n",
    "            all_targets.extend(tags)\n",
    "\n",
    "            # Calculate F1 score\n",
    "            test_f1 += f1_score(tags, preds, average='macro')\n",
    "\n",
    "    # Calculate average test loss and F1 score\n",
    "    avg_test_loss = test_loss / len(test_data)\n",
    "    avg_test_f1 = test_f1 / len(test_data)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(all_targets, all_predictions)    \n",
    "    return {'Embedding': model_name, 'Test Macro-F1': avg_test_f1, 'Test Accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = {}\n",
    "base_path = 'Part 2/Non Trainable Embeddings'\n",
    "models = os.listdir(base_path)\n",
    "\n",
    "for model_name in models:\n",
    "    final_path = base_path + '/' + model_name + '/model.pt'\n",
    "    model = torch.load(final_path)\n",
    "    model.eval()\n",
    "    eval_metrics[model_name] = evaluate(model, model_name)\n",
    "\n",
    "base_path = 'Part 3/Non Trainable Embeddings'\n",
    "models = os.listdir(base_path)\n",
    "\n",
    "for model_name in models:\n",
    "    final_path = base_path + '/' + model_name + '/model.pt'\n",
    "    model = torch.load(final_path)\n",
    "    model.eval()\n",
    "    eval_metrics[f'{model_name}+BiLSTM-CRF'] = eval_BiLSTM(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(eval_metrics).transpose().reset_index()\n",
    "df.rename(columns={'index': 'Model'}, inplace=True)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
