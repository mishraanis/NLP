{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1b5e8c53810>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #check if cuda is available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = json.load(open('../Dataset/BIO_Tagged/ATE_train.json', 'r'))\n",
    "test_data = json.load(open('../Dataset/BIO_Tagged/ATE_test.json', 'r'))\n",
    "val_data = json.load(open('../Dataset/BIO_Tagged/ATE_val.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = pickle.load(open('../Utils/word_to_idx.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_to_ix = pickle.load(open('../Utils/tag_to_ix.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, target_size, embedding_mat, start_tag, end_tag, tag_to_ix, device='cpu'):\n",
    "        super(RNN_model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_mat)).to(device)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim).to(device)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size).to(device)\n",
    "        self.start_tag = start_tag\n",
    "        self.end_tag = end_tag\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.target_size = target_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        rnn_out, _ = self.rnn(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(rnn_out.view(len(sentence), -1))\n",
    "        tag_scores = nn.functional.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, target_size, embedding_mat, start_tag, end_tag, tag_to_ix, device='cpu'):\n",
    "        super(LSTM_model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_mat)).to(device)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim).to(device)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size).to(device)\n",
    "        self.start_tag = start_tag\n",
    "        self.end_tag = end_tag\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.target_size = target_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = nn.functional.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, target_size, embedding_mat, start_tag, end_tag, tag_to_ix, device='cpu'):\n",
    "        super(GRU_model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_mat)).to(device)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim).to(device)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size).to(device)\n",
    "        self.start_tag = start_tag\n",
    "        self.end_tag = end_tag\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.target_size = target_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        gru_out, _ = self.gru(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(gru_out.view(len(sentence), -1))\n",
    "        tag_scores = nn.functional.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding mats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embedding_mat = pickle.load(open('../Extracted Word Embeddings/legal_bert_embedding_mat.pkl', 'rb'))\n",
    "word2vec_embedding_mat = pickle.load(open('../Extracted Word Embeddings/word2vec_embedding_mat.pkl', 'rb'))\n",
    "glove_embedding_mat = pickle.load(open('../Extracted Word Embeddings/glove_embedding_mat.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glove + RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:04<00:00, 188.60it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 641.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5968134191882268, Val Loss: 0.4906317757008827, Train F1: 0.459286623513282, Val F1: 0.5663856508567454\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:05<00:00, 176.17it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 527.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.47993985639173725, Val Loss: 0.44160774785634044, Train F1: 0.5662979265034384, Val F1: 0.6233874019219469\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:05<00:00, 174.16it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 538.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4413757815743256, Val Loss: 0.4215341919020975, Train F1: 0.6023781967599309, Val F1: 0.6364297677431544\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:05<00:00, 177.19it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 547.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4173726557895837, Val Loss: 0.41181905111765754, Train F1: 0.6290902141263539, Val F1: 0.6435473665837765\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:04<00:00, 189.86it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 615.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3997564797698794, Val Loss: 0.40676956460492253, Train F1: 0.6489653533198935, Val F1: 0.6418748309728011\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:05<00:00, 166.47it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 496.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.38580086319882084, Val Loss: 0.40432043985961236, Train F1: 0.6615917726602287, Val F1: 0.6477432574435013\n",
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:05<00:00, 173.36it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 399.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.37409793594996005, Val Loss: 0.40345122039182, Train F1: 0.67467678933486, Val F1: 0.656578393443616\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:05<00:00, 164.57it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 603.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.363856319527226, Val Loss: 0.4034735304920096, Train F1: 0.6849642588034722, Val F1: 0.6588760977371073\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:04<00:00, 216.33it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 614.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3546119403447668, Val Loss: 0.40396422222596867, Train F1: 0.6928735673691673, Val F1: 0.6623555267227851\n",
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:05<00:00, 177.93it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 391.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.34607392498528455, Val Loss: 0.40475360673244143, Train F1: 0.6998949622485837, Val F1: 0.6630402886840052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rnn_model = RNN_model(len(word_to_idx), 300, 256, len(tag_to_ix), glove_embedding_mat, tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(rnn_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    train_loss_temp = 0\n",
    "    val_loss_temp = 0\n",
    "    train_f1_temp = 0\n",
    "    val_f1_temp = 0\n",
    "    rnn_model.train()\n",
    "    for case in tqdm(train_data):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        targets = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        rnn_model.zero_grad()\n",
    "        tag_scores = rnn_model(sentence)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp += loss.item()\n",
    "        train_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    train_loss.append(train_loss_temp/len(train_data))\n",
    "    train_f1.append(train_f1_temp/len(train_data))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        rnn_model.eval()\n",
    "        for case in tqdm(val_data):\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            tag_scores = rnn_model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            val_loss_temp += loss.item()\n",
    "            val_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    val_loss.append(val_loss_temp/len(val_data))\n",
    "    val_f1.append(val_f1_temp/len(val_data))\n",
    "\n",
    "    print(f'Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Train F1: {train_f1[-1]}, Val F1: {val_f1[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rnn_model, 'Non Trainable Embeddings/Glove+RNN/model.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Glove+RNN/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Glove+RNN/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_f1, open('Non Trainable Embeddings/Glove+RNN/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_f1, open('Non Trainable Embeddings/Glove+RNN/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2vec + RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/906 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:04<00:00, 203.56it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 466.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6535127756019302, Val Loss: 0.5358878160448379, Train F1: 0.38059835866048025, Val F1: 0.4678420440307875\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:04<00:00, 181.92it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 478.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4984212756716126, Val Loss: 0.44232855669167487, Train F1: 0.5195272489184937, Val F1: 0.5812711647647449\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:04<00:00, 199.21it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 417.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.43399601949432304, Val Loss: 0.40195366855897857, Train F1: 0.6063180209195727, Val F1: 0.6456353777059476\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:04<00:00, 186.68it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 493.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4020476406789642, Val Loss: 0.38321965364696775, Train F1: 0.6473607425230786, Val F1: 0.6749788417410171\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:04<00:00, 193.82it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 390.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3822060127436259, Val Loss: 0.3727722850088115, Train F1: 0.6657423564716967, Val F1: 0.6902428977765166\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:05<00:00, 171.27it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 318.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3673849542631416, Val Loss: 0.3661038413916004, Train F1: 0.6774983027510253, Val F1: 0.6983145257049538\n",
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:06<00:00, 150.04it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 567.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.35521191141052094, Val Loss: 0.36174247713257734, Train F1: 0.6860951980741811, Val F1: 0.7001032640530731\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:05<00:00, 171.26it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 609.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3447088405744853, Val Loss: 0.35916402391647095, Train F1: 0.6980137923857486, Val F1: 0.7010041891924879\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:05<00:00, 167.20it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 425.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3354455349806523, Val Loss: 0.3579895687906165, Train F1: 0.7069530224102186, Val F1: 0.7000983633459458\n",
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:06<00:00, 144.96it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 410.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3271152251636482, Val Loss: 0.35792350151563346, Train F1: 0.715636726258825, Val F1: 0.7013391906011901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rnn_model = RNN_model(len(word_to_idx), 300, 256, len(tag_to_ix), word2vec_embedding_mat, tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(rnn_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    train_loss_temp = 0\n",
    "    val_loss_temp = 0\n",
    "    train_f1_temp = 0\n",
    "    val_f1_temp = 0\n",
    "    rnn_model.train()\n",
    "    for case in tqdm(train_data):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        targets = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        rnn_model.zero_grad()\n",
    "        tag_scores = rnn_model(sentence)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp += loss.item()\n",
    "        train_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    train_loss.append(train_loss_temp/len(train_data))\n",
    "    train_f1.append(train_f1_temp/len(train_data))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        rnn_model.eval()\n",
    "        for case in tqdm(val_data):\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            tag_scores = rnn_model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            val_loss_temp += loss.item()\n",
    "            val_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    val_loss.append(val_loss_temp/len(val_data))\n",
    "    val_f1.append(val_f1_temp/len(val_data))\n",
    "\n",
    "    print(f'Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Train F1: {train_f1[-1]}, Val F1: {val_f1[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rnn_model, 'Non Trainable Embeddings/Word2vec+RNN/model.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Word2vec+RNN/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Word2vec+RNN/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_f1, open('Non Trainable Embeddings/Word2vec+RNN/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_f1, open('Non Trainable Embeddings/Word2vec+RNN/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert + RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9/906 [00:00<00:10, 83.66it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 99.79it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 492.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6838453699717458, Val Loss: 0.6475972284192908, Train F1: 0.4108339289631352, Val F1: 0.4209117892116244\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:06<00:00, 135.41it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 449.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6562118282085223, Val Loss: 0.6390599423347543, Train F1: 0.42972467535969533, Val F1: 0.4358104310487969\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:07<00:00, 128.16it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 263.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.646315835580789, Val Loss: 0.6327123050003836, Train F1: 0.4332680885477291, Val F1: 0.4386494484842911\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:11<00:00, 75.87it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 400.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6386674604405367, Val Loss: 0.6274173443448053, Train F1: 0.44198056079720877, Val F1: 0.4428039814250003\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 93.40it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 275.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6321836748627111, Val Loss: 0.623216449806135, Train F1: 0.4501051910519494, Val F1: 0.4415889452939511\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:08<00:00, 105.45it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 396.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6270107110308496, Val Loss: 0.6198079869083074, Train F1: 0.4537786033371692, Val F1: 0.44520201966899364\n",
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:07<00:00, 117.32it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 368.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6225836438519539, Val Loss: 0.6168556880188859, Train F1: 0.45844530894838115, Val F1: 0.44639177981914774\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 96.79it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 403.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6184745836369776, Val Loss: 0.614347759583225, Train F1: 0.46164803585873876, Val F1: 0.449163102256526\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:10<00:00, 88.69it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 290.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6145513336159799, Val Loss: 0.6122919484360577, Train F1: 0.46372080395479465, Val F1: 0.4560875929647532\n",
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:07<00:00, 114.01it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 430.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6107513507865122, Val Loss: 0.6106750203049891, Train F1: 0.4659139108601753, Val F1: 0.4580197127836402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rnn_model = RNN_model(len(word_to_idx), 768, 512, len(tag_to_ix), bert_embedding_mat, tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(rnn_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    train_loss_temp = 0\n",
    "    val_loss_temp = 0\n",
    "    train_f1_temp = 0\n",
    "    val_f1_temp = 0\n",
    "    rnn_model.train()\n",
    "    for case in tqdm(train_data):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        targets = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        rnn_model.zero_grad()\n",
    "        tag_scores = rnn_model(sentence)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp += loss.item()\n",
    "        train_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    train_loss.append(train_loss_temp/len(train_data))\n",
    "    train_f1.append(train_f1_temp/len(train_data))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        rnn_model.eval()\n",
    "        for case in tqdm(val_data):\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            tag_scores = rnn_model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            val_loss_temp += loss.item()\n",
    "            val_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    val_loss.append(val_loss_temp/len(val_data))\n",
    "    val_f1.append(val_f1_temp/len(val_data))\n",
    "\n",
    "    print(f'Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Train F1: {train_f1[-1]}, Val F1: {val_f1[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rnn_model, 'Non Trainable Embeddings/Bert+RNN/model.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Bert+RNN/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Bert+RNN/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_f1, open('Non Trainable Embeddings/Bert+RNN/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_f1, open('Non Trainable Embeddings/Bert+RNN/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glove + GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/906 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:13<00:00, 69.24it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 265.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6877748227079973, Val Loss: 0.5866534595620142, Train F1: 0.36627919093451694, Val F1: 0.4462616602840087\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:12<00:00, 75.13it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 359.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5595951171604213, Val Loss: 0.5214242952871541, Train F1: 0.47824325652397687, Val F1: 0.5374964711655497\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 95.90it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 358.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5112265820633497, Val Loss: 0.4852672877240943, Train F1: 0.5406394252122311, Val F1: 0.571110503522886\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 96.42it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 361.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4820197305043802, Val Loss: 0.46180405518780016, Train F1: 0.5629418499258454, Val F1: 0.5853689802532933\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 95.85it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 326.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4610925462368308, Val Loss: 0.44524542422599445, Train F1: 0.5828699844212507, Val F1: 0.6010898120793638\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 96.10it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 354.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4448357619032691, Val Loss: 0.4330631505748997, Train F1: 0.5939519682322784, Val F1: 0.6138142255490178\n",
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 94.56it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 339.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.43144713103705423, Val Loss: 0.42373843045403425, Train F1: 0.6082213189483494, Val F1: 0.633375810713661\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 95.13it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 325.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4199479476571346, Val Loss: 0.4163357388891586, Train F1: 0.62394911283692, Val F1: 0.6382639654823741\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 92.42it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 351.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4098619041063928, Val Loss: 0.41023150481045517, Train F1: 0.6353051317016546, Val F1: 0.6432222298997439\n",
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 91.27it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 283.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.40089639766392593, Val Loss: 0.4049899061656978, Train F1: 0.6436106278633325, Val F1: 0.639819831002705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gru_model = GRU_model(len(word_to_idx), 300, 256, len(tag_to_ix), glove_embedding_mat, tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(gru_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    train_loss_temp = 0\n",
    "    val_loss_temp = 0\n",
    "    train_f1_temp = 0\n",
    "    val_f1_temp = 0\n",
    "    gru_model.train()\n",
    "    for case in tqdm(train_data):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        targets = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        gru_model.zero_grad()\n",
    "        tag_scores = gru_model(sentence)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp += loss.item()\n",
    "        train_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    train_loss.append(train_loss_temp/len(train_data))\n",
    "    train_f1.append(train_f1_temp/len(train_data))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        gru_model.eval()\n",
    "        for case in tqdm(val_data):\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            tag_scores = gru_model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            val_loss_temp += loss.item()\n",
    "            val_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    val_loss.append(val_loss_temp/len(val_data))\n",
    "    val_f1.append(val_f1_temp/len(val_data))\n",
    "\n",
    "    print(f'Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Train F1: {train_f1[-1]}, Val F1: {val_f1[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gru_model, 'Non Trainable Embeddings/Glove+GRU/model.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Glove+GRU/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Glove+GRU/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_f1, open('Non Trainable Embeddings/Glove+GRU/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_f1, open('Non Trainable Embeddings/Glove+GRU/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2vec + GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/906 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:10<00:00, 86.91it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 339.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7604094644058619, Val Loss: 0.6435012930332253, Train F1: 0.35151441913565007, Val F1: 0.3531173110138638\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 93.42it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 318.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6128206780019975, Val Loss: 0.5698079982576849, Train F1: 0.36366325820668155, Val F1: 0.4066808421913643\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 92.73it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 342.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5495131920373466, Val Loss: 0.5154017844156588, Train F1: 0.43907330125220295, Val F1: 0.512430921428327\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 90.88it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 336.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5031745871302834, Val Loss: 0.4776720395099083, Train F1: 0.5183870917049738, Val F1: 0.5631630225125325\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:10<00:00, 89.84it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 334.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.471383687972233, Val Loss: 0.45215561135446647, Train F1: 0.5562230864249489, Val F1: 0.5967591743697546\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 91.54it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 331.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.44956634886085856, Val Loss: 0.4342395785736711, Train F1: 0.5807132910379856, Val F1: 0.608949143674809\n",
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:12<00:00, 75.04it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 294.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4337269016613497, Val Loss: 0.4209292972604978, Train F1: 0.5998319687435444, Val F1: 0.6145040918152864\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:10<00:00, 87.58it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 322.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.42143394626167174, Val Loss: 0.41049077161098724, Train F1: 0.6088956180109768, Val F1: 0.6260205462626923\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:10<00:00, 88.75it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 324.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.41131051442283667, Val Loss: 0.4019248680505034, Train F1: 0.6190169581061779, Val F1: 0.6440093282600707\n",
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:13<00:00, 68.41it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 229.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.40258057483741255, Val Loss: 0.3946731783891922, Train F1: 0.6280850830387626, Val F1: 0.6592141532923255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gru_model = GRU_model(len(word_to_idx), 300, 256, len(tag_to_ix), word2vec_embedding_mat, tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(gru_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    train_loss_temp = 0\n",
    "    val_loss_temp = 0\n",
    "    train_f1_temp = 0\n",
    "    val_f1_temp = 0\n",
    "    gru_model.train()\n",
    "    for case in tqdm(train_data):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        targets = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        gru_model.zero_grad()\n",
    "        tag_scores = gru_model(sentence)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp += loss.item()\n",
    "        train_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    train_loss.append(train_loss_temp/len(train_data))\n",
    "    train_f1.append(train_f1_temp/len(train_data))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        gru_model.eval()\n",
    "        for case in tqdm(val_data):\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            tag_scores = gru_model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            val_loss_temp += loss.item()\n",
    "            val_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    val_loss.append(val_loss_temp/len(val_data))\n",
    "    val_f1.append(val_f1_temp/len(val_data))\n",
    "\n",
    "    print(f'Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Train F1: {train_f1[-1]}, Val F1: {val_f1[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gru_model, 'Non Trainable Embeddings/Word2vec+GRU/model.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Word2vec+GRU/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Word2vec+GRU/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_f1, open('Non Trainable Embeddings/Word2vec+GRU/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_f1, open('Non Trainable Embeddings/Word2vec+GRU/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert + GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/906 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:31<00:00, 28.90it/s]\n",
      "100%|██████████| 219/219 [00:01<00:00, 174.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6987751507206469, Val Loss: 0.6678536323379708, Train F1: 0.3748922840792369, Val F1: 0.3927246398718795\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:36<00:00, 25.00it/s]\n",
      "100%|██████████| 219/219 [00:01<00:00, 156.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6707709766513201, Val Loss: 0.6562257428147477, Train F1: 0.40512667020145765, Val F1: 0.4167879518205866\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:42<00:00, 21.44it/s]\n",
      "100%|██████████| 219/219 [00:01<00:00, 156.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6604047863195274, Val Loss: 0.6494642232106701, Train F1: 0.41932009923783226, Val F1: 0.42319201072140555\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:39<00:00, 23.10it/s]\n",
      "100%|██████████| 219/219 [00:01<00:00, 145.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6537817921695067, Val Loss: 0.6451395852108525, Train F1: 0.42852710994737475, Val F1: 0.4284831768955887\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:37<00:00, 24.21it/s]\n",
      "100%|██████████| 219/219 [00:01<00:00, 149.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6486948361624418, Val Loss: 0.6417684333237339, Train F1: 0.43496116121895867, Val F1: 0.4337298812627581\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:37<00:00, 24.40it/s]\n",
      "100%|██████████| 219/219 [00:01<00:00, 173.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6442992698409437, Val Loss: 0.6389145513647767, Train F1: 0.43914932005353957, Val F1: 0.4344141799235426\n",
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:37<00:00, 23.95it/s]\n",
      "100%|██████████| 219/219 [00:01<00:00, 165.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6403215026223896, Val Loss: 0.636435652160209, Train F1: 0.44300508971183966, Val F1: 0.4365404359397132\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:38<00:00, 23.53it/s]\n",
      "100%|██████████| 219/219 [00:01<00:00, 162.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6366377842610509, Val Loss: 0.6342574647844654, Train F1: 0.44616485362484753, Val F1: 0.44079209455464113\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:38<00:00, 23.81it/s]\n",
      "100%|██████████| 219/219 [00:01<00:00, 160.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6331809836770789, Val Loss: 0.632328130066667, Train F1: 0.4495940085047949, Val F1: 0.4442398268555838\n",
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:39<00:00, 23.01it/s]\n",
      "100%|██████████| 219/219 [00:01<00:00, 152.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6299121094657886, Val Loss: 0.6306091001316837, Train F1: 0.45096639102456526, Val F1: 0.44672981845427473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gru_model = GRU_model(len(word_to_idx), 768, 512, len(tag_to_ix), bert_embedding_mat, tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(gru_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    train_loss_temp = 0\n",
    "    val_loss_temp = 0\n",
    "    train_f1_temp = 0\n",
    "    val_f1_temp = 0\n",
    "    gru_model.train()\n",
    "    for case in tqdm(train_data):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        targets = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        gru_model.zero_grad()\n",
    "        tag_scores = gru_model(sentence)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp += loss.item()\n",
    "        train_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    train_loss.append(train_loss_temp/len(train_data))\n",
    "    train_f1.append(train_f1_temp/len(train_data))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        gru_model.eval()\n",
    "        for case in tqdm(val_data):\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            tag_scores = gru_model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            val_loss_temp += loss.item()\n",
    "            val_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    val_loss.append(val_loss_temp/len(val_data))\n",
    "    val_f1.append(val_f1_temp/len(val_data))\n",
    "\n",
    "    print(f'Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Train F1: {train_f1[-1]}, Val F1: {val_f1[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gru_model, 'Non Trainable Embeddings/Bert+GRU/model.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Bert+GRU/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Bert+GRU/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_f1, open('Non Trainable Embeddings/Bert+GRU/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_f1, open('Non Trainable Embeddings/Bert+GRU/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glove + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:06<00:00, 133.89it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 342.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7816405328562192, Val Loss: 0.6583317537285965, Train F1: 0.3515741671625853, Val F1: 0.3531173110138638\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:05<00:00, 151.06it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 400.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6441842318568008, Val Loss: 0.604199711182346, Train F1: 0.35803346825563187, Val F1: 0.38346339543386804\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:06<00:00, 139.51it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 366.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5941185467027408, Val Loss: 0.5583752292340205, Train F1: 0.4065685712718787, Val F1: 0.48281831330461805\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:06<00:00, 146.53it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 345.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5537682570533511, Val Loss: 0.5235781236870648, Train F1: 0.48463369581230736, Val F1: 0.5307228574394807\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:06<00:00, 135.24it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 342.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.523562083131825, Val Loss: 0.4971816104977098, Train F1: 0.5248353214608321, Val F1: 0.5583468355845068\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:06<00:00, 139.96it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 346.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5004034005352204, Val Loss: 0.47629274489128426, Train F1: 0.55027184182903, Val F1: 0.5700771985616385\n",
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:06<00:00, 140.85it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 348.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4818193700912904, Val Loss: 0.4593843416264068, Train F1: 0.566907656149681, Val F1: 0.589930165461253\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:06<00:00, 131.28it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 340.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4664415038020142, Val Loss: 0.4456977086932692, Train F1: 0.5790176834997439, Val F1: 0.5950062855061207\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:06<00:00, 143.93it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 364.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.45341039362628727, Val Loss: 0.4345080314297654, Train F1: 0.5877747024987416, Val F1: 0.6027366631881801\n",
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:06<00:00, 150.63it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 372.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4419508746292681, Val Loss: 0.4251132215879279, Train F1: 0.5954390516435106, Val F1: 0.6057877371685894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_model = LSTM_model(len(word_to_idx), 300, 256, len(tag_to_ix), glove_embedding_mat, tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(lstm_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    train_loss_temp = 0\n",
    "    val_loss_temp = 0\n",
    "    train_f1_temp = 0\n",
    "    val_f1_temp = 0\n",
    "    lstm_model.train()\n",
    "    for case in tqdm(train_data):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        targets = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        lstm_model.zero_grad()\n",
    "        tag_scores = lstm_model(sentence)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp += loss.item()\n",
    "        train_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    train_loss.append(train_loss_temp/len(train_data))\n",
    "    train_f1.append(train_f1_temp/len(train_data))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        lstm_model.eval()\n",
    "        for case in tqdm(val_data):\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            tag_scores = lstm_model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            val_loss_temp += loss.item()\n",
    "            val_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    val_loss.append(val_loss_temp/len(val_data))\n",
    "    val_f1.append(val_f1_temp/len(val_data))\n",
    "\n",
    "    print(f'Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Train F1: {train_f1[-1]}, Val F1: {val_f1[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm_model, 'Non Trainable Embeddings/Glove+LSTM/model.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Glove+LSTM/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Glove+LSTM/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_f1, open('Non Trainable Embeddings/Glove+LSTM/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_f1, open('Non Trainable Embeddings/Glove+LSTM/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2vec + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:06<00:00, 138.14it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 288.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8449351207310001, Val Loss: 0.6961075300223207, Train F1: 0.3483793784747095, Val F1: 0.3531173110138638\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:07<00:00, 127.08it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 355.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6822504956793312, Val Loss: 0.6535659399751115, Train F1: 0.35221363021064966, Val F1: 0.3531173110138638\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:06<00:00, 134.11it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 344.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6484705856106928, Val Loss: 0.6231789111274563, Train F1: 0.35221363021064966, Val F1: 0.3531173110138638\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 99.78it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 274.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6191497126030869, Val Loss: 0.5933153915078673, Train F1: 0.3536845551176199, Val F1: 0.35800240050874527\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:06<00:00, 132.94it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 310.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5893256704078341, Val Loss: 0.5631597685759471, Train F1: 0.3676505491866325, Val F1: 0.3944398364779217\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:07<00:00, 128.07it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 348.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5596519062423022, Val Loss: 0.5345916339115465, Train F1: 0.41465366858600916, Val F1: 0.4676783862661805\n",
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:07<00:00, 123.25it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 347.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5323463338067463, Val Loss: 0.5095369756630023, Train F1: 0.4651418506933484, Val F1: 0.5190232596165744\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:06<00:00, 132.44it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 331.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5089066948980159, Val Loss: 0.48845615146094806, Train F1: 0.5090672007879095, Val F1: 0.5473228506226737\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:06<00:00, 130.94it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 339.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4892592430937106, Val Loss: 0.47070595696908696, Train F1: 0.5413926229152894, Val F1: 0.5652154331093707\n",
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:06<00:00, 129.49it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 375.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.472696417412221, Val Loss: 0.4556077920818982, Train F1: 0.557499713473176, Val F1: 0.5850595774745727\n"
     ]
    }
   ],
   "source": [
    "lstm_model = LSTM_model(len(word_to_idx), 300, 256, len(tag_to_ix), word2vec_embedding_mat, tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(lstm_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    train_loss_temp = 0\n",
    "    val_loss_temp = 0\n",
    "    train_f1_temp = 0\n",
    "    val_f1_temp = 0\n",
    "    lstm_model.train()\n",
    "    for case in tqdm(train_data):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        targets = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        lstm_model.zero_grad()\n",
    "        tag_scores = lstm_model(sentence)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp += loss.item()\n",
    "        train_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    train_loss.append(train_loss_temp/len(train_data))\n",
    "    train_f1.append(train_f1_temp/len(train_data))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        lstm_model.eval()\n",
    "        for case in tqdm(val_data):\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            tag_scores = lstm_model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            val_loss_temp += loss.item()\n",
    "            val_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    val_loss.append(val_loss_temp/len(val_data))\n",
    "    val_f1.append(val_f1_temp/len(val_data))\n",
    "\n",
    "    print(f'Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Train F1: {train_f1[-1]}, Val F1: {val_f1[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm_model, 'Non Trainable Embeddings/Word2vec+LSTM/model.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Word2vec+LSTM/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Word2vec+LSTM/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_f1, open('Non Trainable Embeddings/Word2vec+LSTM/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_f1, open('Non Trainable Embeddings/Word2vec+LSTM/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:27<00:00, 33.53it/s]\n",
      "100%|██████████| 219/219 [00:01<00:00, 157.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7167522716963002, Val Loss: 0.676812830717052, Train F1: 0.3547878682102667, Val F1: 0.35788249528469185\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:25<00:00, 35.01it/s]\n",
      "100%|██████████| 219/219 [00:01<00:00, 127.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6783066020073744, Val Loss: 0.6663340824107601, Train F1: 0.36790737678011937, Val F1: 0.38888443691028063\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:31<00:00, 28.82it/s]\n",
      "100%|██████████| 219/219 [00:02<00:00, 91.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6687125607371067, Val Loss: 0.6593067778027766, Train F1: 0.38656198439304984, Val F1: 0.42570941145959823\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:33<00:00, 27.29it/s]\n",
      "100%|██████████| 219/219 [00:02<00:00, 89.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6615877430104788, Val Loss: 0.6532567226451281, Train F1: 0.407689665085332, Val F1: 0.4316405581707851\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:30<00:00, 29.36it/s]\n",
      "100%|██████████| 219/219 [00:01<00:00, 169.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6563322812516168, Val Loss: 0.648992141904352, Train F1: 0.4196121755728854, Val F1: 0.43117207740887525\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:22<00:00, 39.87it/s]\n",
      "100%|██████████| 219/219 [00:02<00:00, 93.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.652534528250989, Val Loss: 0.6458674903329649, Train F1: 0.425348784515098, Val F1: 0.4277023179828389\n",
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:24<00:00, 37.37it/s]\n",
      "100%|██████████| 219/219 [00:01<00:00, 151.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6493790270082208, Val Loss: 0.6433128713200625, Train F1: 0.42886445526165556, Val F1: 0.4280994155913633\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:26<00:00, 33.94it/s]\n",
      "100%|██████████| 219/219 [00:02<00:00, 101.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6465524618100646, Val Loss: 0.6410244481476475, Train F1: 0.43197066002060525, Val F1: 0.4353229748345722\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:27<00:00, 33.03it/s]\n",
      "100%|██████████| 219/219 [00:01<00:00, 140.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6439385161330106, Val Loss: 0.6388820456043226, Train F1: 0.433782664336745, Val F1: 0.4356017746607418\n",
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:24<00:00, 36.80it/s]\n",
      "100%|██████████| 219/219 [00:01<00:00, 152.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6414778806910609, Val Loss: 0.6368587898881468, Train F1: 0.4376682314077051, Val F1: 0.4383686839631542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_model = LSTM_model(len(word_to_idx), 768, 512, len(tag_to_ix), bert_embedding_mat, tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(lstm_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    train_loss_temp = 0\n",
    "    val_loss_temp = 0\n",
    "    train_f1_temp = 0\n",
    "    val_f1_temp = 0\n",
    "    lstm_model.train()\n",
    "    for case in tqdm(train_data):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        targets = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        lstm_model.zero_grad()\n",
    "        tag_scores = lstm_model(sentence)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp += loss.item()\n",
    "        train_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    train_loss.append(train_loss_temp/len(train_data))\n",
    "    train_f1.append(train_f1_temp/len(train_data))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        lstm_model.eval()\n",
    "        for case in tqdm(val_data):\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            tag_scores = lstm_model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            val_loss_temp += loss.item()\n",
    "            val_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    val_loss.append(val_loss_temp/len(val_data))\n",
    "    val_f1.append(val_f1_temp/len(val_data))\n",
    "\n",
    "    print(f'Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Train F1: {train_f1[-1]}, Val F1: {val_f1[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm_model, 'Non Trainable Embeddings/Bert+LSTM/model.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Bert+LSTM/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Bert+LSTM/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_f1, open('Non Trainable Embeddings/Bert+LSTM/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_f1, open('Non Trainable Embeddings/Bert+LSTM/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
