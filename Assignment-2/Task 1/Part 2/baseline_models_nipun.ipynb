{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x231a62e8c30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #check if cuda is available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = json.load(open('../Dataset/NER_train.json', 'r'))\n",
    "test_data = json.load(open('../Dataset/NER_test.json', 'r'))\n",
    "val_data = json.load(open('../Dataset/NER_val.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = pickle.load(open('../Utils/word_to_idx.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_to_ix = pickle.load(open('../Utils/tag_to_ix.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, target_size, embedding_mat, start_tag, end_tag, tag_to_ix, device='cpu'):\n",
    "        super(RNN_model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_mat)).to(device)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim).to(device)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size).to(device)\n",
    "        self.start_tag = start_tag\n",
    "        self.end_tag = end_tag\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.target_size = target_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        rnn_out, _ = self.rnn(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(rnn_out.view(len(sentence), -1))\n",
    "        tag_scores = nn.functional.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, target_size, embedding_mat, start_tag, end_tag, tag_to_ix, device='cpu'):\n",
    "        super(LSTM_model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_mat)).to(device)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim).to(device)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size).to(device)\n",
    "        self.start_tag = start_tag\n",
    "        self.end_tag = end_tag\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.target_size = target_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = nn.functional.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, target_size, embedding_mat, start_tag, end_tag, tag_to_ix, device='cpu'):\n",
    "        super(GRU_model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_mat)).to(device)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim).to(device)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size).to(device)\n",
    "        self.start_tag = start_tag\n",
    "        self.end_tag = end_tag\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.target_size = target_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        gru_out, _ = self.gru(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(gru_out.view(len(sentence), -1))\n",
    "        tag_scores = nn.functional.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding mats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embedding_mat = pickle.load(open('../Utils/legal_bert_embedding_mat.pkl', 'rb'))\n",
    "word2vec_embedding_mat = pickle.load(open('../Utils/word2vec_embedding_mat.pkl', 'rb'))\n",
    "glove_embedding_mat = pickle.load(open('../Utils/glove_embedding_mat.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glove + RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8019 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "rnn_model = RNN_model(len(word_to_idx), 300, 256, len(tag_to_ix), glove_embedding_mat, tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(rnn_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    train_loss_temp = 0\n",
    "    val_loss_temp = 0\n",
    "    train_f1_temp = 0\n",
    "    val_f1_temp = 0\n",
    "    rnn_model.train()\n",
    "    for case in tqdm(train_data):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        targets = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        rnn_model.zero_grad()\n",
    "        tag_scores = rnn_model(sentence)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp += loss.item()\n",
    "        train_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    train_loss.append(train_loss_temp/len(train_data))\n",
    "    train_f1.append(train_f1_temp/len(train_data))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        rnn_model.eval()\n",
    "        for case in tqdm(val_data):\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            tag_scores = rnn_model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            val_loss_temp += loss.item()\n",
    "            val_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    val_loss.append(val_loss_temp/len(val_data))\n",
    "    val_f1.append(val_f1_temp/len(val_data))\n",
    "\n",
    "    print(f'Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Train F1: {train_f1[-1]}, Val F1: {val_f1[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rnn_model, 'Non Trainable Embeddings/Glove+RNN/model.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Glove+RNN/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Glove+RNN/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_f1, open('Non Trainable Embeddings/Glove+RNN/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_f1, open('Non Trainable Embeddings/Glove+RNN/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2vec + RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = RNN_model(len(word_to_idx), 300, 256, len(tag_to_ix), word2vec_embedding_mat, tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(rnn_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    train_loss_temp = 0\n",
    "    val_loss_temp = 0\n",
    "    train_f1_temp = 0\n",
    "    val_f1_temp = 0\n",
    "    rnn_model.train()\n",
    "    for case in tqdm(train_data):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        targets = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        rnn_model.zero_grad()\n",
    "        tag_scores = rnn_model(sentence)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp += loss.item()\n",
    "        train_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    train_loss.append(train_loss_temp/len(train_data))\n",
    "    train_f1.append(train_f1_temp/len(train_data))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        rnn_model.eval()\n",
    "        for case in tqdm(val_data):\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            tag_scores = rnn_model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            val_loss_temp += loss.item()\n",
    "            val_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    val_loss.append(val_loss_temp/len(val_data))\n",
    "    val_f1.append(val_f1_temp/len(val_data))\n",
    "\n",
    "    print(f'Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Train F1: {train_f1[-1]}, Val F1: {val_f1[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rnn_model, 'Non Trainable Embeddings/Word2vec+RNN/model.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Word2vec+RNN/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Word2vec+RNN/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_f1, open('Non Trainable Embeddings/Word2vec+RNN/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_f1, open('Non Trainable Embeddings/Word2vec+RNN/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert + RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:50<07:37, 50.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.00014700164319947362, Val loss: 0.00012835307279601693\n",
      "Train macro f1: 0.4298899923618197, Val macro f1: 0.44013995358029906\n",
      "Epoch 1/10 done\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [01:42<06:51, 51.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.00011928575986530632, Val loss: 0.00011143186566187069\n",
      "Train macro f1: 0.47232006862324005, Val macro f1: 0.4705441667847287\n",
      "Epoch 2/10 done\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [02:37<06:12, 53.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.00010447297972859815, Val loss: 0.00010207809100393206\n",
      "Train macro f1: 0.4986533180118992, Val macro f1: 0.4879303498648421\n",
      "Epoch 3/10 done\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [03:35<05:30, 55.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 9.694076288724318e-05, Val loss: 9.402784053236246e-05\n",
      "Train macro f1: 0.5181887238848357, Val macro f1: 0.4992261886920438\n",
      "Epoch 4/10 done\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [04:44<05:00, 60.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 9.272542229155079e-05, Val loss: 8.805939432932064e-05\n",
      "Train macro f1: 0.5313980425154133, Val macro f1: 0.5103638777358255\n",
      "Epoch 5/10 done\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [05:45<04:01, 60.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 9.054591646417975e-05, Val loss: 8.332310972036794e-05\n",
      "Train macro f1: 0.5428629820788246, Val macro f1: 0.5262532842109489\n",
      "Epoch 6/10 done\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [06:52<03:07, 62.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 8.965966117102653e-05, Val loss: 7.923923112684861e-05\n",
      "Train macro f1: 0.5518149781784606, Val macro f1: 0.5350296434234294\n",
      "Epoch 7/10 done\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [08:00<02:08, 64.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 8.915618673199788e-05, Val loss: 7.565174018964171e-05\n",
      "Train macro f1: 0.5593830546409031, Val macro f1: 0.5405909514671249\n",
      "Epoch 8/10 done\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [09:01<01:03, 63.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 8.859747322276235e-05, Val loss: 7.305510371224955e-05\n",
      "Train macro f1: 0.5662834766962865, Val macro f1: 0.5441634178303829\n",
      "Epoch 9/10 done\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [09:55<00:00, 59.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 8.804789831629023e-05, Val loss: 7.051078137010336e-05\n",
      "Train macro f1: 0.5725575789822337, Val macro f1: 0.5451702618390271\n",
      "Epoch 10/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rnn_model = RNN_model(len(word_to_idx), 768, 512, len(tag_to_ix), bert_embedding_mat, tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(rnn_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    train_loss_temp = 0\n",
    "    val_loss_temp = 0\n",
    "    train_f1_temp = 0\n",
    "    val_f1_temp = 0\n",
    "    rnn_model.train()\n",
    "    for case in tqdm(train_data):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        targets = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        rnn_model.zero_grad()\n",
    "        tag_scores = rnn_model(sentence)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp += loss.item()\n",
    "        train_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    train_loss.append(train_loss_temp/len(train_data))\n",
    "    train_f1.append(train_f1_temp/len(train_data))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        rnn_model.eval()\n",
    "        for case in tqdm(val_data):\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            tag_scores = rnn_model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            val_loss_temp += loss.item()\n",
    "            val_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    val_loss.append(val_loss_temp/len(val_data))\n",
    "    val_f1.append(val_f1_temp/len(val_data))\n",
    "\n",
    "    print(f'Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Train F1: {train_f1[-1]}, Val F1: {val_f1[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rnn_model, 'Non Trainable Embeddings/Bert+RNN/model.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Bert+RNN/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Bert+RNN/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_f1, open('Non Trainable Embeddings/Bert+RNN/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_f1, open('Non Trainable Embeddings/Bert+RNN/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
