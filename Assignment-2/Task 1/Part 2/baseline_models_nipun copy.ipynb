{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x23c7a6f0c30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #check if cuda is available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = json.load(open('../Dataset/NER_train.json', 'r'))\n",
    "test_data = json.load(open('../Dataset/NER_test.json', 'r'))\n",
    "val_data = json.load(open('../Dataset/NER_val.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = pickle.load(open('../Utils/word_to_idx.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_to_ix = pickle.load(open('../Utils/tag_to_ix.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_to_tag = {v: k for k, v in tag_to_ix.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = set()\n",
    "for tag in tag_to_ix.keys():\n",
    "    if tag.startswith('B_') or tag.startswith('I_'):\n",
    "        labels.add(tag[2:])\n",
    "labels = list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_tags_to_entities(tag_sequence):\n",
    "    \"\"\"Convert a sequence of BIO tags into a list of entities with spans and types.\"\"\"\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "\n",
    "    for i, tag in enumerate(tag_sequence):\n",
    "        if tag.startswith('B_'):\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "            current_entity = [tag[2:], i, i]  # Entity type, start, end\n",
    "        elif tag.startswith('I_') and current_entity and current_entity[0] == tag[2:]:\n",
    "            current_entity[2] = i  # Extend the current entity to the new index\n",
    "        else:\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = None\n",
    "            if tag == 'O':\n",
    "                continue\n",
    "            else:  # Incorrect sequence (e.g., I- tag without a preceding B- tag of the same type)\n",
    "                current_entity = [tag[2:], i, i]  # Treat as a new entity for robustness\n",
    "\n",
    "    if current_entity:\n",
    "        entities.append(current_entity)\n",
    "\n",
    "    return entities\n",
    "\n",
    "def strict_f1(actual, pred, labels):\n",
    "    # get bio tags from indices\n",
    "    actual = [ix_to_tag[i] for i in actual]\n",
    "    pred = [ix_to_tag[i] for i in pred]\n",
    "    actual_entities = bio_tags_to_entities(actual)\n",
    "    pred_entities = bio_tags_to_entities(pred)\n",
    "\n",
    "    f1_scores = []\n",
    "\n",
    "    for label in labels:\n",
    "        label_tp = sum(1 for e in pred_entities if e[0] == label and e in actual_entities)\n",
    "        label_fp = sum(1 for e in pred_entities if e[0] == label and e not in actual_entities)\n",
    "        label_fn = sum(1 for e in actual_entities if e[0] == label and e not in pred_entities)\n",
    "\n",
    "        precision = label_tp / (label_tp + label_fp) if label_tp + label_fp > 0 else 0\n",
    "        recall = label_tp / (label_tp + label_fn) if label_tp + label_fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    macro_f1 = sum(f1_scores) / len(f1_scores)\n",
    "    return macro_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, target_size, embedding_mat, start_tag, end_tag, tag_to_ix, device='cpu'):\n",
    "        super(RNN_model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_mat), freeze=False).to(device)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim).to(device)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size).to(device)\n",
    "        self.start_tag = start_tag\n",
    "        self.end_tag = end_tag\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.target_size = target_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        rnn_out, _ = self.rnn(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(rnn_out.view(len(sentence), -1))\n",
    "        tag_scores = nn.functional.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, target_size, embedding_mat, start_tag, end_tag, tag_to_ix, device='cpu'):\n",
    "        super(LSTM_model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_mat)).to(device)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim).to(device)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size).to(device)\n",
    "        self.start_tag = start_tag\n",
    "        self.end_tag = end_tag\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.target_size = target_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = nn.functional.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, target_size, embedding_mat, start_tag, end_tag, tag_to_ix, device='cpu'):\n",
    "        super(GRU_model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_mat)).to(device)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim).to(device)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size).to(device)\n",
    "        self.start_tag = start_tag\n",
    "        self.end_tag = end_tag\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.target_size = target_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        gru_out, _ = self.gru(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(gru_out.view(len(sentence), -1))\n",
    "        tag_scores = nn.functional.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding mats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embedding_mat = pickle.load(open('../Utils/legal_bert_embedding_mat.pkl', 'rb'))\n",
    "word2vec_embedding_mat = pickle.load(open('../Utils/word2vec_embedding_mat.pkl', 'rb'))\n",
    "glove_embedding_mat = pickle.load(open('../Utils/glove_embedding_mat.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glove + RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/8019 [00:00<03:40, 36.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [02:48<00:00, 47.69it/s]\n",
      "100%|██████████| 1416/1416 [00:01<00:00, 886.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6217549527264596, Val Loss: 0.523366157995145, Train F1: 0.0033218636732406832, Val F1: 0.009070331739823244\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [02:46<00:00, 48.29it/s]\n",
      "100%|██████████| 1416/1416 [00:01<00:00, 873.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.47405354631749674, Val Loss: 0.43529565876322185, Train F1: 0.01865396730716614, Val F1: 0.02360128101653522\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 742/8019 [00:15<02:30, 48.51it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m tag_scores \u001b[38;5;241m=\u001b[39m rnn_model(sentence)\n\u001b[0;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(tag_scores, targets)\n\u001b[1;32m---> 24\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     26\u001b[0m train_loss_temp \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn_model = RNN_model(len(word_to_idx), 300, 256, len(tag_to_ix), glove_embedding_mat, tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(rnn_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    train_loss_temp = 0\n",
    "    val_loss_temp = 0\n",
    "    train_f1_temp = 0\n",
    "    val_f1_temp = 0\n",
    "    rnn_model.train()\n",
    "    for case in tqdm(train_data):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        targets = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        rnn_model.zero_grad()\n",
    "        tag_scores = rnn_model(sentence)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp += loss.item()\n",
    "        train_f1_temp += strict_f1(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), labels)\n",
    "    train_loss.append(train_loss_temp/len(train_data))\n",
    "    train_f1.append(train_f1_temp/len(train_data))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        rnn_model.eval()\n",
    "        for case in tqdm(val_data):\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            tag_scores = rnn_model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            val_loss_temp += loss.item()\n",
    "            val_f1_temp += strict_f1(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), labels)\n",
    "    val_loss.append(val_loss_temp/len(val_data))\n",
    "    val_f1.append(val_f1_temp/len(val_data))\n",
    "\n",
    "    print(f'Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Train F1: {train_f1[-1]}, Val F1: {val_f1[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rnn_model, 'Non Trainable Embeddings/Glove+RNN/model.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Glove+RNN/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Glove+RNN/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_f1, open('Non Trainable Embeddings/Glove+RNN/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_f1, open('Non Trainable Embeddings/Glove+RNN/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2vec + RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:01<00:00, 131.22it/s]\n",
      "100%|██████████| 1416/1416 [00:03<00:00, 376.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6568109598340905, Val Loss: 0.5123076539343265, Train F1: 0.45022232001638784, Val F1: 0.5075236501389049\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:03<00:00, 127.22it/s]\n",
      "100%|██████████| 1416/1416 [00:03<00:00, 393.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.46343237259119485, Val Loss: 0.4099017054983349, Train F1: 0.5393963593588122, Val F1: 0.5650450992340578\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:00<00:00, 132.63it/s]\n",
      "100%|██████████| 1416/1416 [00:03<00:00, 379.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3935642132259288, Val Loss: 0.36856085436455155, Train F1: 0.5896951710526034, Val F1: 0.5976405353162305\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:00<00:00, 133.05it/s]\n",
      "100%|██████████| 1416/1416 [00:04<00:00, 351.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.35709066869295225, Val Loss: 0.3457368135658299, Train F1: 0.6150465788912581, Val F1: 0.6155416658324357\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:01<00:00, 129.91it/s]\n",
      "100%|██████████| 1416/1416 [00:03<00:00, 378.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3315858886363667, Val Loss: 0.33031778971485054, Train F1: 0.6306646733857134, Val F1: 0.6285339054627583\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:01<00:00, 131.31it/s]\n",
      "100%|██████████| 1416/1416 [00:03<00:00, 379.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.31367390837457154, Val Loss: 0.3201191327903758, Train F1: 0.6436502582115257, Val F1: 0.6374339891746351\n",
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:01<00:00, 129.47it/s]\n",
      "100%|██████████| 1416/1416 [00:04<00:00, 352.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2988803275990276, Val Loss: 0.31305965234721667, Train F1: 0.6539981862033819, Val F1: 0.6425627022798542\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:02<00:00, 128.24it/s]\n",
      "100%|██████████| 1416/1416 [00:04<00:00, 323.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2866293002884059, Val Loss: 0.3094808140665271, Train F1: 0.6634050770304782, Val F1: 0.6430225970760293\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:02<00:00, 129.14it/s]\n",
      "100%|██████████| 1416/1416 [00:04<00:00, 333.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.27520281374611794, Val Loss: 0.3058609219299124, Train F1: 0.6721823049135188, Val F1: 0.6471620169636737\n",
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:04<00:00, 125.12it/s]\n",
      "100%|██████████| 1416/1416 [00:04<00:00, 345.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.26516265273937567, Val Loss: 0.30503580675272624, Train F1: 0.6799706768963885, Val F1: 0.6527177738060994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rnn_model = RNN_model(len(word_to_idx), 300, 256, len(tag_to_ix), word2vec_embedding_mat, tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(rnn_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    train_loss_temp = 0\n",
    "    val_loss_temp = 0\n",
    "    train_f1_temp = 0\n",
    "    val_f1_temp = 0\n",
    "    rnn_model.train()\n",
    "    for case in tqdm(train_data):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        targets = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        rnn_model.zero_grad()\n",
    "        tag_scores = rnn_model(sentence)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp += loss.item()\n",
    "        train_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    train_loss.append(train_loss_temp/len(train_data))\n",
    "    train_f1.append(train_f1_temp/len(train_data))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        rnn_model.eval()\n",
    "        for case in tqdm(val_data):\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            tag_scores = rnn_model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            val_loss_temp += loss.item()\n",
    "            val_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    val_loss.append(val_loss_temp/len(val_data))\n",
    "    val_f1.append(val_f1_temp/len(val_data))\n",
    "\n",
    "    print(f'Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Train F1: {train_f1[-1]}, Val F1: {val_f1[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rnn_model, 'Non Trainable Embeddings/Word2vec+RNN/model.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Word2vec+RNN/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Word2vec+RNN/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_f1, open('Non Trainable Embeddings/Word2vec+RNN/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_f1, open('Non Trainable Embeddings/Word2vec+RNN/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert + RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:22<00:00, 96.96it/s] \n",
      "100%|██████████| 1416/1416 [00:04<00:00, 291.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7368409467210908, Val Loss: 0.6759775052331177, Train F1: 0.4277597931855873, Val F1: 0.4363632110328191\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:22<00:00, 96.97it/s] \n",
      "100%|██████████| 1416/1416 [00:04<00:00, 328.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6338107338778406, Val Loss: 0.6241690827859828, Train F1: 0.46883327355645327, Val F1: 0.46872462492834777\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:18<00:00, 102.77it/s]\n",
      "100%|██████████| 1416/1416 [00:04<00:00, 301.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5837268076949951, Val Loss: 0.5889268368666749, Train F1: 0.4958340108381039, Val F1: 0.4847640812893645\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:22<00:00, 96.62it/s] \n",
      "100%|██████████| 1416/1416 [00:05<00:00, 278.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5520310315170757, Val Loss: 0.5676649358928583, Train F1: 0.5151703970972682, Val F1: 0.49522527562232593\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:21<00:00, 97.81it/s] \n",
      "100%|██████████| 1416/1416 [00:04<00:00, 291.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5288441904446184, Val Loss: 0.5486583332110218, Train F1: 0.5269894172271448, Val F1: 0.5062500949197646\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:21<00:00, 98.78it/s] \n",
      "100%|██████████| 1416/1416 [00:04<00:00, 319.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5102261729334732, Val Loss: 0.5340848006058148, Train F1: 0.5382163843493825, Val F1: 0.5194841667735018\n",
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:21<00:00, 98.73it/s] \n",
      "100%|██████████| 1416/1416 [00:04<00:00, 315.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4927880446745838, Val Loss: 0.5229834659567029, Train F1: 0.5484856357378282, Val F1: 0.5284026581061378\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:21<00:00, 98.76it/s] \n",
      "100%|██████████| 1416/1416 [00:04<00:00, 322.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.47838883303430557, Val Loss: 0.5122087614355598, Train F1: 0.5566179090157893, Val F1: 0.5356131004121726\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:20<00:00, 99.94it/s] \n",
      "100%|██████████| 1416/1416 [00:04<00:00, 289.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4688579626056207, Val Loss: 0.5046447033633473, Train F1: 0.5635292667932726, Val F1: 0.5387408807515905\n",
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:18<00:00, 101.60it/s]\n",
      "100%|██████████| 1416/1416 [00:04<00:00, 308.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.45632064041517484, Val Loss: 0.49597533890651274, Train F1: 0.5704571205647169, Val F1: 0.5436559214550242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rnn_model = RNN_model(len(word_to_idx), 768, 512, len(tag_to_ix), bert_embedding_mat, tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(rnn_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    train_loss_temp = 0\n",
    "    val_loss_temp = 0\n",
    "    train_f1_temp = 0\n",
    "    val_f1_temp = 0\n",
    "    rnn_model.train()\n",
    "    for case in tqdm(train_data):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        targets = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        rnn_model.zero_grad()\n",
    "        tag_scores = rnn_model(sentence)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp += loss.item()\n",
    "        train_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    train_loss.append(train_loss_temp/len(train_data))\n",
    "    train_f1.append(train_f1_temp/len(train_data))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        rnn_model.eval()\n",
    "        for case in tqdm(val_data):\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            tag_scores = rnn_model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            val_loss_temp += loss.item()\n",
    "            val_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    val_loss.append(val_loss_temp/len(val_data))\n",
    "    val_f1.append(val_f1_temp/len(val_data))\n",
    "\n",
    "    print(f'Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Train F1: {train_f1[-1]}, Val F1: {val_f1[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rnn_model, 'Non Trainable Embeddings/Bert+RNN/model.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Bert+RNN/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Bert+RNN/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_f1, open('Non Trainable Embeddings/Bert+RNN/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_f1, open('Non Trainable Embeddings/Bert+RNN/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glove + GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [02:11<00:00, 60.95it/s]\n",
      "100%|██████████| 1416/1416 [00:07<00:00, 195.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6882539816075419, Val Loss: 0.5976716481796387, Train F1: 0.42870465145343867, Val F1: 0.4430916613604604\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [02:19<00:00, 57.65it/s]\n",
      "100%|██████████| 1416/1416 [00:06<00:00, 207.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5495924759435642, Val Loss: 0.5153488048957726, Train F1: 0.47836394664500254, Val F1: 0.48875410650824846\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [02:26<00:00, 54.74it/s]\n",
      "100%|██████████| 1416/1416 [00:06<00:00, 214.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.474431908247064, Val Loss: 0.452491026939571, Train F1: 0.5323216788718329, Val F1: 0.5428037358104196\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [02:20<00:00, 57.01it/s]\n",
      "100%|██████████| 1416/1416 [00:06<00:00, 204.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4226939980635995, Val Loss: 0.4179243547917544, Train F1: 0.5665807420094102, Val F1: 0.5700014063784595\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [02:30<00:00, 53.28it/s]\n",
      "100%|██████████| 1416/1416 [00:06<00:00, 208.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3920614507017979, Val Loss: 0.39534163706888914, Train F1: 0.5876207473111807, Val F1: 0.5884854919378038\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [02:26<00:00, 54.69it/s]\n",
      "100%|██████████| 1416/1416 [00:07<00:00, 185.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.37001058997610237, Val Loss: 0.37807923638455715, Train F1: 0.6025012219368726, Val F1: 0.6006151393876423\n",
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [02:32<00:00, 52.55it/s]\n",
      "100%|██████████| 1416/1416 [00:06<00:00, 203.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3524040918543519, Val Loss: 0.3645944263728774, Train F1: 0.6161772119032293, Val F1: 0.610336335187634\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [02:22<00:00, 56.47it/s]\n",
      "100%|██████████| 1416/1416 [00:06<00:00, 211.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3376779318998096, Val Loss: 0.35407042138165523, Train F1: 0.6265041783405458, Val F1: 0.618687691598818\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [02:15<00:00, 59.38it/s]\n",
      "100%|██████████| 1416/1416 [00:06<00:00, 219.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.325041548060872, Val Loss: 0.34575062517185584, Train F1: 0.6359955101217711, Val F1: 0.626447280659448\n",
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [02:27<00:00, 54.47it/s]\n",
      "100%|██████████| 1416/1416 [00:07<00:00, 196.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.31393000045901454, Val Loss: 0.33899746557787597, Train F1: 0.6445698320951921, Val F1: 0.6314090060799529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gru_model = GRU_model(len(word_to_idx), 300, 256, len(tag_to_ix), glove_embedding_mat, tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(gru_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    train_loss_temp = 0\n",
    "    val_loss_temp = 0\n",
    "    train_f1_temp = 0\n",
    "    val_f1_temp = 0\n",
    "    gru_model.train()\n",
    "    for case in tqdm(train_data):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        targets = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        gru_model.zero_grad()\n",
    "        tag_scores = gru_model(sentence)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp += loss.item()\n",
    "        train_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    train_loss.append(train_loss_temp/len(train_data))\n",
    "    train_f1.append(train_f1_temp/len(train_data))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        gru_model.eval()\n",
    "        for case in tqdm(val_data):\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            tag_scores = gru_model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            val_loss_temp += loss.item()\n",
    "            val_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    val_loss.append(val_loss_temp/len(val_data))\n",
    "    val_f1.append(val_f1_temp/len(val_data))\n",
    "\n",
    "    print(f'Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Train F1: {train_f1[-1]}, Val F1: {val_f1[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gru_model, 'Non Trainable Embeddings/Glove+GRU/model.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Glove+GRU/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Glove+GRU/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_f1, open('Non Trainable Embeddings/Glove+GRU/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_f1, open('Non Trainable Embeddings/Glove+GRU/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2vec + GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [02:26<00:00, 54.72it/s]\n",
      "100%|██████████| 1416/1416 [00:07<00:00, 182.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.737562996353968, Val Loss: 0.6298637887660945, Train F1: 0.4227261132052267, Val F1: 0.41879864694068475\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [02:25<00:00, 55.19it/s]\n",
      "100%|██████████| 1416/1416 [00:07<00:00, 201.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.57293946362301, Val Loss: 0.5307543434467072, Train F1: 0.4667838049308789, Val F1: 0.48871960265662123\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [02:32<00:00, 52.61it/s]\n",
      "100%|██████████| 1416/1416 [00:06<00:00, 214.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4934417404609714, Val Loss: 0.46526762644140746, Train F1: 0.5107597230959102, Val F1: 0.5227218723974749\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [02:28<00:00, 53.98it/s]\n",
      "100%|██████████| 1416/1416 [00:07<00:00, 200.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4372184418201015, Val Loss: 0.4215953272530917, Train F1: 0.545327936667112, Val F1: 0.5535032209425645\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [02:21<00:00, 56.70it/s]\n",
      "100%|██████████| 1416/1416 [00:06<00:00, 218.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.39919055633732276, Val Loss: 0.39206782220785524, Train F1: 0.5751892419151332, Val F1: 0.5789999833861168\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [02:20<00:00, 56.97it/s]\n",
      "100%|██████████| 1416/1416 [00:06<00:00, 203.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.37083544028278165, Val Loss: 0.3695542228412762, Train F1: 0.5982670078389624, Val F1: 0.5947915837945644\n",
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [02:20<00:00, 57.15it/s]\n",
      "100%|██████████| 1416/1416 [00:07<00:00, 199.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.34821875970447863, Val Loss: 0.3515243546245192, Train F1: 0.6172448474912244, Val F1: 0.6123087902573725\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [02:28<00:00, 54.11it/s]\n",
      "100%|██████████| 1416/1416 [00:07<00:00, 192.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3296885823105584, Val Loss: 0.33693415130675036, Train F1: 0.6300532342860933, Val F1: 0.62197680186225\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [02:21<00:00, 56.70it/s]\n",
      "100%|██████████| 1416/1416 [00:07<00:00, 198.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.31441248062531857, Val Loss: 0.325275576554813, Train F1: 0.6397966262046584, Val F1: 0.6283436745609907\n",
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [02:21<00:00, 56.74it/s]\n",
      "100%|██████████| 1416/1416 [00:07<00:00, 198.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.30172139871192627, Val Loss: 0.3159502736656742, Train F1: 0.6484249065592157, Val F1: 0.633736342605233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gru_model = GRU_model(len(word_to_idx), 300, 256, len(tag_to_ix), word2vec_embedding_mat, tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(gru_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    train_loss_temp = 0\n",
    "    val_loss_temp = 0\n",
    "    train_f1_temp = 0\n",
    "    val_f1_temp = 0\n",
    "    gru_model.train()\n",
    "    for case in tqdm(train_data):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        targets = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        gru_model.zero_grad()\n",
    "        tag_scores = gru_model(sentence)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp += loss.item()\n",
    "        train_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    train_loss.append(train_loss_temp/len(train_data))\n",
    "    train_f1.append(train_f1_temp/len(train_data))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        gru_model.eval()\n",
    "        for case in tqdm(val_data):\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            tag_scores = gru_model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            val_loss_temp += loss.item()\n",
    "            val_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    val_loss.append(val_loss_temp/len(val_data))\n",
    "    val_f1.append(val_f1_temp/len(val_data))\n",
    "\n",
    "    print(f'Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Train F1: {train_f1[-1]}, Val F1: {val_f1[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gru_model, 'Non Trainable Embeddings/Word2vec+GRU/model.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Word2vec+GRU/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Word2vec+GRU/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_f1, open('Non Trainable Embeddings/Word2vec+GRU/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_f1, open('Non Trainable Embeddings/Word2vec+GRU/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert + GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8019 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [04:16<00:00, 31.22it/s]\n",
      "100%|██████████| 1416/1416 [00:08<00:00, 159.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7718177673970524, Val Loss: 0.712550135905523, Train F1: 0.4236934122815464, Val F1: 0.42170276101007276\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [04:55<00:00, 27.14it/s]\n",
      "100%|██████████| 1416/1416 [00:11<00:00, 122.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6646527963973724, Val Loss: 0.6401108220972036, Train F1: 0.44142563517677624, Val F1: 0.4487380815411967\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [04:37<00:00, 28.87it/s]\n",
      "100%|██████████| 1416/1416 [00:09<00:00, 144.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6034736071032343, Val Loss: 0.5962978212312811, Train F1: 0.47328289447101607, Val F1: 0.4715909469108296\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [04:50<00:00, 27.59it/s]\n",
      "100%|██████████| 1416/1416 [00:15<00:00, 90.91it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5652169087843758, Val Loss: 0.5675093091400814, Train F1: 0.49156399682866636, Val F1: 0.4845472168531068\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [04:14<00:00, 31.49it/s]\n",
      "100%|██████████| 1416/1416 [00:08<00:00, 169.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5380313842882445, Val Loss: 0.5463344408054495, Train F1: 0.507321971787787, Val F1: 0.4958758372142531\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [03:53<00:00, 34.31it/s]\n",
      "100%|██████████| 1416/1416 [00:08<00:00, 159.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5163939018439502, Val Loss: 0.5297214849628403, Train F1: 0.5208051627273991, Val F1: 0.5034881854134534\n",
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [04:43<00:00, 28.25it/s]\n",
      "100%|██████████| 1416/1416 [00:10<00:00, 131.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.49807887467905165, Val Loss: 0.5163377769512393, Train F1: 0.5335578044374705, Val F1: 0.5107599031044531\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [05:22<00:00, 24.90it/s]\n",
      "100%|██████████| 1416/1416 [00:12<00:00, 115.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4821960475611798, Val Loss: 0.5054146561108656, Train F1: 0.5432516232031239, Val F1: 0.5186259280894187\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [04:03<00:00, 32.91it/s]\n",
      "100%|██████████| 1416/1416 [00:08<00:00, 158.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4682953209257489, Val Loss: 0.4963482150809292, Train F1: 0.5514721365929035, Val F1: 0.5250640637391422\n",
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [03:39<00:00, 36.51it/s]\n",
      "100%|██████████| 1416/1416 [00:08<00:00, 166.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.45596104070301746, Val Loss: 0.4886721015873623, Train F1: 0.5595118671059827, Val F1: 0.5309913574711819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gru_model = GRU_model(len(word_to_idx), 768, 512, len(tag_to_ix), bert_embedding_mat, tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(gru_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    train_loss_temp = 0\n",
    "    val_loss_temp = 0\n",
    "    train_f1_temp = 0\n",
    "    val_f1_temp = 0\n",
    "    gru_model.train()\n",
    "    for case in tqdm(train_data):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        targets = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        gru_model.zero_grad()\n",
    "        tag_scores = gru_model(sentence)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp += loss.item()\n",
    "        train_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    train_loss.append(train_loss_temp/len(train_data))\n",
    "    train_f1.append(train_f1_temp/len(train_data))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        gru_model.eval()\n",
    "        for case in tqdm(val_data):\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            tag_scores = gru_model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            val_loss_temp += loss.item()\n",
    "            val_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    val_loss.append(val_loss_temp/len(val_data))\n",
    "    val_f1.append(val_f1_temp/len(val_data))\n",
    "\n",
    "    print(f'Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Train F1: {train_f1[-1]}, Val F1: {val_f1[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gru_model, 'Non Trainable Embeddings/Bert+GRU/model.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Bert+GRU/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Bert+GRU/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_f1, open('Non Trainable Embeddings/Bert+GRU/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_f1, open('Non Trainable Embeddings/Bert+GRU/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glove + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [00:54<00:00, 147.82it/s]\n",
      "100%|██████████| 1416/1416 [00:05<00:00, 280.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7460598774406, Val Loss: 0.6544797988647942, Train F1: 0.42508995751908224, Val F1: 0.4216943793316018\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:01<00:00, 129.50it/s]\n",
      "100%|██████████| 1416/1416 [00:05<00:00, 246.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.611286590423444, Val Loss: 0.5812828869773943, Train F1: 0.44119222107682715, Val F1: 0.45504164818478976\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:05<00:00, 121.58it/s]\n",
      "100%|██████████| 1416/1416 [00:05<00:00, 245.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5451993814715482, Val Loss: 0.5141528328950714, Train F1: 0.4784273578063643, Val F1: 0.49307966444325396\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:08<00:00, 116.27it/s]\n",
      "100%|██████████| 1416/1416 [00:06<00:00, 229.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.48119108553700335, Val Loss: 0.46045840614574624, Train F1: 0.5172136748517872, Val F1: 0.5274958640883904\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:09<00:00, 116.03it/s]\n",
      "100%|██████████| 1416/1416 [00:05<00:00, 238.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.43566570920179376, Val Loss: 0.4272615499736111, Train F1: 0.5513706001105356, Val F1: 0.556485275577772\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:07<00:00, 118.14it/s]\n",
      "100%|██████████| 1416/1416 [00:05<00:00, 272.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.40367328855518564, Val Loss: 0.4032549147963087, Train F1: 0.575762640984738, Val F1: 0.574476640223407\n",
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:04<00:00, 123.85it/s]\n",
      "100%|██████████| 1416/1416 [00:06<00:00, 232.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.38053527020649297, Val Loss: 0.3877811541474664, Train F1: 0.5916291290045934, Val F1: 0.5873142158249975\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:08<00:00, 116.95it/s]\n",
      "100%|██████████| 1416/1416 [00:04<00:00, 285.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.36223064416297013, Val Loss: 0.37481622979499396, Train F1: 0.6019901011110216, Val F1: 0.596579407022704\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:01<00:00, 130.60it/s]\n",
      "100%|██████████| 1416/1416 [00:04<00:00, 285.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.34632849099186774, Val Loss: 0.36451842268310797, Train F1: 0.6137311961358733, Val F1: 0.607048867482402\n",
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:00<00:00, 132.55it/s]\n",
      "100%|██████████| 1416/1416 [00:04<00:00, 290.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.332572354783671, Val Loss: 0.35643205488945645, Train F1: 0.624278752916603, Val F1: 0.6170964888249129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_model = LSTM_model(len(word_to_idx), 300, 256, len(tag_to_ix), glove_embedding_mat, tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(lstm_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    train_loss_temp = 0\n",
    "    val_loss_temp = 0\n",
    "    train_f1_temp = 0\n",
    "    val_f1_temp = 0\n",
    "    lstm_model.train()\n",
    "    for case in tqdm(train_data):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        targets = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        lstm_model.zero_grad()\n",
    "        tag_scores = lstm_model(sentence)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp += loss.item()\n",
    "        train_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    train_loss.append(train_loss_temp/len(train_data))\n",
    "    train_f1.append(train_f1_temp/len(train_data))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        lstm_model.eval()\n",
    "        for case in tqdm(val_data):\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            tag_scores = lstm_model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            val_loss_temp += loss.item()\n",
    "            val_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    val_loss.append(val_loss_temp/len(val_data))\n",
    "    val_f1.append(val_f1_temp/len(val_data))\n",
    "\n",
    "    print(f'Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Train F1: {train_f1[-1]}, Val F1: {val_f1[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm_model, 'Non Trainable Embeddings/Glove+LSTM/model.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Glove+LSTM/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Glove+LSTM/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_f1, open('Non Trainable Embeddings/Glove+LSTM/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_f1, open('Non Trainable Embeddings/Glove+LSTM/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2vec + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:02<00:00, 128.69it/s]\n",
      "100%|██████████| 1416/1416 [00:04<00:00, 299.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8227432623100498, Val Loss: 0.6962273388464362, Train F1: 0.42253654090304393, Val F1: 0.4191522214298386\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:01<00:00, 131.24it/s]\n",
      "100%|██████████| 1416/1416 [00:04<00:00, 289.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6502300609249382, Val Loss: 0.6238453137049699, Train F1: 0.4263505287644238, Val F1: 0.42403403350740615\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:02<00:00, 128.13it/s]\n",
      "100%|██████████| 1416/1416 [00:04<00:00, 292.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5865603690773127, Val Loss: 0.5654897332332218, Train F1: 0.4473758342387352, Val F1: 0.4622641498921814\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:02<00:00, 129.02it/s]\n",
      "100%|██████████| 1416/1416 [00:04<00:00, 290.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5323489941225131, Val Loss: 0.5103811479124601, Train F1: 0.48157647217528604, Val F1: 0.49081135755915\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:01<00:00, 130.22it/s]\n",
      "100%|██████████| 1416/1416 [00:04<00:00, 289.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.479845833732062, Val Loss: 0.46069785070672664, Train F1: 0.5081355938904248, Val F1: 0.5171208162919425\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:01<00:00, 129.52it/s]\n",
      "100%|██████████| 1416/1416 [00:04<00:00, 290.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.43604966501574927, Val Loss: 0.4222450606076219, Train F1: 0.5392481896692126, Val F1: 0.5512403706508415\n",
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:04<00:00, 124.11it/s]\n",
      "100%|██████████| 1416/1416 [00:04<00:00, 289.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.40142806201545295, Val Loss: 0.39359480250746176, Train F1: 0.566758820439511, Val F1: 0.5725770479065012\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:02<00:00, 127.71it/s]\n",
      "100%|██████████| 1416/1416 [00:04<00:00, 284.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.37480780907984057, Val Loss: 0.37224403546700824, Train F1: 0.5864893015303219, Val F1: 0.5866231054262977\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:02<00:00, 129.01it/s]\n",
      "100%|██████████| 1416/1416 [00:05<00:00, 268.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3529992197997122, Val Loss: 0.3556284400818052, Train F1: 0.6030219802336886, Val F1: 0.6005365972403784\n",
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [01:02<00:00, 127.65it/s]\n",
      "100%|██████████| 1416/1416 [00:06<00:00, 207.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3349447936105953, Val Loss: 0.3429008135387505, Train F1: 0.6171922774295108, Val F1: 0.6119369816960177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_model = LSTM_model(len(word_to_idx), 300, 256, len(tag_to_ix), word2vec_embedding_mat, tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(lstm_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    train_loss_temp = 0\n",
    "    val_loss_temp = 0\n",
    "    train_f1_temp = 0\n",
    "    val_f1_temp = 0\n",
    "    lstm_model.train()\n",
    "    for case in tqdm(train_data):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        targets = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        lstm_model.zero_grad()\n",
    "        tag_scores = lstm_model(sentence)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp += loss.item()\n",
    "        train_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    train_loss.append(train_loss_temp/len(train_data))\n",
    "    train_f1.append(train_f1_temp/len(train_data))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        lstm_model.eval()\n",
    "        for case in tqdm(val_data):\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            tag_scores = lstm_model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            val_loss_temp += loss.item()\n",
    "            val_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    val_loss.append(val_loss_temp/len(val_data))\n",
    "    val_f1.append(val_f1_temp/len(val_data))\n",
    "\n",
    "    print(f'Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Train F1: {train_f1[-1]}, Val F1: {val_f1[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm_model, 'Non Trainable Embeddings/Word2vec+LSTM/model.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Word2vec+LSTM/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Word2vec+LSTM/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_f1, open('Non Trainable Embeddings/Word2vec+LSTM/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_f1, open('Non Trainable Embeddings/Word2vec+LSTM/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [03:08<00:00, 42.51it/s]\n",
      "100%|██████████| 1416/1416 [00:11<00:00, 127.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8090987489394419, Val Loss: 0.7614968724859258, Train F1: 0.4218863639172591, Val F1: 0.41721149948390157\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [03:06<00:00, 42.94it/s]\n",
      "100%|██████████| 1416/1416 [00:12<00:00, 115.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7181812747980791, Val Loss: 0.707030585882817, Train F1: 0.4275592383267264, Val F1: 0.4230149647993177\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [03:15<00:00, 40.99it/s]\n",
      "100%|██████████| 1416/1416 [00:12<00:00, 110.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6607951759067479, Val Loss: 0.65577495609026, Train F1: 0.43772948992759475, Val F1: 0.43677108179533564\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [03:29<00:00, 38.32it/s]\n",
      "100%|██████████| 1416/1416 [00:12<00:00, 113.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.613725942568483, Val Loss: 0.6140029739370139, Train F1: 0.46123876291067567, Val F1: 0.4583928132171114\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [03:09<00:00, 42.26it/s]\n",
      "100%|██████████| 1416/1416 [00:18<00:00, 74.90it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5776995284269605, Val Loss: 0.5836308524933365, Train F1: 0.48168950027618285, Val F1: 0.4744164385757485\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [03:08<00:00, 42.55it/s]\n",
      "100%|██████████| 1416/1416 [00:08<00:00, 157.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5504277543694269, Val Loss: 0.5608321339599683, Train F1: 0.49485250182167684, Val F1: 0.4852158981371446\n",
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [02:44<00:00, 48.64it/s]\n",
      "100%|██████████| 1416/1416 [00:09<00:00, 147.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5282529647711073, Val Loss: 0.543449678773976, Train F1: 0.5062130914485871, Val F1: 0.49412449231447125\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [02:44<00:00, 48.84it/s]\n",
      "100%|██████████| 1416/1416 [00:09<00:00, 153.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5096635013008204, Val Loss: 0.529840325638073, Train F1: 0.5178209172023773, Val F1: 0.4990178079725728\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [02:40<00:00, 49.87it/s]\n",
      "100%|██████████| 1416/1416 [00:09<00:00, 150.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.49355230407467876, Val Loss: 0.5186011874883851, Train F1: 0.5295591679044606, Val F1: 0.5092878348683654\n",
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8019/8019 [02:47<00:00, 47.76it/s]\n",
      "100%|██████████| 1416/1416 [00:09<00:00, 147.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.47902098420054123, Val Loss: 0.5082034010027067, Train F1: 0.5395419626211806, Val F1: 0.5164417034263564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_model = LSTM_model(len(word_to_idx), 768, 512, len(tag_to_ix), bert_embedding_mat, tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(lstm_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    train_loss_temp = 0\n",
    "    val_loss_temp = 0\n",
    "    train_f1_temp = 0\n",
    "    val_f1_temp = 0\n",
    "    lstm_model.train()\n",
    "    for case in tqdm(train_data):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        targets = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        lstm_model.zero_grad()\n",
    "        tag_scores = lstm_model(sentence)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp += loss.item()\n",
    "        train_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    train_loss.append(train_loss_temp/len(train_data))\n",
    "    train_f1.append(train_f1_temp/len(train_data))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        lstm_model.eval()\n",
    "        for case in tqdm(val_data):\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            tag_scores = lstm_model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            val_loss_temp += loss.item()\n",
    "            val_f1_temp += f1_score(targets.detach().numpy(), torch.argmax(tag_scores, dim=1).detach().numpy(), average='macro')\n",
    "    val_loss.append(val_loss_temp/len(val_data))\n",
    "    val_f1.append(val_f1_temp/len(val_data))\n",
    "\n",
    "    print(f'Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Train F1: {train_f1[-1]}, Val F1: {val_f1[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm_model, 'Non Trainable Embeddings/Bert+LSTM/model.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Bert+LSTM/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Bert+LSTM/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_f1, open('Non Trainable Embeddings/Bert+LSTM/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_f1, open('Non Trainable Embeddings/Bert+LSTM/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
