{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x25b9facc850>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BiLSTM CRF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, target_size, embedding_mat, start_tag, end_tag, tag_to_ix, batch_size=1, device='cpu'):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.target_size = target_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_mat)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size)\n",
    "\n",
    "        self.transitions_to = nn.Parameter(torch.randn(target_size, target_size))\n",
    "        self.transitions_to.data[start_tag, :] = -10000\n",
    "        self.transitions_to.data[:, end_tag] = -10000\n",
    "\n",
    "        self.transitions_from = nn.Parameter(torch.randn(target_size, target_size))\n",
    "        self.transitions_from.data[:, start_tag] = -10000\n",
    "        self.transitions_from.data[end_tag, :] = -10000\n",
    "\n",
    "        self.hidden = (torch.randn(2, 1, hidden_dim // 2),\n",
    "                       torch.randn(2, 1, hidden_dim // 2))\n",
    "        \n",
    "\n",
    "    def get_lstm_features(self, sentence):\n",
    "        embeds = self.embedding(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "\n",
    "\n",
    "    def _forward_algo(self, lstm_features):\n",
    "\n",
    "        scores = torch.full((1, self.target_size), -10000.).to(self.device)\n",
    "        scores[0][self.tag_to_ix['START_TAG']] = 0.\n",
    "\n",
    "        forward_var = scores\n",
    "\n",
    "        for feat in lstm_features:\n",
    "            next_tag_var = self.transitions_to + feat.view(-1, 1).expand(-1, self.target_size) + forward_var.expand(self.target_size, -1)\n",
    "            #use log sum exp to avoid underflow\n",
    "            max_score = next_tag_var.max()\n",
    "            next_tag_var = next_tag_var - max_score\n",
    "            forward_var = max_score + torch.logsumexp(next_tag_var, dim=0).view(1, -1)\n",
    "            \n",
    "        terminal_var = (forward_var + self.transitions_to[self.tag_to_ix['END_TAG']]).view(1, -1)\n",
    "        alpha = terminal_var\n",
    "        return alpha\n",
    "    \n",
    "\n",
    "    def _score_sentence(self, lstm_features, tags):\n",
    "        score = torch.zeros(1).to(self.device)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix['START_TAG']], dtype=torch.long).to(self.device), tags])\n",
    "        for i, feat in enumerate(lstm_features):\n",
    "            score += self.transitions_to[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "                \n",
    "        score += self.transitions_to[self.tag_to_ix['END_TAG'], tags[-1]]\n",
    "        return score\n",
    "    \n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        lstm_feats = self.get_lstm_features(sentence)\n",
    "        forward_score = self._forward_algo(lstm_feats)\n",
    "        gold_score = self._score_sentence(lstm_feats, tags)\n",
    "        return forward_score - gold_score\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11])\n",
      "torch.Size([11, 1, 5])\n",
      "torch.Size([11, 1, 4])\n",
      "torch.Size([11, 4])\n",
      "torch.Size([11, 5])\n"
     ]
    }
   ],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "EMBEDDING_DIM = 5\n",
    "HIDDEN_DIM = 4\n",
    "\n",
    "# Make up some training data\n",
    "training_data = [(\n",
    "    \"the wall street journal reported today that apple corporation made money\".split(),\n",
    "    \"B I I I O O O B I O O\".split()\n",
    "), (\n",
    "    \"georgia tech is a university in georgia\".split(),\n",
    "    \"B I O O O O B\".split()\n",
    ")]\n",
    "\n",
    "word_to_ix = {}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}\n",
    "\n",
    "model = BiLSTM_CRF(len(word_to_ix), 5, HIDDEN_DIM, len(tag_to_ix), torch.randn(len(word_to_ix), 5), tag_to_ix[START_TAG], tag_to_ix[STOP_TAG])\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "sentence_in = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "print(sentence_in.shape)\n",
    "print(model.get_lstm_features(sentence_in).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0, -1, -1, -1, -1],\n",
       "         [ 0, -1, -1, -1, -1],\n",
       "         [ 0, -1, -1, -1, -1],\n",
       "         [ 0, -1, -1, -1, -1],\n",
       "         [ 0, -1, -1, -1, -1]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = torch.full((1, 5), -1)\n",
    "temp[0][0] = 0\n",
    "temp.unsqueeze(1).expand(-1, 5, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04],\n",
      "        [-1.6608e+00, -1.1081e+00,  1.1770e+00, -1.0175e-02, -1.0000e+04],\n",
      "        [ 5.8099e-01, -1.4000e+00,  2.3599e-01,  4.4388e-01, -1.0000e+04],\n",
      "        [-1.2586e+00, -6.9026e-01, -7.9675e-01,  3.1644e-01, -1.0000e+04],\n",
      "        [ 1.4568e+00,  9.1951e-01,  3.5823e-01,  4.0518e-01, -1.0000e+04]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "transition = nn.Parameter(torch.randn(5, 5))\n",
    "transition.data[0, :] = -10000\n",
    "transition.data[:, 4] = -10000\n",
    "\n",
    "print(transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0000e+04, -1.0001e+04, -1.0001e+04, -1.0001e+04, -1.0001e+04],\n",
       "         [-1.6608e+00, -2.1081e+00,  1.7705e-01, -1.0102e+00, -1.0001e+04],\n",
       "         [ 5.8099e-01, -2.4000e+00, -7.6401e-01, -5.5612e-01, -1.0001e+04],\n",
       "         [-1.2586e+00, -1.6903e+00, -1.7967e+00, -6.8356e-01, -1.0001e+04],\n",
       "         [ 1.4568e+00, -8.0491e-02, -6.4177e-01, -5.9482e-01, -1.0001e+04]],\n",
       "\n",
       "        [[-1.0000e+04, -1.0001e+04, -1.0001e+04, -1.0001e+04, -1.0001e+04],\n",
       "         [-1.6608e+00, -2.1081e+00,  1.7705e-01, -1.0102e+00, -1.0001e+04],\n",
       "         [ 5.8099e-01, -2.4000e+00, -7.6401e-01, -5.5612e-01, -1.0001e+04],\n",
       "         [-1.2586e+00, -1.6903e+00, -1.7967e+00, -6.8356e-01, -1.0001e+04],\n",
       "         [ 1.4568e+00, -8.0491e-02, -6.4177e-01, -5.9482e-01, -1.0001e+04]],\n",
       "\n",
       "        [[-1.0000e+04, -1.0001e+04, -1.0001e+04, -1.0001e+04, -1.0001e+04],\n",
       "         [-1.6608e+00, -2.1081e+00,  1.7705e-01, -1.0102e+00, -1.0001e+04],\n",
       "         [ 5.8099e-01, -2.4000e+00, -7.6401e-01, -5.5612e-01, -1.0001e+04],\n",
       "         [-1.2586e+00, -1.6903e+00, -1.7967e+00, -6.8356e-01, -1.0001e+04],\n",
       "         [ 1.4568e+00, -8.0491e-02, -6.4177e-01, -5.9482e-01, -1.0001e+04]],\n",
       "\n",
       "        [[-1.0000e+04, -1.0001e+04, -1.0001e+04, -1.0001e+04, -1.0001e+04],\n",
       "         [-1.6608e+00, -2.1081e+00,  1.7705e-01, -1.0102e+00, -1.0001e+04],\n",
       "         [ 5.8099e-01, -2.4000e+00, -7.6401e-01, -5.5612e-01, -1.0001e+04],\n",
       "         [-1.2586e+00, -1.6903e+00, -1.7967e+00, -6.8356e-01, -1.0001e+04],\n",
       "         [ 1.4568e+00, -8.0491e-02, -6.4177e-01, -5.9482e-01, -1.0001e+04]],\n",
       "\n",
       "        [[-1.0000e+04, -1.0001e+04, -1.0001e+04, -1.0001e+04, -1.0001e+04],\n",
       "         [-1.6608e+00, -2.1081e+00,  1.7705e-01, -1.0102e+00, -1.0001e+04],\n",
       "         [ 5.8099e-01, -2.4000e+00, -7.6401e-01, -5.5612e-01, -1.0001e+04],\n",
       "         [-1.2586e+00, -1.6903e+00, -1.7967e+00, -6.8356e-01, -1.0001e+04],\n",
       "         [ 1.4568e+00, -8.0491e-02, -6.4177e-01, -5.9482e-01, -1.0001e+04]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition.unsqueeze(0).expand(5, -1, -1) + temp.unsqueeze(1).expand(-1, 5, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.0919,  0.5058,  1.3415,  1.5653, -1.1470])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0919,  1.0919,  1.0919,  1.0919,  1.0919],\n",
       "        [ 0.5058,  0.5058,  0.5058,  0.5058,  0.5058],\n",
       "        [ 1.3415,  1.3415,  1.3415,  1.3415,  1.3415],\n",
       "        [ 1.5653,  1.5653,  1.5653,  1.5653,  1.5653],\n",
       "        [-1.1470, -1.1470, -1.1470, -1.1470, -1.1470]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat = torch.randn(5)\n",
    "print(feat)\n",
    "\n",
    "feat.view(5, 1).expand(-1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
