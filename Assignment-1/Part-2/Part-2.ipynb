{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "C:\\Users\\Dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\pipelines\\text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from utils import emotion_scores\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLM:\n",
    "    def __init__(self):\n",
    "        self.vocab = set()\n",
    "        self.bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.unigram_counts = defaultdict(int)\n",
    "        self.bigram_probs = None\n",
    "        self.beta_values = None\n",
    "        self.emotion_dict = {'sadness': 0, 'joy': 1, 'love': 2, 'anger': 3, 'fear': 4, 'surprise': 5}\n",
    "\n",
    "    def learn_from_dataset(self, dataset):\n",
    "        for sentence in dataset:\n",
    "            tokens = sentence.split()                        \n",
    "            for i in range(len(tokens) - 1):\n",
    "                word1, word2 = tokens[i], tokens[i + 1]\n",
    "                self.vocab.add(word1)\n",
    "                self.vocab.add(word2)\n",
    "                self.bigram_counts[word1][word2] += 1\n",
    "                self.unigram_counts[word1] += 1                            \n",
    "                \n",
    "        self.vocab = list(self.vocab)\n",
    "        print(f\"Vocabulary size: {len(self.vocab)}\")\n",
    "\n",
    "    def calculate_beta_values(self):\n",
    "            num_words = len(self.vocab)\n",
    "            self.beta_values = np.zeros((num_words, num_words, 6))\n",
    "            for i, word1 in tqdm(enumerate(self.vocab)):\n",
    "                if word1 not in self.bigram_counts.keys():\n",
    "                    continue\n",
    "                for j, word2 in enumerate(self.vocab):\n",
    "                    if word2 not in self.bigram_counts[word1].keys():\n",
    "                        continue\n",
    "                    emotions = emotion_scores(word1 + \" \" + word2)\n",
    "                    self.beta_values[i][j] = np.array([emotions[k]['score'] for k in range(6)])\n",
    "                    \n",
    "    def calculate_bigram_probs(self):\n",
    "        num_words = len(self.vocab)\n",
    "        self.bigram_probs = np.zeros((num_words, num_words))        \n",
    "        \n",
    "        for i, word1 in tqdm(enumerate(self.vocab)):\n",
    "            for j, word2 in enumerate(self.vocab):\n",
    "                if self.unigram_counts[word1] > 0:\n",
    "                    self.bigram_probs[i][j] = float(self.bigram_counts[word1][word2]) / float(self.unigram_counts[word1])\n",
    "                    # if self.bigram_probs[i][j] == 0:\n",
    "                    #     continue\n",
    "                    # emotions = emotion_scores(word1 + \" \" + word2)\n",
    "                    # self.beta_values[i][j] = np.array([emotions[k]['score'] for k in range(6)])\n",
    "\n",
    "                    \n",
    "    def calculate_bigram_probs_laplace(self):\n",
    "        num_words = len(self.vocab)\n",
    "        self.bigram_probs = np.zeros((num_words, num_words))\n",
    "\n",
    "        for i, word1 in enumerate(self.vocab):\n",
    "            for j, word2 in enumerate(self.vocab):\n",
    "                self.bigram_probs[i][j] = (self.bigram_counts[word1][word2] + 1) / (self.unigram_counts[word1] + num_words)\n",
    "\n",
    "                # if self.bigram_probs[i][j] == 0:\n",
    "                #     continue\n",
    "                # emotions = emotion_scores(word1 + \" \" + word2)\n",
    "                # self.beta_values[i][j] = np.array([emotions[k]['score'] for k in range(6)])\n",
    "                \n",
    "    \n",
    "    def calculate_bigram_probs_kneser_ney(self, discount=0.75):\n",
    "        num_words = len(self.vocab)\n",
    "        self.bigram_probs = np.zeros((num_words, num_words))\n",
    "\n",
    "        continuation_counts = defaultdict(set)\n",
    "        # Calculate continuation counts\n",
    "        for word1, word2_dict in self.bigram_counts.items():\n",
    "            for word2 in word2_dict:\n",
    "                continuation_counts[word2].add(word1)\n",
    "\n",
    "        # Total number of word1 that can precede any word2        \n",
    "        total_continuations = {word2: len(word1s) for word2, word1s in continuation_counts.items()}\n",
    "\n",
    "        for i, word1 in tqdm(enumerate(self.vocab)):\n",
    "            sum_adjusted_counts = sum(max(self.bigram_counts[word1][word2] - discount, 0) for word2 in self.vocab)\n",
    "            for j, word2 in enumerate(self.vocab):\n",
    "                if word2 in total_continuations.keys():\n",
    "                    adjusted_count = max(self.bigram_counts[word1][word2] - discount, 0)\n",
    "                    continuation_prob = total_continuations[word2] / sum(total_continuations.values()) if sum(total_continuations.values()) > 0 else 0\n",
    "                    lower_order_weight = (discount * continuation_prob) / self.unigram_counts[word1] if self.unigram_counts[word1] > 0 else 0\n",
    "\n",
    "                    self.bigram_probs[i][j] = adjusted_count / self.unigram_counts[word1] + lower_order_weight if self.unigram_counts[word1] > 0 else 0\n",
    "\n",
    "                    # if self.bigram_probs[i][j] == 0:\n",
    "                    #     continue\n",
    "                    # emotions = emotion_scores(word1 + \" \" + word2)\n",
    "                    # self.beta_values[i][j] = np.array([emotions[k]['score'] for k in range(6)])\n",
    "                    \n",
    "\n",
    "    # def generate_sentence(self, emotion, max_length=20):\n",
    "    #     sentence = []\n",
    "    #     current_word = random.choice(['i', 'im'])\n",
    "    #     sentence.append(current_word)\n",
    "\n",
    "    #     for i in range(max_length):\n",
    "    #         current_word = self.generate_next_word(current_word, emotion)\n",
    "    #         if current_word == \"\":\n",
    "    #             break\n",
    "    #         sentence.append(current_word)\n",
    "        \n",
    "    #     return \" \".join(sentence)\n",
    "\n",
    "    # def generate_next_word(self, current_word, emotion):\n",
    "    #     if current_word not in self.vocab:\n",
    "    #         raise ValueError(f\"{current_word} not found in the vocabulary.\")\n",
    "\n",
    "    #     word_index = self.vocab.index(current_word)\n",
    "    #     next_word_probs = self.bigram_probs[word_index] + self.beta_values[word_index, :, self.emotion_dict[emotion]]\n",
    "\n",
    "    #     # next_word_index = list(next_word_probs).index(max(next_word_probs))\n",
    "    #     try:\n",
    "    #         next_word_index = random.choices(range(len(next_word_probs)), weights=next_word_probs)[0]\n",
    "    #     except:\n",
    "    #         return \"\"\n",
    "\n",
    "    #     next_word = list(self.vocab)[next_word_index]\n",
    "\n",
    "    #     return next_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('../Dataset/corpus.txt')\n",
    "dataset = []\n",
    "for i in corpus.readlines():\n",
    "    dataset.append('<SOS> ' + i + ' <EOS>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5431\n"
     ]
    }
   ],
   "source": [
    "model = BigramLM()\n",
    "model.learn_from_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model.vocab, open('Checkpoints/vocab.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5431it [08:38, 10.47it/s]\n"
     ]
    }
   ],
   "source": [
    "model.calculate_beta_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model.beta_values, open('Checkpoints/beta_values.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5431it [00:16, 321.77it/s]\n"
     ]
    }
   ],
   "source": [
    "model.calculate_bigram_probs()\n",
    "pickle.dump(model.bigram_probs, open('Checkpoints/bigram_probs.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.calculate_bigram_probs_kneser_ney()\n",
    "pickle.dump(model.bigram_probs, open('Checkpoints/bigram_probs_kneser_ney.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.calculate_bigram_probs_laplace()\n",
    "pickle.dump(model.bigram_probs, open('Checkpoints/bigram_probs_laplace.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 50 sentences for each emotion ansd write to a file with name gen_emotion.txt\n",
    "for emotion in ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']:\n",
    "    with open('Test Samples/coeff_0.1_no/gen_' + emotion + '.txt', 'w') as f:\n",
    "        for i in range(50):\n",
    "            f.write(model.generate_sentence(emotion) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sadness: 40\n",
      "joy: 40\n",
      "love: 36\n",
      "anger: 30\n",
      "fear: 33\n",
      "surprise: 36\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "with open('Test Samples/coeff_0.1_no/gen_sadness.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        score = emotion_scores(line)[model.emotion_dict['sadness']]['score']\n",
    "        \n",
    "        if(score > 0.5):\n",
    "            count += 1\n",
    "\n",
    "print('sadness: ' + str(count))\n",
    "\n",
    "count = 0\n",
    "with open('Test Samples/coeff_0.1_no/gen_joy.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        score = emotion_scores(line)[model.emotion_dict['joy']]['score']\n",
    "        \n",
    "        if(score > 0.5):\n",
    "            count += 1\n",
    "\n",
    "print('joy: ' + str(count))\n",
    "\n",
    "count = 0\n",
    "with open('Test Samples/coeff_0.1_no/gen_love.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        score = emotion_scores(line)[model.emotion_dict['love']]['score']\n",
    "        \n",
    "        if(score > 0.5):\n",
    "            count += 1\n",
    "\n",
    "print('love: ' + str(count))\n",
    "\n",
    "count = 0\n",
    "with open('Test Samples/coeff_0.1_no/gen_anger.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        score = emotion_scores(line)[model.emotion_dict['anger']]['score']\n",
    "        \n",
    "        if(score > 0.5):\n",
    "            count += 1\n",
    "\n",
    "print('anger: ' + str(count))\n",
    "\n",
    "count = 0\n",
    "with open('Test Samples/coeff_0.1_no/gen_fear.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        score = emotion_scores(line)[model.emotion_dict['fear']]['score']\n",
    "        \n",
    "        if(score > 0.5):\n",
    "            count += 1\n",
    "\n",
    "print('fear: ' + str(count))\n",
    "\n",
    "count = 0\n",
    "with open('Test Samples/coeff_0.1_no/gen_surprise.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        score = emotion_scores(line)[model.emotion_dict['surprise']]['score']\n",
    "        \n",
    "        if(score > 0.5):\n",
    "            count += 1\n",
    "\n",
    "print('surprise: ' + str(count))\n",
    "\n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
