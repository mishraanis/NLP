{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x19be1eb3790>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = json.load(open('../Dataset/BIO_Tagged/ATE_train.json', 'r'))\n",
    "test_data = json.load(open('../Dataset/BIO_Tagged/ATE_test.json', 'r'))\n",
    "val_data = json.load(open('../Dataset/BIO_Tagged/ATE_val.json', 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word to Index and Tag to Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {}\n",
    "\n",
    "for case in train_data:\n",
    "    for text in train_data[case]['text'].split(' '):\n",
    "        if text not in word_to_idx:\n",
    "            word_to_idx[text] = len(word_to_idx)\n",
    "\n",
    "for case in test_data:\n",
    "    for text in test_data[case]['text'].split(' '):\n",
    "        if text not in word_to_idx:\n",
    "            word_to_idx[text] = len(word_to_idx)\n",
    "\n",
    "for case in val_data:\n",
    "    for text in val_data[case]['text'].split(' '):\n",
    "        if text not in word_to_idx:\n",
    "            word_to_idx[text] = len(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_to_ix = {}\n",
    "\n",
    "for case in train_data:\n",
    "    for tag in train_data[case]['labels']:\n",
    "        if tag not in tag_to_ix:\n",
    "            tag_to_ix[tag] = len(tag_to_ix)\n",
    "\n",
    "for case in test_data:\n",
    "    for tag in test_data[case]['labels']:\n",
    "        if tag not in tag_to_ix:\n",
    "            tag_to_ix[tag] = len(tag_to_ix)\n",
    "\n",
    "for case in val_data:\n",
    "    for tag in val_data[case]['labels']:\n",
    "        if tag not in tag_to_ix:\n",
    "            tag_to_ix[tag] = len(tag_to_ix)\n",
    "\n",
    "tag_to_ix['START_TAG'] = len(tag_to_ix)\n",
    "tag_to_ix['END_TAG'] = len(tag_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(word_to_idx, open('word_to_idx.pkl', 'wb'))\n",
    "pickle.dump(tag_to_ix, open('tag_to_ix.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = pickle.load(open('word_to_idx.pkl', 'rb'))\n",
    "tag_to_idx = pickle.load(open('tag_to_ix.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Bert Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('nlpaueb/legal-bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3495/3495 [02:08<00:00, 27.25it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_mat = np.zeros((len(word_to_idx), 768))\n",
    "\n",
    "for word, idx in tqdm(word_to_idx.items()):\n",
    "    # if word in tokenizer.vocab:\n",
    "    #     embedding_mat[idx] = bert_model(tokenizer.encode(word))[1].detach().numpy()\n",
    "    # else:\n",
    "    #     embedding_mat[idx] = np.random.rand(768)\n",
    "    try:\n",
    "        tokens = tokenizer.batch_encode_plus([word], return_tensors='pt', add_special_tokens=False)\n",
    "    except:\n",
    "        tokens = tokenizer.batch_encode_plus(['unk'], return_tensors='pt', add_special_tokens=False)\n",
    "        continue\n",
    "    embeddings = None\n",
    "    with torch.no_grad():\n",
    "        # outputs = bert_model(**tokens)\n",
    "        # embeddings = outputs.last_hidden_state\n",
    "        try:\n",
    "            outputs = bert_model(**tokens)\n",
    "            embeddings = outputs.last_hidden_state\n",
    "        except:\n",
    "            tokens = tokenizer.batch_encode_plus(['unk'], return_tensors='pt', add_special_tokens=False)\n",
    "            outputs = bert_model(**tokens)\n",
    "            embeddings = outputs.last_hidden_state\n",
    "    embeddings = embeddings.squeeze(0)\n",
    "    word_embeddings = embeddings.mean(dim = 0)\n",
    "    embedding_mat[idx] = word_embeddings.squeeze(0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(embedding_mat, open('../Extracted Word Embeddings/legal_bert_embedding_mat.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_embeddings = pickle.load(open('../Original Word Embeddings/word2vec.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get word2vec embeddings from this model\n",
    "embedding_mat = np.zeros((len(word_to_idx), 300))\n",
    "for word, idx in word_to_idx.items():\n",
    "    if word in word2vec_embeddings:\n",
    "        embedding_mat[idx] = word2vec_embeddings[word]\n",
    "    else:\n",
    "        embedding_mat[idx] = np.random.rand(300)\n",
    "\n",
    "with open('../Extracted Word Embeddings/word2vec_embedding_mat.pkl', 'wb') as f:\n",
    "    pickle.dump(embedding_mat, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings = pickle.load(open('../Original Word Embeddings/glove.pkl', 'rb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_mat = np.zeros((len(word_to_idx), 300))\n",
    "for word, idx in word_to_idx.items():\n",
    "    if word in glove_embeddings:\n",
    "        embedding_mat[idx] = glove_embeddings[word]\n",
    "    else:\n",
    "        embedding_mat[idx] = np.random.rand(300)\n",
    "\n",
    "with open('../Extracted Word Embeddings/glove_embedding_mat.pkl', 'wb') as f:\n",
    "    pickle.dump(embedding_mat, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
