{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2add41d3810>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #check if cuda is available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = json.load(open('../Dataset/BIO_Tagged/ATE_train.json', 'r'))\n",
    "test_data = json.load(open('../Dataset/BIO_Tagged/ATE_test.json', 'r'))\n",
    "val_data = json.load(open('../Dataset/BIO_Tagged/ATE_val.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = pickle.load(open('../Utils/word_to_idx.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_to_ix = pickle.load(open('../Utils/tag_to_ix.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, target_size, embedding_mat, start_tag, end_tag, tag_to_ix, device='cpu'):\n",
    "        super(RNN_model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_mat)).to(device)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim).to(device)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size).to(device)\n",
    "        self.start_tag = start_tag\n",
    "        self.end_tag = end_tag\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.target_size = target_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        rnn_out, _ = self.rnn(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(rnn_out.view(len(sentence), -1))\n",
    "        tag_scores = nn.functional.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, target_size, embedding_mat, start_tag, end_tag, tag_to_ix, device='cpu'):\n",
    "        super(LSTM_model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_mat)).to(device)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim).to(device)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size).to(device)\n",
    "        self.start_tag = start_tag\n",
    "        self.end_tag = end_tag\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.target_size = target_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = nn.functional.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, target_size, embedding_mat, start_tag, end_tag, tag_to_ix, device='cpu'):\n",
    "        super(GRU_model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_mat)).to(device)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim).to(device)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size).to(device)\n",
    "        self.start_tag = start_tag\n",
    "        self.end_tag = end_tag\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.target_size = target_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        gru_out, _ = self.gru(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(gru_out.view(len(sentence), -1))\n",
    "        tag_scores = nn.functional.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding mats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embedding_mat = pickle.load(open('../Extracted Word Embeddings/bert_embedding_mat.pkl', 'rb'))\n",
    "word2vec_embedding_mat = pickle.load(open('../Extracted Word Embeddings/word2vec_embedding_mat.pkl', 'rb'))\n",
    "glove_embedding_mat = pickle.load(open('../Extracted Word Embeddings/glove_embedding_mat.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glove + RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:04<00:00, 198.24it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 954.03it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5906751319996305, Val Loss: 0.48600818461751283, Train F1: 0.24792421500113568, Val F1: 0.4936049709652359\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:04<00:00, 190.52it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 1280.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4745467263243056, Val Loss: 0.4407622191255495, Train F1: 0.5159481994997834, Val F1: 0.5871878967190712\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:05<00:00, 180.74it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 1074.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.436975707552883, Val Loss: 0.42309416020841906, Train F1: 0.5771188530310494, Val F1: 0.6123415450966992\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:04<00:00, 222.60it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 1108.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4137342054947849, Val Loss: 0.4148335832439057, Train F1: 0.6144161160651196, Val F1: 0.624637177538558\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:04<00:00, 221.03it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 787.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3965740699452609, Val Loss: 0.41056347464861936, Train F1: 0.6410232287690835, Val F1: 0.6385879376026248\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:04<00:00, 220.48it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 742.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3829378330870377, Val Loss: 0.40833529379201805, Train F1: 0.6590469017991157, Val F1: 0.6427277125883862\n",
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:04<00:00, 197.85it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 438.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3715327756960463, Val Loss: 0.4072454960412903, Train F1: 0.6722689250356608, Val F1: 0.6476423403885582\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:05<00:00, 168.18it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 913.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.36156520150420945, Val Loss: 0.40679565660621475, Train F1: 0.6840069156304057, Val F1: 0.6536894304246933\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:04<00:00, 202.18it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 864.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3525366788693019, Val Loss: 0.406747530771581, Train F1: 0.6943506197311087, Val F1: 0.6561248612243575\n",
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:04<00:00, 186.30it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 683.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3441290736494475, Val Loss: 0.40703572426416557, Train F1: 0.7053258764128988, Val F1: 0.6564892657316198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rnn_model = RNN_model(len(word_to_idx), 300, 256, len(tag_to_ix), glove_embedding_mat, tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(rnn_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    preds = []\n",
    "    actuals = []\n",
    "    train_loss_temp = 0\n",
    "    val_loss_temp = 0\n",
    "    rnn_model.train()\n",
    "    for case in tqdm(train_data):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        targets = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        rnn_model.zero_grad()\n",
    "        tag_scores = rnn_model(sentence)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp += loss.item()\n",
    "        preds.extend(torch.argmax(tag_scores, dim=1).detach().numpy().tolist())\n",
    "        actuals.extend(targets.detach().numpy().tolist())\n",
    "    train_f1_temp = f1_score(actuals, preds, average='macro')\n",
    "    train_loss.append(train_loss_temp/len(train_data))\n",
    "    train_f1.append(train_f1_temp)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        rnn_model.eval()\n",
    "        preds = []\n",
    "        actuals = []\n",
    "        for case in tqdm(val_data):\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            tag_scores = rnn_model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            val_loss_temp += loss.item()\n",
    "            preds.extend(torch.argmax(tag_scores, dim=1).detach().numpy().tolist())\n",
    "            actuals.extend(targets.detach().numpy().tolist())\n",
    "        val_f1_temp = f1_score(actuals, preds, average='macro')\n",
    "        val_loss.append(val_loss_temp/len(val_data))\n",
    "        val_f1.append(val_f1_temp)\n",
    "    print(f'Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Train F1: {train_f1[-1]}, Val F1: {val_f1[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rnn_model, 'Non Trainable Embeddings/Glove+RNN/model.pt')\n",
    "torch.save(rnn_model, '../../Deliverables/Task 2/Saved Models/t2_RNN_Glove.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Glove+RNN/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Glove+RNN/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_f1, open('Non Trainable Embeddings/Glove+RNN/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_f1, open('Non Trainable Embeddings/Glove+RNN/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2vec + RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 12/906 [00:00<00:07, 119.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:08<00:00, 104.03it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 682.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6518684866412586, Val Loss: 0.5325831061901023, Train F1: 0.19861567460388913, Val F1: 0.41638778399959125\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:06<00:00, 131.48it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 652.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.49532461168026554, Val Loss: 0.43930739069093855, Train F1: 0.452669848606394, Val F1: 0.5175740333635072\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:06<00:00, 148.56it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 542.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4309496403266789, Val Loss: 0.40018840290640045, Train F1: 0.5444306655742012, Val F1: 0.6159484980146271\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:07<00:00, 117.16it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 353.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3988724973866876, Val Loss: 0.3824650713808188, Train F1: 0.608791918149919, Val F1: 0.6503085926021616\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 97.58it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 224.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3787854417385513, Val Loss: 0.3729464514989015, Train F1: 0.6400835434181352, Val F1: 0.6667882869727687\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:08<00:00, 112.12it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 479.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3638512512738868, Val Loss: 0.36728311791063445, Train F1: 0.6583324549794562, Val F1: 0.675230393169353\n",
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:05<00:00, 158.90it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 974.10it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.35170825776822917, Val Loss: 0.36390301910097195, Train F1: 0.6744333863491455, Val F1: 0.6811537881245284\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:04<00:00, 214.81it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 1111.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.34138174529781534, Val Loss: 0.36205828336139795, Train F1: 0.6879242306632719, Val F1: 0.6802942154697792\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:05<00:00, 152.63it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 1027.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3323538691035696, Val Loss: 0.3613403914283672, Train F1: 0.7002416810095701, Val F1: 0.6863871510307821\n",
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:05<00:00, 154.06it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 1000.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3242399464700593, Val Loss: 0.3615543705381487, Train F1: 0.7078857935966575, Val F1: 0.6902368501743693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rnn_model = RNN_model(len(word_to_idx), 300, 256, len(tag_to_ix), word2vec_embedding_mat, tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(rnn_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    preds = []\n",
    "    actuals = []\n",
    "    train_loss_temp = 0\n",
    "    val_loss_temp = 0\n",
    "    rnn_model.train()\n",
    "    for case in tqdm(train_data):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        targets = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        rnn_model.zero_grad()\n",
    "        tag_scores = rnn_model(sentence)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp += loss.item()\n",
    "        preds.extend(torch.argmax(tag_scores, dim=1).detach().numpy().tolist())\n",
    "        actuals.extend(targets.detach().numpy().tolist())\n",
    "    train_f1_temp = f1_score(actuals, preds, average='macro')\n",
    "    train_loss.append(train_loss_temp/len(train_data))\n",
    "    train_f1.append(train_f1_temp)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        rnn_model.eval()\n",
    "        preds = []\n",
    "        actuals = []\n",
    "        for case in tqdm(val_data):\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            tag_scores = rnn_model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            val_loss_temp += loss.item()\n",
    "            preds.extend(torch.argmax(tag_scores, dim=1).detach().numpy().tolist())\n",
    "            actuals.extend(targets.detach().numpy().tolist())\n",
    "        val_f1_temp = f1_score(actuals, preds, average='macro')\n",
    "        val_loss.append(val_loss_temp/len(val_data))\n",
    "        val_f1.append(val_f1_temp)\n",
    "    print(f'Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Train F1: {train_f1[-1]}, Val F1: {val_f1[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rnn_model, 'Non Trainable Embeddings/Word2vec+RNN/model.pt')\n",
    "torch.save(rnn_model, '../../Deliverables/Task 2/Saved Models/t2_RNN_Word2Vec.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Word2vec+RNN/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Word2vec+RNN/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_f1, open('Non Trainable Embeddings/Word2vec+RNN/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_f1, open('Non Trainable Embeddings/Word2vec+RNN/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert + RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/906 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:08<00:00, 111.15it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 487.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5115846748531654, Val Loss: 0.4185662794875228, Train F1: 0.5117648202263461, Val F1: 0.6008823407211678\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 96.82it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 515.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.41822173813189367, Val Loss: 0.3837210721076896, Train F1: 0.6175650847875512, Val F1: 0.6424363513573539\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:08<00:00, 109.69it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 744.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3862682700519004, Val Loss: 0.36664632456079466, Train F1: 0.6535374801888602, Val F1: 0.680609009576704\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:06<00:00, 131.70it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 587.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.36434132257110496, Val Loss: 0.3568663612756555, Train F1: 0.6751026246309463, Val F1: 0.6834288317269515\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:07<00:00, 128.89it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 655.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3467783506663627, Val Loss: 0.3509362359195267, Train F1: 0.6908299900868101, Val F1: 0.6872757093584769\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:08<00:00, 112.69it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 632.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.33156428097543733, Val Loss: 0.34741868276029964, Train F1: 0.7015191680757655, Val F1: 0.6896188751891584\n",
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:07<00:00, 125.56it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 702.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3177414035083311, Val Loss: 0.3456251445358202, Train F1: 0.7146268246536095, Val F1: 0.6967714249165936\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:08<00:00, 106.49it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 305.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3047435293775915, Val Loss: 0.3452359177172184, Train F1: 0.7284251571421084, Val F1: 0.6933911384251172\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:07<00:00, 120.47it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 691.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.292218828347674, Val Loss: 0.3461604512479479, Train F1: 0.7390190525020355, Val F1: 0.6932770945008352\n",
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:07<00:00, 113.68it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 459.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.27994560464501184, Val Loss: 0.34846479016181814, Train F1: 0.7498621914033691, Val F1: 0.690814526254795\n"
     ]
    }
   ],
   "source": [
    "rnn_model = RNN_model(len(word_to_idx), 768, 512, len(tag_to_ix), bert_embedding_mat, tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(rnn_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    preds = []\n",
    "    actuals = []\n",
    "    train_loss_temp = 0\n",
    "    val_loss_temp = 0\n",
    "    rnn_model.train()\n",
    "    for case in tqdm(train_data):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        targets = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        rnn_model.zero_grad()\n",
    "        tag_scores = rnn_model(sentence)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp += loss.item()\n",
    "        preds.extend(torch.argmax(tag_scores, dim=1).detach().numpy().tolist())\n",
    "        actuals.extend(targets.detach().numpy().tolist())\n",
    "    train_f1_temp = f1_score(actuals, preds, average='macro')\n",
    "    train_loss.append(train_loss_temp/len(train_data))\n",
    "    train_f1.append(train_f1_temp)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        rnn_model.eval()\n",
    "        preds = []\n",
    "        actuals = []\n",
    "        for case in tqdm(val_data):\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            tag_scores = rnn_model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            val_loss_temp += loss.item()\n",
    "            preds.extend(torch.argmax(tag_scores, dim=1).detach().numpy().tolist())\n",
    "            actuals.extend(targets.detach().numpy().tolist())\n",
    "        val_f1_temp = f1_score(actuals, preds, average='macro')\n",
    "        val_loss.append(val_loss_temp/len(val_data))\n",
    "        val_f1.append(val_f1_temp)\n",
    "    print(f'Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Train F1: {train_f1[-1]}, Val F1: {val_f1[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rnn_model, 'Non Trainable Embeddings/Bert+RNN/model.pt')\n",
    "torch.save(rnn_model, '../../Deliverables/Task 2/Saved Models/t2_RNN_Bert.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Bert+RNN/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Bert+RNN/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_f1, open('Non Trainable Embeddings/Bert+RNN/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_f1, open('Non Trainable Embeddings/Bert+RNN/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glove + GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/906 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:10<00:00, 82.67it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 490.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:07<00:00, 120.72it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 558.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:10<00:00, 90.30it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 326.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 91.25it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 510.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 100.05it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 616.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:07<00:00, 126.35it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 634.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:06<00:00, 135.10it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 626.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 98.95it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 464.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 93.32it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 309.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:12<00:00, 69.96it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 259.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gru_model = GRU_model(len(word_to_idx), 300, 256, len(tag_to_ix), glove_embedding_mat, tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(gru_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    preds = []\n",
    "    actuals = []\n",
    "    train_loss_temp = 0\n",
    "    val_loss_temp = 0\n",
    "    rnn_model.train()\n",
    "    for case in tqdm(train_data):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        targets = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        rnn_model.zero_grad()\n",
    "        tag_scores = rnn_model(sentence)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp += loss.item()\n",
    "        preds.extend(torch.argmax(tag_scores, dim=1).detach().numpy().tolist())\n",
    "        actuals.extend(targets.detach().numpy().tolist())\n",
    "    train_f1_temp = f1_score(actuals, preds, average='macro')\n",
    "    train_loss.append(train_loss_temp/len(train_data))\n",
    "    train_f1.append(train_f1_temp)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        rnn_model.eval()\n",
    "        preds = []\n",
    "        actuals = []\n",
    "        for case in tqdm(val_data):\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            tag_scores = rnn_model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            val_loss_temp += loss.item()\n",
    "            preds.extend(torch.argmax(tag_scores, dim=1).detach().numpy().tolist())\n",
    "            actuals.extend(targets.detach().numpy().tolist())\n",
    "        val_f1_temp = f1_score(actuals, preds, average='macro')\n",
    "        val_loss.append(val_loss_temp/len(val_data))\n",
    "        val_f1.append(val_f1_temp)\n",
    "    print(f'Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Train F1: {train_f1[-1]}, Val F1: {val_f1[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gru_model, 'Non Trainable Embeddings/Glove+GRU/model.pt')\n",
    "torch.save(gru_model, '../../Deliverables/Task 2/Saved Models/t2_GRU_Glove.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Glove+GRU/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Glove+GRU/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_f1, open('Non Trainable Embeddings/Glove+GRU/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_f1, open('Non Trainable Embeddings/Glove+GRU/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2vec + GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/906 [00:00<00:17, 52.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:08<00:00, 109.15it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 791.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:06<00:00, 132.37it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 395.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:07<00:00, 124.07it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 659.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:06<00:00, 146.05it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 663.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:06<00:00, 131.21it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 626.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 97.62it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 604.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 95.10it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 466.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 96.33it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 539.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:07<00:00, 122.57it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 610.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 94.83it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 456.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gru_model = GRU_model(len(word_to_idx), 300, 256, len(tag_to_ix), word2vec_embedding_mat, tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(gru_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    preds = []\n",
    "    actuals = []\n",
    "    train_loss_temp = 0\n",
    "    val_loss_temp = 0\n",
    "    rnn_model.train()\n",
    "    for case in tqdm(train_data):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        targets = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        rnn_model.zero_grad()\n",
    "        tag_scores = rnn_model(sentence)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp += loss.item()\n",
    "        preds.extend(torch.argmax(tag_scores, dim=1).detach().numpy().tolist())\n",
    "        actuals.extend(targets.detach().numpy().tolist())\n",
    "    train_f1_temp = f1_score(actuals, preds, average='macro')\n",
    "    train_loss.append(train_loss_temp/len(train_data))\n",
    "    train_f1.append(train_f1_temp)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        rnn_model.eval()\n",
    "        preds = []\n",
    "        actuals = []\n",
    "        for case in tqdm(val_data):\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            tag_scores = rnn_model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            val_loss_temp += loss.item()\n",
    "            preds.extend(torch.argmax(tag_scores, dim=1).detach().numpy().tolist())\n",
    "            actuals.extend(targets.detach().numpy().tolist())\n",
    "        val_f1_temp = f1_score(actuals, preds, average='macro')\n",
    "        val_loss.append(val_loss_temp/len(val_data))\n",
    "        val_f1.append(val_f1_temp)\n",
    "    print(f'Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Train F1: {train_f1[-1]}, Val F1: {val_f1[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gru_model, 'Non Trainable Embeddings/Word2vec+GRU/model.pt')\n",
    "torch.save(gru_model, '../../Deliverables/Task 2/Saved Models/t2_GRU_Word2Vec.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Word2vec+GRU/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Word2vec+GRU/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_f1, open('Non Trainable Embeddings/Word2vec+GRU/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_f1, open('Non Trainable Embeddings/Word2vec+GRU/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert + GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:10<00:00, 85.05it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 445.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:07<00:00, 116.86it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 561.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:07<00:00, 115.45it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 582.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 92.66it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 445.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:08<00:00, 106.84it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 469.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:07<00:00, 116.05it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 493.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 94.71it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 568.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:07<00:00, 128.71it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 595.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:07<00:00, 122.22it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 594.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:08<00:00, 110.21it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 296.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n"
     ]
    }
   ],
   "source": [
    "gru_model = GRU_model(len(word_to_idx), 768, 512, len(tag_to_ix), bert_embedding_mat, tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(gru_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    preds = []\n",
    "    actuals = []\n",
    "    train_loss_temp = 0\n",
    "    val_loss_temp = 0\n",
    "    rnn_model.train()\n",
    "    for case in tqdm(train_data):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        targets = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        rnn_model.zero_grad()\n",
    "        tag_scores = rnn_model(sentence)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp += loss.item()\n",
    "        preds.extend(torch.argmax(tag_scores, dim=1).detach().numpy().tolist())\n",
    "        actuals.extend(targets.detach().numpy().tolist())\n",
    "    train_f1_temp = f1_score(actuals, preds, average='macro')\n",
    "    train_loss.append(train_loss_temp/len(train_data))\n",
    "    train_f1.append(train_f1_temp)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        rnn_model.eval()\n",
    "        preds = []\n",
    "        actuals = []\n",
    "        for case in tqdm(val_data):\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            tag_scores = rnn_model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            val_loss_temp += loss.item()\n",
    "            preds.extend(torch.argmax(tag_scores, dim=1).detach().numpy().tolist())\n",
    "            actuals.extend(targets.detach().numpy().tolist())\n",
    "        val_f1_temp = f1_score(actuals, preds, average='macro')\n",
    "        val_loss.append(val_loss_temp/len(val_data))\n",
    "        val_f1.append(val_f1_temp)\n",
    "    print(f'Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Train F1: {train_f1[-1]}, Val F1: {val_f1[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gru_model, 'Non Trainable Embeddings/Bert+GRU/model.pt')\n",
    "torch.save(gru_model, '../../Deliverables/Task 2/Saved Models/t2_GRU_Bert.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Bert+GRU/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Bert+GRU/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_f1, open('Non Trainable Embeddings/Bert+GRU/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_f1, open('Non Trainable Embeddings/Bert+GRU/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glove + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 14/906 [00:00<00:15, 58.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 91.77it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 621.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 98.60it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 547.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:10<00:00, 84.30it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 375.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:10<00:00, 89.64it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 530.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 98.75it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 340.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:10<00:00, 85.99it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 630.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:07<00:00, 119.99it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 419.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 100.03it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 668.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:07<00:00, 125.21it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 548.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 96.87it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 659.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_model = LSTM_model(len(word_to_idx), 300, 256, len(tag_to_ix), glove_embedding_mat, tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(lstm_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    preds = []\n",
    "    actuals = []\n",
    "    train_loss_temp = 0\n",
    "    val_loss_temp = 0\n",
    "    rnn_model.train()\n",
    "    for case in tqdm(train_data):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        targets = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        rnn_model.zero_grad()\n",
    "        tag_scores = rnn_model(sentence)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp += loss.item()\n",
    "        preds.extend(torch.argmax(tag_scores, dim=1).detach().numpy().tolist())\n",
    "        actuals.extend(targets.detach().numpy().tolist())\n",
    "    train_f1_temp = f1_score(actuals, preds, average='macro')\n",
    "    train_loss.append(train_loss_temp/len(train_data))\n",
    "    train_f1.append(train_f1_temp)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        rnn_model.eval()\n",
    "        preds = []\n",
    "        actuals = []\n",
    "        for case in tqdm(val_data):\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            tag_scores = rnn_model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            val_loss_temp += loss.item()\n",
    "            preds.extend(torch.argmax(tag_scores, dim=1).detach().numpy().tolist())\n",
    "            actuals.extend(targets.detach().numpy().tolist())\n",
    "        val_f1_temp = f1_score(actuals, preds, average='macro')\n",
    "        val_loss.append(val_loss_temp/len(val_data))\n",
    "        val_f1.append(val_f1_temp)\n",
    "    print(f'Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Train F1: {train_f1[-1]}, Val F1: {val_f1[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm_model, 'Non Trainable Embeddings/Glove+LSTM/model.pt')\n",
    "torch.save(lstm_model, '../../Deliverables/Task 2/Saved Models/t2_LSTM_Glove.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Glove+LSTM/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Glove+LSTM/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_f1, open('Non Trainable Embeddings/Glove+LSTM/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_f1, open('Non Trainable Embeddings/Glove+LSTM/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2vec + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 95.43it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 603.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:07<00:00, 118.98it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 675.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:12<00:00, 72.06it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 484.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 99.48it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 503.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:08<00:00, 104.17it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 574.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:08<00:00, 105.12it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 600.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 95.61it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 304.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:08<00:00, 104.86it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 538.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 100.23it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 606.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:10<00:00, 87.12it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 300.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_model = LSTM_model(len(word_to_idx), 300, 256, len(tag_to_ix), word2vec_embedding_mat, tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(lstm_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    preds = []\n",
    "    actuals = []\n",
    "    train_loss_temp = 0\n",
    "    val_loss_temp = 0\n",
    "    rnn_model.train()\n",
    "    for case in tqdm(train_data):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        targets = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        rnn_model.zero_grad()\n",
    "        tag_scores = rnn_model(sentence)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp += loss.item()\n",
    "        preds.extend(torch.argmax(tag_scores, dim=1).detach().numpy().tolist())\n",
    "        actuals.extend(targets.detach().numpy().tolist())\n",
    "    train_f1_temp = f1_score(actuals, preds, average='macro')\n",
    "    train_loss.append(train_loss_temp/len(train_data))\n",
    "    train_f1.append(train_f1_temp)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        rnn_model.eval()\n",
    "        preds = []\n",
    "        actuals = []\n",
    "        for case in tqdm(val_data):\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            tag_scores = rnn_model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            val_loss_temp += loss.item()\n",
    "            preds.extend(torch.argmax(tag_scores, dim=1).detach().numpy().tolist())\n",
    "            actuals.extend(targets.detach().numpy().tolist())\n",
    "        val_f1_temp = f1_score(actuals, preds, average='macro')\n",
    "        val_loss.append(val_loss_temp/len(val_data))\n",
    "        val_f1.append(val_f1_temp)\n",
    "    print(f'Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Train F1: {train_f1[-1]}, Val F1: {val_f1[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm_model, 'Non Trainable Embeddings/Word2vec+LSTM/model.pt')\n",
    "torch.save(lstm_model, '../../Deliverables/Task 2/Saved Models/t2_LSTM_Word2Vec.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Word2vec+LSTM/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Word2vec+LSTM/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_f1, open('Non Trainable Embeddings/Word2vec+LSTM/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_f1, open('Non Trainable Embeddings/Word2vec+LSTM/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/906 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:08<00:00, 103.63it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 626.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:09<00:00, 99.30it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 549.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:08<00:00, 111.60it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 614.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:10<00:00, 89.74it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 390.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:10<00:00, 83.08it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 403.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:10<00:00, 86.90it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 395.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:12<00:00, 74.97it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 309.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:12<00:00, 71.19it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 393.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:10<00:00, 84.13it/s] \n",
      "100%|██████████| 219/219 [00:00<00:00, 420.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n",
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 906/906 [00:14<00:00, 63.88it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 309.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2489375187614501, Val Loss: 0.34846479016181814, Train F1: 0.7814219835995798, Val F1: 0.690814526254795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_model = LSTM_model(len(word_to_idx), 768, 512, len(tag_to_ix), bert_embedding_mat, tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(lstm_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    preds = []\n",
    "    actuals = []\n",
    "    train_loss_temp = 0\n",
    "    val_loss_temp = 0\n",
    "    rnn_model.train()\n",
    "    for case in tqdm(train_data):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        targets = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        rnn_model.zero_grad()\n",
    "        tag_scores = rnn_model(sentence)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp += loss.item()\n",
    "        preds.extend(torch.argmax(tag_scores, dim=1).detach().numpy().tolist())\n",
    "        actuals.extend(targets.detach().numpy().tolist())\n",
    "    train_f1_temp = f1_score(actuals, preds, average='macro')\n",
    "    train_loss.append(train_loss_temp/len(train_data))\n",
    "    train_f1.append(train_f1_temp)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        rnn_model.eval()\n",
    "        preds = []\n",
    "        actuals = []\n",
    "        for case in tqdm(val_data):\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            tag_scores = rnn_model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            val_loss_temp += loss.item()\n",
    "            preds.extend(torch.argmax(tag_scores, dim=1).detach().numpy().tolist())\n",
    "            actuals.extend(targets.detach().numpy().tolist())\n",
    "        val_f1_temp = f1_score(actuals, preds, average='macro')\n",
    "        val_loss.append(val_loss_temp/len(val_data))\n",
    "        val_f1.append(val_f1_temp)\n",
    "    print(f'Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Train F1: {train_f1[-1]}, Val F1: {val_f1[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm_model, 'Non Trainable Embeddings/Bert+LSTM/model.pt')\n",
    "torch.save(lstm_model, '../../Deliverables/Task 2/Saved Models/t2_LSTM_Bert.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Bert+LSTM/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Bert+LSTM/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_f1, open('Non Trainable Embeddings/Bert+LSTM/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_f1, open('Non Trainable Embeddings/Bert+LSTM/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
