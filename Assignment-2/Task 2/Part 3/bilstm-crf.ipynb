{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical \n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import json\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from keras.callbacks import Callback\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a368b874d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = json.load(open('../Dataset/BIO_Tagged/ATE_train.json'))\n",
    "val_data = json.load(open('../Dataset/BIO_Tagged/ATE_val.json'))\n",
    "test_data = json.load(open('../Dataset/BIO_Tagged/ATE_test.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = pickle.load(open('Word Embeddings/word2vec.pkl', 'rb'))\n",
    "glove = pickle.load(open('Word Embeddings/glove.pkl', 'rb'))\n",
    "# Add another embedding here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(train_data, embedding_type):\n",
    "    X, Y = [], []\n",
    "    for data in train_data:\n",
    "        X.append(train_data[data]['text'].split())\n",
    "        Y.append(train_data[data]['labels'])\n",
    "        \n",
    "    num_words = len(set([word.lower() for sentence in X for word in sentence]))       \n",
    "    word_tokenizer = Tokenizer()                      \n",
    "    word_tokenizer.fit_on_texts(X)                    \n",
    "    X_encoded = word_tokenizer.texts_to_sequences(X)\n",
    "\n",
    "    tag_tokenizer = Tokenizer()\n",
    "    tag_tokenizer.fit_on_texts(Y)\n",
    "    Y_encoded = tag_tokenizer.texts_to_sequences(Y)\n",
    "\n",
    "    max_seq_length = 50\n",
    "\n",
    "    X_padded = pad_sequences(X_encoded, maxlen=max_seq_length, padding=\"pre\", truncating=\"post\")\n",
    "    Y_padded = pad_sequences(Y_encoded, maxlen=max_seq_length, padding=\"pre\", truncating=\"post\")\n",
    "\n",
    "    X, Y = X_padded, Y_padded    \n",
    "\n",
    "    embedding_matrix = np.zeros((num_words+1, 300))\n",
    "    \n",
    "    word2id = word_tokenizer.word_index\n",
    "    for word, index in word2id.items():\n",
    "        try:\n",
    "            embedding_matrix[index, :] = embedding_type[word]\n",
    "        except KeyError:\n",
    "            pass    \n",
    "\n",
    "    Y = to_categorical(Y)\n",
    "\n",
    "    return X, Y, embedding_matrix, word_tokenizer, tag_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareBertData(train_data):\n",
    "    X, Y = [], []\n",
    "    for data in train_data:\n",
    "        X.append(train_data[data]['text'].split())\n",
    "        Y.append(train_data[data]['labels'])\n",
    "        \n",
    "    num_words = len(set([word.lower() for sentence in X for word in sentence]))    \n",
    "    model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    word_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    word_tokenizer(X, return_tensors='tf')   \n",
    "    word_tokenized_output = word_tokenizer(X, return_tensors='tf', padding=True, truncation=True)\n",
    "    X_encoded = word_tokenized_output['input_ids'].numpy().tolist()\n",
    "\n",
    "    tag_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    tag_tokenizer(Y, return_tensors='tf')  \n",
    "    tag_tokenized_output = tag_tokenizer(Y, return_tensors='tf', padding=True, truncation=True)\n",
    "    Y_encoded = tag_tokenized_output['input_ids'].numpy().tolist()\n",
    "\n",
    "    max_seq_length = 50\n",
    "\n",
    "    X_padded = pad_sequences(X_encoded, maxlen=max_seq_length, padding=\"pre\", truncating=\"post\")\n",
    "    Y_padded = pad_sequences(Y_encoded, maxlen=max_seq_length, padding=\"pre\", truncating=\"post\")\n",
    "\n",
    "    X, Y = X_padded, Y_padded    \n",
    "\n",
    "    Y = to_categorical(Y)\n",
    "\n",
    "    return X, Y, TFBertModel, word_tokenizer, tag_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['word2vec'] = {}\n",
    "data['glove'] = {}\n",
    "data['bert'] = {}\n",
    "\n",
    "data['word2vec']['X_train'], data['word2vec']['Y_train'], data['word2vec']['train_embedding_matrix'], word_tokenizer, tag_tokenizer = prepareData(train_data, word2vec)\n",
    "data['word2vec']['X_val'], data['word2vec']['Y_val'], data['word2vec']['val_embedding_matrix'], _, _ = prepareData(val_data, word2vec)\n",
    "data['word2vec']['X_test'], data['word2vec']['Y_test'], data['word2vec']['test_embedding_matrix'], _, _ = prepareData(test_data, word2vec)\n",
    "\n",
    "data['glove']['X_train'], data['glove']['Y_train'], data['glove']['train_embedding_matrix'], word_tokenizer, tag_tokenizer = prepareData(train_data, glove)\n",
    "data['glove']['X_val'], data['glove']['Y_val'], data['glove']['val_embedding_matrix'],  _, _ = prepareData(val_data, glove)\n",
    "data['glove']['X_test'], data['glove']['Y_test'], data['glove']['test_embedding_matrix'],  _, _ = prepareData(test_data, glove)\n",
    "\n",
    "# data['bert']['X_train'], data['bert']['Y_train'], data['bert']['train_embedding_matrix'], word_tokenizer, tag_tokenizer = prepareBertData(train_data)\n",
    "# data['bert']['X_val'], data['bert']['Y_val'], data['bert']['val_embedding_matrix'],  _, _ = prepareBertData(val_data)\n",
    "# data['bert']['X_test'], data['bert']['Y_test'], data['bert']['test_embedding_matrix'],  _, _ = prepareBertData(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MacroF1ScoreCallback(Callback):\n",
    "    def __init__(self, train_data, val_data):\n",
    "        super().__init__()\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.train_f1s = []\n",
    "        self.val_f1s = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        train_pred = np.argmax(self.model.predict(self.train_data[0]), axis=-1)\n",
    "        train_true = np.argmax(self.train_data[1], axis=-1)\n",
    "        train_f1 = f1_score(train_true.flatten(), train_pred.flatten(), average='macro')\n",
    "\n",
    "        val_pred = np.argmax(self.model.predict(self.val_data[0]), axis=-1)\n",
    "        val_true = np.argmax(self.val_data[1], axis=-1)\n",
    "        val_f1 = f1_score(val_true.flatten(), val_pred.flatten(), average='macro')\n",
    "\n",
    "        self.train_f1s.append(train_f1)\n",
    "        self.val_f1s.append(val_f1)\n",
    "        print(f'Epoch {epoch + 1} - Train Macro-F1: {train_f1:.4f} - Val Macro-F1: {val_f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()\n",
    "\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + \\\n",
    "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "\n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "        # Matrix of transition parameters.  Entry i,j is the score of\n",
    "        # transitioning *to* i *from* j.\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # These two statements enforce the constraint that we never transfer\n",
    "        # to the start tag and we never transfer from the stop tag\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
    "                torch.randn(2, 1, self.hidden_dim // 2))\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        # Do the forward algorithm to compute the partition function\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        # Iterate through the sentence\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = feat[next_tag].view(\n",
    "                    1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the\n",
    "                # scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # Gives the score of a provided tag sequence\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + \\\n",
    "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare data?\n",
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "EMBEDDING_DIM = 5\n",
    "HIDDEN_DIM = 4\n",
    "\n",
    "# Make up some training data\n",
    "training_data = [(\n",
    "    \"the wall street journal reported today that apple corporation made money\".split(),\n",
    "    \"B I I I O O O B I O O\".split()\n",
    "), (\n",
    "    \"georgia tech is a university in georgia\".split(),\n",
    "    \"B I O O O O B\".split()\n",
    ")]\n",
    "\n",
    "word_to_ix = {}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}\n",
    "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "# Check predictions before training\n",
    "with torch.no_grad():\n",
    "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long)\n",
    "    print(model(precheck_sent))\n",
    "\n",
    "# Make sure prepare_sequence from earlier in the LSTM section is loaded\n",
    "for epoch in range(\n",
    "        300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is,\n",
    "        # turn them into Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Check predictions after training\n",
    "with torch.no_grad():\n",
    "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    print(model(precheck_sent))\n",
    "# We got it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aspect_terms(test_pred, X_test):\n",
    "    aspect_terms = []\n",
    "    for i in range(len(test_pred)):\n",
    "        aspect_term = []\n",
    "        for j in range(len(test_pred[i])):\n",
    "            if test_pred[i][j] == 1:\n",
    "                aspect_term.append(word_tokenizer.index_word[X_test[i][j]])\n",
    "        aspect_terms.append(aspect_term)\n",
    "    return aspect_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, Y_test):\n",
    "    Y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "    Y_true = np.argmax(Y_test, axis=-1)\n",
    "\n",
    "    aspect_terms = get_aspect_terms(Y_pred, X_test)\n",
    "\n",
    "    print(f'Macro F1 Score: {f1_score(Y_true.flatten(), Y_pred.flatten(), average=\"macro\"):.4f}')\n",
    "    print(f'Accuracy: {accuracy_score(Y_true.flatten(), Y_pred.flatten())*100:.2f}%')\n",
    "\n",
    "    return aspect_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Word2Vec\n",
      "11/11 [==============================] - 0s 8ms/step\n",
      "Macro F1 Score: 0.5388\n",
      "Accuracy: 90.84%\n",
      "\n",
      "RNN Glove\n",
      "11/11 [==============================] - 0s 8ms/step\n",
      "Macro F1 Score: 0.5311\n",
      "Accuracy: 90.79%\n",
      "\n",
      "LSTM Word2Vec\n",
      "11/11 [==============================] - 1s 15ms/step\n",
      "Macro F1 Score: 0.4959\n",
      "Accuracy: 91.90%\n",
      "\n",
      "LSTM Glove\n",
      "11/11 [==============================] - 1s 21ms/step\n",
      "Macro F1 Score: 0.5019\n",
      "Accuracy: 91.61%\n",
      "\n",
      "GRU Word2Vec\n",
      "11/11 [==============================] - 1s 17ms/step\n",
      "Macro F1 Score: 0.5225\n",
      "Accuracy: 90.68%\n",
      "\n",
      "GRU Glove\n",
      "11/11 [==============================] - 1s 17ms/step\n",
      "Macro F1 Score: 0.5280\n",
      "Accuracy: 90.80%\n"
     ]
    }
   ],
   "source": [
    "bilstim_crf_word2vec = load_model('Models/t2_bilstim_crf_word2vec.h5')\n",
    "bilstim_crf_glove = load_model('Models/t2_bilstim_crf_glove.h5')\n",
    "# bilstim_crf_bert = load_model('Models/t2_bilstim_crf_bert.h5')\n",
    "\n",
    "\n",
    "print(\"BILSTM-CRF Word2Vec\")\n",
    "bilstim_crf_word2vec_aspect_terms = evaluate_model(bilstim_crf_word2vec, data['word2vec']['X_test'], data['word2vec']['Y_test'])\n",
    "\n",
    "print()\n",
    "print(\"BILSTM-CRF Glove\")\n",
    "bilstim_crf_glove_aspect_terms = evaluate_model(bilstim_crf_glove, data['glove']['X_test'], data['glove']['Y_test'])\n",
    "\n",
    "# print()\n",
    "# print(\"BILSTM-CRF bert\")\n",
    "# bilstim_crf_bert_aspect_terms = evaluate_model(bilstim_crf_bert, data['bert']['X_test'], data['bert']['Y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
