{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(train_losses, val_losses, embedding):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.plot(epochs, train_losses, 'b', label='Training Loss')\n",
    "    plt.plot(epochs, val_losses, 'r', label='Validation Loss')\n",
    "    plt.title(f'Loss - {embedding}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'Plots/loss_{embedding}.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_f1s(train_f1_scores, val_f1_scores, embedding):\n",
    "    epochs = range(1, len(train_f1_scores) + 1)\n",
    "    plt.plot(epochs, train_f1_scores, 'b', label='Training Loss')\n",
    "    plt.plot(epochs, val_f1_scores, 'r', label='Validation Loss')\n",
    "    plt.title(f'F1 Score - {embedding}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'Plots/f1_{embedding}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'Part 2/Non Trainable Embeddings'\n",
    "models = os.listdir(base_path)\n",
    "\n",
    "for model in models:\n",
    "    final_path = base_path + '/' + model\n",
    "    train_losses = pickle.load(open(final_path + '/train_loss.pkl', 'rb'))\n",
    "    val_losses = pickle.load(open(final_path + '/val_loss.pkl', 'rb'))\n",
    "    train_f1s = pickle.load(open(final_path + '/train_f1.pkl', 'rb'))\n",
    "    val_f1s = pickle.load(open(final_path + '/val_f1.pkl', 'rb'))\n",
    "\n",
    "    plot_losses(train_losses, val_losses, model)\n",
    "    plot_f1s(train_f1s, val_f1s, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting BiLSTM-CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'Part 3/Non Trainable Embeddings'\n",
    "models = os.listdir(base_path)\n",
    "\n",
    "for model in models:\n",
    "    final_path = base_path + '/' + model\n",
    "    train_losses = pickle.load(open(final_path + '/train_loss.pkl', 'rb'))\n",
    "    train_losses = [float(loss.item()) for loss in train_losses]\n",
    "\n",
    "    val_losses = pickle.load(open(final_path + '/val_loss.pkl', 'rb'))\n",
    "    val_losses = [float(loss.item()) for loss in val_losses]\n",
    "\n",
    "    train_f1s = pickle.load(open(final_path + '/train_f1.pkl', 'rb'))\n",
    "    train_f1s = [float(f1.item()) for f1 in train_f1s]\n",
    "\n",
    "    val_f1s = pickle.load(open(final_path + '/val_f1.pkl', 'rb'))\n",
    "    val_f1s = [float(f1.item()) for f1 in val_f1s]\n",
    "\n",
    "    plot_losses(train_losses, val_losses, f'{model} + BiLSTM CRF')\n",
    "    plot_f1s(train_f1s, val_f1s, f'{model} + BiLSTM CRF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = json.load(open('Dataset/NER_test.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()\n",
    "\n",
    "word_to_idx = pickle.load(open('Utils/word_to_idx.pkl', 'rb'))\n",
    "tag_to_ix = pickle.load(open('Utils/tag_to_ix.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, model_name):\n",
    "    test_f1 = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for case in tqdm(test_data):\n",
    "            sentence = prepare_sequence(test_data[case]['text'].split(' '), word_to_idx)\n",
    "            targets = prepare_sequence(test_data[case]['labels'], tag_to_ix)\n",
    "\n",
    "            # Make predictions\n",
    "            tag_scores = model(sentence)\n",
    "\n",
    "            # Convert predictions and targets to numpy arrays\n",
    "            predictions = torch.argmax(tag_scores, dim=1).detach().numpy()\n",
    "            targets = targets.detach().numpy()\n",
    "\n",
    "            # Collect predictions and targets for later evaluation\n",
    "            all_predictions.extend(predictions)\n",
    "            all_targets.extend(targets)\n",
    "\n",
    "            # Calculate F1 score\n",
    "            test_f1 += f1_score(targets, predictions, average='macro')\n",
    "\n",
    "    avg_test_f1 = test_f1 / len(test_data)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(all_targets, all_predictions)    \n",
    "    return {'Test Macro-F1': avg_test_f1, 'Test Accuracy': accuracy}\n",
    "\n",
    "def eval_BiLSTM(model, model_name):\n",
    "    test_f1 = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for case in tqdm(test_data):\n",
    "            sentence = prepare_sequence(test_data[case]['text'].split(' '), word_to_idx)\n",
    "            tags = prepare_sequence(test_data[case]['labels'], tag_to_ix)\n",
    "            # sentence = sentence.to(device)\n",
    "            # tags = tags.to(device)\n",
    "\n",
    "            preds = model(sentence)[1]\n",
    "\n",
    "            # Collect predictions and targets for later evaluation\n",
    "            all_predictions.extend(preds)\n",
    "            all_targets.extend(tags)\n",
    "\n",
    "            # Calculate F1 score\n",
    "            test_f1 += f1_score(tags, preds, average='macro')\n",
    "\n",
    "    avg_test_f1 = test_f1 / len(test_data)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(all_targets, all_predictions)    \n",
    "    return {'Test Macro-F1': avg_test_f1, 'Test Accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, target_size, embedding_mat, start_tag, end_tag, tag_to_ix, device='cpu'):\n",
    "        super(RNN_model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_mat)).to(device)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim).to(device)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size).to(device)\n",
    "        self.start_tag = start_tag\n",
    "        self.end_tag = end_tag\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.target_size = target_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        rnn_out, _ = self.rnn(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(rnn_out.view(len(sentence), -1))\n",
    "        tag_scores = nn.functional.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "    \n",
    "class LSTM_model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, target_size, embedding_mat, start_tag, end_tag, tag_to_ix, device='cpu'):\n",
    "        super(LSTM_model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_mat)).to(device)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim).to(device)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size).to(device)\n",
    "        self.start_tag = start_tag\n",
    "        self.end_tag = end_tag\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.target_size = target_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = nn.functional.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "    \n",
    "class GRU_model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, target_size, embedding_mat, start_tag, end_tag, tag_to_ix, device='cpu'):\n",
    "        super(GRU_model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_mat)).to(device)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim).to(device)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size).to(device)\n",
    "        self.start_tag = start_tag\n",
    "        self.end_tag = end_tag\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.target_size = target_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        gru_out, _ = self.gru(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(gru_out.view(len(sentence), -1))\n",
    "        tag_scores = nn.functional.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, target_size, embedding_mat, start_tag, end_tag, tag_to_ix, batch_size=1, device='cpu'):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.target_size = target_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.start_tag = start_tag\n",
    "        self.end_tag = end_tag\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_mat, freeze=False).to(device)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True).to(device)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size).to(device)\n",
    "\n",
    "        self.transitions_to = nn.Parameter(torch.randn(target_size, target_size)).to(device)\n",
    "        self.transitions_to.data[start_tag, :] = -10000\n",
    "        self.transitions_to.data[:, end_tag] = -10000\n",
    "\n",
    "        # self.transitions_from = nn.Parameter(torch.randn(target_size, target_size))\n",
    "        # self.transitions_from.data[:, start_tag] = -10000\n",
    "        # self.transitions_from.data[end_tag, :] = -10000\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2).to(self.device),\n",
    "                torch.randn(2, 1, self.hidden_dim // 2).to(self.device))\n",
    "\n",
    "    def get_lstm_features(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.embedding(sentence).view(len(sentence), 1, -1)\n",
    "        #convert embeds to torch float32\n",
    "        embeds = embeds.float()\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "\n",
    "\n",
    "    def _forward_algo(self, lstm_features):\n",
    "\n",
    "        scores = torch.full((1, self.target_size), -10000.).to(self.device)\n",
    "        scores[0][self.start_tag] = 0.\n",
    "\n",
    "        forward_var = scores\n",
    "\n",
    "        for feat in lstm_features:\n",
    "            next_tag_var = self.transitions_to + feat.view(-1, 1).expand(-1, self.target_size) + forward_var.expand(self.target_size, -1)\n",
    "            max_score = next_tag_var.max(dim=1).values.view(-1, 1)\n",
    "            next_tag_var = next_tag_var - max_score\n",
    "            forward_var = (max_score + torch.logsumexp(next_tag_var, dim=1).view(-1, 1)).view(1, -1)\n",
    "            \n",
    "        terminal_var = forward_var + (self.transitions_to[self.end_tag]).view(1, -1)\n",
    "        alpha = terminal_var\n",
    "        max_score = alpha.max()\n",
    "        alpha = max_score + torch.logsumexp(alpha - max_score, dim=1)\n",
    "        return alpha\n",
    "    \n",
    "\n",
    "    def _score_sentence(self, lstm_features, tags):\n",
    "        score = torch.zeros(1).to(self.device)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix['START_TAG']], dtype=torch.long).to(self.device), tags]).to(self.device)\n",
    "        for i, feat in enumerate(lstm_features):\n",
    "            score += self.transitions_to[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "                \n",
    "        score += self.transitions_to[self.tag_to_ix['END_TAG'], tags[-1]]\n",
    "        return score\n",
    "    \n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        lstm_feats = self.get_lstm_features(sentence)\n",
    "        forward_score = self._forward_algo(lstm_feats)\n",
    "        gold_score = self._score_sentence(lstm_feats, tags)\n",
    "        return forward_score - gold_score\n",
    "    \n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.target_size), -10000.).to(self.device)\n",
    "        init_vvars[0][self.start_tag] = 0\n",
    "\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = [] \n",
    "            viterbivars_t = [] \n",
    "\n",
    "            next_tag_var = self.transitions_to + forward_var.expand(self.target_size, -1)\n",
    "            best_tag_id = torch.argmax(next_tag_var, dim=1)\n",
    "            bptrs_t = best_tag_id\n",
    "            viterbivars_t = next_tag_var[range(len(best_tag_id)), best_tag_id].view(1, -1)\n",
    "            \n",
    "            forward_var = (viterbivars_t + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        \n",
    "        terminal_var = forward_var + self.transitions_to[self.end_tag]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "       \n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id.item())\n",
    "        \n",
    "        start = best_path.pop()\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "        \n",
    "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self.get_lstm_features(sentence)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 949/949 [00:07<00:00, 128.13it/s]\n",
      "100%|██████████| 949/949 [00:04<00:00, 210.85it/s]\n",
      "100%|██████████| 949/949 [00:04<00:00, 206.94it/s]\n",
      "100%|██████████| 949/949 [00:08<00:00, 110.29it/s]\n",
      "100%|██████████| 949/949 [00:09<00:00, 98.52it/s] \n",
      "100%|██████████| 949/949 [00:05<00:00, 177.37it/s]\n",
      "100%|██████████| 949/949 [00:05<00:00, 162.77it/s]\n",
      "100%|██████████| 949/949 [00:03<00:00, 244.26it/s]\n",
      "100%|██████████| 949/949 [00:03<00:00, 262.42it/s]\n",
      "100%|██████████| 949/949 [00:09<00:00, 97.68it/s] \n",
      "100%|██████████| 949/949 [00:12<00:00, 79.05it/s]\n",
      "100%|██████████| 949/949 [00:08<00:00, 108.30it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_metrics = {}\n",
    "base_path = 'Part 2/Non Trainable Embeddings'\n",
    "models = os.listdir(base_path)\n",
    "\n",
    "for model_name in models:\n",
    "    final_path = base_path + '/' + model_name + '/model.pt'\n",
    "    model = torch.load(final_path)\n",
    "    model.eval()\n",
    "    eval_metrics[model_name] = evaluate(model, model_name)\n",
    "\n",
    "base_path = 'Part 3/Non Trainable Embeddings'\n",
    "models = os.listdir(base_path)\n",
    "\n",
    "for model_name in models:\n",
    "    final_path = base_path + '/' + model_name + '/model.pt'\n",
    "    model = torch.load(final_path)\n",
    "    model.eval()\n",
    "    eval_metrics[f'{model_name}+BiLSTM-CRF'] = eval_BiLSTM(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Test Macro-F1</th>\n",
       "      <th>Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Glove+GRU</td>\n",
       "      <td>0.648467</td>\n",
       "      <td>0.916860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Glove+LSTM</td>\n",
       "      <td>0.629417</td>\n",
       "      <td>0.913100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Glove+RNN</td>\n",
       "      <td>0.653825</td>\n",
       "      <td>0.919034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Legal Bert+GRU</td>\n",
       "      <td>0.550488</td>\n",
       "      <td>0.887570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Legal Bert+LSTM</td>\n",
       "      <td>0.529526</td>\n",
       "      <td>0.882635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Legal Bert+RNN</td>\n",
       "      <td>0.551570</td>\n",
       "      <td>0.886424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Word2vec+GRU</td>\n",
       "      <td>0.657630</td>\n",
       "      <td>0.919945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Word2vec+LSTM</td>\n",
       "      <td>0.626174</td>\n",
       "      <td>0.913687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Word2vec+RNN</td>\n",
       "      <td>0.669708</td>\n",
       "      <td>0.922119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Glove+BiLSTM-CRF</td>\n",
       "      <td>0.729350</td>\n",
       "      <td>0.931931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Legal Bert+BiLSTM-CRF</td>\n",
       "      <td>0.645445</td>\n",
       "      <td>0.906637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Word2vec+BiLSTM-CRF</td>\n",
       "      <td>0.776724</td>\n",
       "      <td>0.945063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model  Test Macro-F1  Test Accuracy\n",
       "0               Glove+GRU       0.648467       0.916860\n",
       "1              Glove+LSTM       0.629417       0.913100\n",
       "2               Glove+RNN       0.653825       0.919034\n",
       "3          Legal Bert+GRU       0.550488       0.887570\n",
       "4         Legal Bert+LSTM       0.529526       0.882635\n",
       "5          Legal Bert+RNN       0.551570       0.886424\n",
       "6            Word2vec+GRU       0.657630       0.919945\n",
       "7           Word2vec+LSTM       0.626174       0.913687\n",
       "8            Word2vec+RNN       0.669708       0.922119\n",
       "9        Glove+BiLSTM-CRF       0.729350       0.931931\n",
       "10  Legal Bert+BiLSTM-CRF       0.645445       0.906637\n",
       "11    Word2vec+BiLSTM-CRF       0.776724       0.945063"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(eval_metrics).transpose().reset_index()\n",
    "df.rename(columns={'index': 'Model'}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load('Part 3/Non Trainable Embeddings/word2vec/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 949/949 [00:07<00:00, 118.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.7748816120825264\n",
      "Accuracy: 0.9438291371661918\n",
      "Macro F1: 0.647657221491045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_model.eval()\n",
    "pred = []\n",
    "true = []\n",
    "f1 = 0\n",
    "for case in tqdm(test_data):\n",
    "    sentence = prepare_sequence(test_data[case]['text'].split(' '), word_to_idx)\n",
    "    targets = prepare_sequence(test_data[case]['labels'], tag_to_ix)\n",
    "\n",
    "    # Make predictions\n",
    "    preds = best_model(sentence)[1]\n",
    "    targets = targets.numpy().tolist()\n",
    "\n",
    "    pred.extend(preds)\n",
    "    true.extend(targets)\n",
    "    f1 += f1_score(targets, preds, average='macro')\n",
    "\n",
    "print(f'Accuracy: {accuracy_score(true, pred)}')\n",
    "print(f'Macro F1: {f1_score(true, pred, average=\"macro\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
