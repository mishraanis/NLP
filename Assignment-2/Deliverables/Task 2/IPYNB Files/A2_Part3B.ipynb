{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x208d2787810>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if cuda is available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BiLSTM CRF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, target_size, embedding_mat, start_tag, end_tag, tag_to_ix, batch_size=1, device='cpu'):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.target_size = target_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.start_tag = start_tag\n",
    "        self.end_tag = end_tag\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_mat, freeze=False).to(device)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True).to(device)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size).to(device)\n",
    "\n",
    "        self.transitions_to = nn.Parameter(torch.randn(target_size, target_size)).to(device)\n",
    "        self.transitions_to.data[start_tag, :] = -10000\n",
    "        self.transitions_to.data[:, end_tag] = -10000\n",
    "\n",
    "        # self.transitions_from = nn.Parameter(torch.randn(target_size, target_size))\n",
    "        # self.transitions_from.data[:, start_tag] = -10000\n",
    "        # self.transitions_from.data[end_tag, :] = -10000\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2).to(self.device),\n",
    "                torch.randn(2, 1, self.hidden_dim // 2).to(self.device))\n",
    "\n",
    "    def get_lstm_features(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.embedding(sentence).view(len(sentence), 1, -1)\n",
    "        #convert embeds to torch float32\n",
    "        embeds = embeds.float()\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "\n",
    "\n",
    "    def _forward_algo(self, lstm_features):\n",
    "\n",
    "        scores = torch.full((1, self.target_size), -10000.).to(self.device)\n",
    "        scores[0][self.start_tag] = 0.\n",
    "\n",
    "        forward_var = scores\n",
    "\n",
    "        for feat in lstm_features:\n",
    "            next_tag_var = self.transitions_to + feat.view(-1, 1).expand(-1, self.target_size) + forward_var.expand(self.target_size, -1)\n",
    "            max_score = next_tag_var.max(dim=1).values.view(-1, 1)\n",
    "            next_tag_var = next_tag_var - max_score\n",
    "            forward_var = (max_score + torch.logsumexp(next_tag_var, dim=1).view(-1, 1)).view(1, -1)\n",
    "            \n",
    "        terminal_var = forward_var + (self.transitions_to[self.end_tag]).view(1, -1)\n",
    "        alpha = terminal_var\n",
    "        max_score = alpha.max()\n",
    "        alpha = max_score + torch.logsumexp(alpha - max_score, dim=1)\n",
    "        return alpha\n",
    "    \n",
    "\n",
    "    def _score_sentence(self, lstm_features, tags):\n",
    "        score = torch.zeros(1).to(self.device)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix['START_TAG']], dtype=torch.long).to(self.device), tags]).to(self.device)\n",
    "        for i, feat in enumerate(lstm_features):\n",
    "            score += self.transitions_to[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "                \n",
    "        score += self.transitions_to[self.tag_to_ix['END_TAG'], tags[-1]]\n",
    "        return score\n",
    "    \n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        lstm_feats = self.get_lstm_features(sentence)\n",
    "        forward_score = self._forward_algo(lstm_feats)\n",
    "        gold_score = self._score_sentence(lstm_feats, tags)\n",
    "        return forward_score - gold_score\n",
    "    \n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.target_size), -10000.).to(self.device)\n",
    "        init_vvars[0][self.start_tag] = 0\n",
    "\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = [] \n",
    "            viterbivars_t = [] \n",
    "\n",
    "            next_tag_var = self.transitions_to + forward_var.expand(self.target_size, -1)\n",
    "            best_tag_id = torch.argmax(next_tag_var, dim=1)\n",
    "            bptrs_t = best_tag_id\n",
    "            viterbivars_t = next_tag_var[range(len(best_tag_id)), best_tag_id].view(1, -1)\n",
    "            \n",
    "            forward_var = (viterbivars_t + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        \n",
    "        terminal_var = forward_var + self.transitions_to[self.end_tag]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "       \n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id.item())\n",
    "        \n",
    "        start = best_path.pop()\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "        \n",
    "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self.get_lstm_features(sentence)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = json.load(open('../Dataset/BIO_Tagged/ATE_train.json', 'r'))\n",
    "test_data = json.load(open('../Dataset/BIO_Tagged/ATE_test.json', 'r'))\n",
    "val_data = json.load(open('../Dataset/BIO_Tagged/ATE_val.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = pickle.load(open('../Utils/word_to_idx.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_to_ix = pickle.load(open('../Utils/tag_to_ix.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embedding_mat = pickle.load(open('../Extracted Word Embeddings/bert_embedding_mat.pkl', 'rb'))\n",
    "word2vec_embedding_mat = pickle.load(open('../Extracted Word Embeddings/word2vec_embedding_mat.pkl', 'rb'))\n",
    "glove_embedding_mat = pickle.load(open('../Extracted Word Embeddings/glove_embedding_mat.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 906it [00:24, 37.47it/s]?, ?it/s]\n",
      "Epoch:  10%|â–ˆ         | 1/10 [00:26<03:55, 26.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor([0.0435], grad_fn=<DivBackward0>), Val loss: tensor([0.0943])\n",
      "Train macro f1: 0.5689398223325419, Val macro f1: 0.6274407505161015\n",
      "Epoch 1/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 906it [00:21, 42.91it/s]\n",
      "Epoch:  20%|â–ˆâ–ˆ        | 2/10 [00:49<03:14, 24.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor([0.0239], grad_fn=<DivBackward0>), Val loss: tensor([0.0870])\n",
      "Train macro f1: 0.7323482430685896, Val macro f1: 0.6746126532503784\n",
      "Epoch 2/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 906it [00:21, 42.27it/s]\n",
      "Epoch:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [01:12<02:47, 23.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor([0.0182], grad_fn=<DivBackward0>), Val loss: tensor([0.0741])\n",
      "Train macro f1: 0.7912070912818194, Val macro f1: 0.7016651649523988\n",
      "Epoch 3/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 906it [00:26, 33.95it/s]\n",
      "Epoch:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:44<02:42, 27.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor([0.0159], grad_fn=<DivBackward0>), Val loss: tensor([0.0659])\n",
      "Train macro f1: 0.8329265833134578, Val macro f1: 0.7017158569333649\n",
      "Epoch 4/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 906it [00:44, 20.17it/s]\n",
      "Epoch:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [02:33<02:55, 35.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor([0.0125], grad_fn=<DivBackward0>), Val loss: tensor([0.0597])\n",
      "Train macro f1: 0.8727252829884712, Val macro f1: 0.713663576440681\n",
      "Epoch 5/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 906it [00:40, 22.45it/s]\n",
      "Epoch:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [03:16<02:30, 37.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor([0.0107], grad_fn=<DivBackward0>), Val loss: tensor([0.0626])\n",
      "Train macro f1: 0.9048778593059905, Val macro f1: 0.720734862136981\n",
      "Epoch 6/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 906it [00:22, 40.69it/s]\n",
      "Epoch:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [03:40<01:39, 33.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor([0.0077], grad_fn=<DivBackward0>), Val loss: tensor([0.0559])\n",
      "Train macro f1: 0.9284045772849862, Val macro f1: 0.7227649209679775\n",
      "Epoch 7/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 906it [00:22, 40.46it/s]\n",
      "Epoch:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [04:05<01:00, 30.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor([0.0068], grad_fn=<DivBackward0>), Val loss: tensor([0.0566])\n",
      "Train macro f1: 0.9453833173860222, Val macro f1: 0.7046250986898516\n",
      "Epoch 8/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 906it [00:25, 35.75it/s]\n",
      "Epoch:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [04:32<00:29, 29.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor([0.0055], grad_fn=<DivBackward0>), Val loss: tensor([0.0544])\n",
      "Train macro f1: 0.9640079294147783, Val macro f1: 0.7236335294969286\n",
      "Epoch 9/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 906it [00:23, 38.50it/s]\n",
      "Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [04:58<00:00, 29.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor([0.0035], grad_fn=<DivBackward0>), Val loss: tensor([0.0526])\n",
      "Train macro f1: 0.9730837339054916, Val macro f1: 0.7275297537996208\n",
      "Epoch 10/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTM_CRF(len(word_to_idx), 300, 256, len(tag_to_ix), torch.tensor(glove_embedding_mat).to(device), tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device=device).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "EPOCHS = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_macro_f1 = []\n",
    "val_macro_f1 = []\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), desc='Epoch'):\n",
    "    loss = 0\n",
    "    f1 = 0\n",
    "    for i, case in tqdm(enumerate(train_data), desc=f'Epoch {epoch + 1}/{EPOCHS}'):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        tags = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        sentence = sentence.to(device)\n",
    "        tags = tags.to(device)\n",
    "        model.train(True)\n",
    "        model.zero_grad()\n",
    "        loss = model.neg_log_likelihood(sentence, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss += loss.item()\n",
    "        with torch.no_grad():\n",
    "            preds = model(sentence)[1]\n",
    "            tags = tags.cpu().numpy()\n",
    "            f1 += f1_score(tags, preds, average='macro')\n",
    "    \n",
    "    train_loss.append(loss/len(train_data))\n",
    "    train_macro_f1.append(f1/len(train_data))\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        f1 = 0\n",
    "        for case in val_data:\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            tags = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            setence = sentence.to(device)\n",
    "            tags = tags.to(device)\n",
    "            loss = model.neg_log_likelihood(sentence, tags)\n",
    "            loss += loss.item()\n",
    "            preds = model(sentence)[1]\n",
    "            f1 += f1_score(tags.cpu().numpy(), preds, average='macro')\n",
    "        val_macro_f1.append(f1/len(val_data))\n",
    "        val_loss.append(loss/len(val_data))\n",
    "\n",
    "    print()\n",
    "    print(f'Train loss: {train_loss[-1]}, Val loss: {val_loss[-1]}')\n",
    "    print(f'Train macro f1: {train_macro_f1[-1]}, Val macro f1: {val_macro_f1[-1]}')\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'Non Trainable Embeddings/Glove/bilstm_crf_glove.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Glove/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Glove/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_macro_f1, open('Non Trainable Embeddings/Glove/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_macro_f1, open('Non Trainable Embeddings/Glove/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 906it [00:26, 34.67it/s]?, ?it/s]\n",
      "Epoch:  10%|â–ˆ         | 1/10 [00:28<04:16, 28.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor([0.0153], grad_fn=<DivBackward0>), Val loss: tensor([0.0623])\n",
      "Train macro f1: 0.5148400644199287, Val macro f1: 0.6226556215851898\n",
      "Epoch 1/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 906it [00:28, 32.33it/s]\n",
      "Epoch:  20%|â–ˆâ–ˆ        | 2/10 [00:59<03:57, 29.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor([0.0139], grad_fn=<DivBackward0>), Val loss: tensor([0.0658])\n",
      "Train macro f1: 0.7332052658651897, Val macro f1: 0.6673895721582574\n",
      "Epoch 2/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 906it [00:29, 30.70it/s]\n",
      "Epoch:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [01:31<03:36, 30.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor([0.0076], grad_fn=<DivBackward0>), Val loss: tensor([0.0615])\n",
      "Train macro f1: 0.8006913105864415, Val macro f1: 0.6909190872092661\n",
      "Epoch 3/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 906it [00:30, 29.85it/s]\n",
      "Epoch:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [02:04<03:10, 31.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor([0.0056], grad_fn=<DivBackward0>), Val loss: tensor([0.0604])\n",
      "Train macro f1: 0.8501165186298597, Val macro f1: 0.6986429420288189\n",
      "Epoch 4/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 906it [00:29, 30.29it/s]\n",
      "Epoch:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [02:38<02:41, 32.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor([0.0058], grad_fn=<DivBackward0>), Val loss: tensor([0.0547])\n",
      "Train macro f1: 0.8896861500028165, Val macro f1: 0.7081994810641663\n",
      "Epoch 5/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 906it [00:31, 28.72it/s]\n",
      "Epoch:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [03:13<02:13, 33.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor([0.0050], grad_fn=<DivBackward0>), Val loss: tensor([0.0506])\n",
      "Train macro f1: 0.9107535355449209, Val macro f1: 0.7104669302187158\n",
      "Epoch 6/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 906it [00:31, 28.93it/s]\n",
      "Epoch:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [03:47<01:41, 33.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor([0.0045], grad_fn=<DivBackward0>), Val loss: tensor([0.0569])\n",
      "Train macro f1: 0.9334208359197944, Val macro f1: 0.7206652045950666\n",
      "Epoch 7/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 906it [00:30, 29.37it/s]\n",
      "Epoch:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [04:21<01:07, 33.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor([0.0040], grad_fn=<DivBackward0>), Val loss: tensor([0.0456])\n",
      "Train macro f1: 0.9471715801605495, Val macro f1: 0.7170925207976782\n",
      "Epoch 8/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 906it [00:34, 26.45it/s]\n",
      "Epoch:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [05:00<00:35, 35.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor([0.0038], grad_fn=<DivBackward0>), Val loss: tensor([0.0407])\n",
      "Train macro f1: 0.9638492751434581, Val macro f1: 0.720509042653718\n",
      "Epoch 9/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 906it [00:40, 22.41it/s]\n",
      "Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [05:45<00:00, 34.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor([0.0042], grad_fn=<DivBackward0>), Val loss: tensor([0.0289])\n",
      "Train macro f1: 0.9703085228004333, Val macro f1: 0.7296129764487568\n",
      "Epoch 10/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTM_CRF(len(word_to_idx), 300, 256, len(tag_to_ix), torch.tensor(word2vec_embedding_mat).to(device), tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device=device).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "EPOCHS = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_macro_f1 = []\n",
    "val_macro_f1 = []\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), desc='Epoch'):\n",
    "    loss = 0\n",
    "    f1 = 0\n",
    "    for i, case in tqdm(enumerate(train_data), desc=f'Epoch {epoch + 1}/{EPOCHS}'):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        tags = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        sentence = sentence.to(device)\n",
    "        tags = tags.to(device)\n",
    "        model.train(True)\n",
    "        model.zero_grad()\n",
    "        loss = model.neg_log_likelihood(sentence, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss += loss.item()\n",
    "        with torch.no_grad():\n",
    "            preds = model(sentence)[1]\n",
    "            tags = tags.cpu().numpy()\n",
    "            f1 += f1_score(tags, preds, average='macro')\n",
    "    \n",
    "    train_loss.append(loss/len(train_data))\n",
    "    train_macro_f1.append(f1/len(train_data))\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        f1 = 0\n",
    "        for case in val_data:\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            tags = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            setence = sentence.to(device)\n",
    "            tags = tags.to(device)\n",
    "            loss = model.neg_log_likelihood(sentence, tags)\n",
    "            loss += loss.item()\n",
    "            preds = model(sentence)[1]\n",
    "            f1 += f1_score(tags.cpu().numpy(), preds, average='macro')\n",
    "        val_macro_f1.append(f1/len(val_data))\n",
    "        val_loss.append(loss/len(val_data))\n",
    "\n",
    "    print()\n",
    "    print(f'Train loss: {train_loss[-1]}, Val loss: {val_loss[-1]}')\n",
    "    print(f'Train macro f1: {train_macro_f1[-1]}, Val macro f1: {val_macro_f1[-1]}')\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'Non Trainable Embeddings/Word2vec/bilstm_crf_word2vec.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Word2vec/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Word2vec/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_macro_f1, open('Non Trainable Embeddings/Word2vec/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_macro_f1, open('Non Trainable Embeddings/Word2vec/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 906it [00:45, 19.78it/s]?, ?it/s]\n",
      "Epoch:  10%|â–ˆ         | 1/10 [00:49<07:23, 49.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor([0.0358], grad_fn=<DivBackward0>), Val loss: tensor([0.0815])\n",
      "Train macro f1: 0.36694032087374634, Val macro f1: 0.3531173110138638\n",
      "Epoch 1/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 906it [00:53, 16.94it/s]\n",
      "Epoch:  20%|â–ˆâ–ˆ        | 2/10 [01:46<07:12, 54.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor([0.0347], grad_fn=<DivBackward0>), Val loss: tensor([0.0819])\n",
      "Train macro f1: 0.40508630161526976, Val macro f1: 0.37404826273550174\n",
      "Epoch 2/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 906it [00:54, 16.64it/s]\n",
      "Epoch:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [02:44<06:31, 55.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor([0.0375], grad_fn=<DivBackward0>), Val loss: tensor([0.0845])\n",
      "Train macro f1: 0.45468653302112916, Val macro f1: 0.39203105029785335\n",
      "Epoch 3/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 906it [01:00, 15.06it/s]\n",
      "Epoch:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [03:49<05:55, 59.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor([0.0576], grad_fn=<DivBackward0>), Val loss: tensor([0.1140])\n",
      "Train macro f1: 0.49296118118421783, Val macro f1: 0.3785166813897871\n",
      "Epoch 4/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 906it [00:49, 18.34it/s]\n",
      "Epoch:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [04:42<04:45, 57.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor([0.0568], grad_fn=<DivBackward0>), Val loss: tensor([0.1334])\n",
      "Train macro f1: 0.5820106073321538, Val macro f1: 0.4038000640938741\n",
      "Epoch 5/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 906it [00:48, 18.67it/s]\n",
      "Epoch:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [05:35<03:42, 55.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor([0.0561], grad_fn=<DivBackward0>), Val loss: tensor([0.1497])\n",
      "Train macro f1: 0.6701683869915602, Val macro f1: 0.42019510032647706\n",
      "Epoch 6/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 906it [00:59, 15.21it/s]\n",
      "Epoch:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [06:38<02:54, 58.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor([0.0269], grad_fn=<DivBackward0>), Val loss: tensor([0.1098])\n",
      "Train macro f1: 0.7296074826025787, Val macro f1: 0.49898600035467616\n",
      "Epoch 7/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 906it [00:53, 16.87it/s]\n",
      "Epoch:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [07:35<01:55, 57.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor([0.0323], grad_fn=<DivBackward0>), Val loss: tensor([0.1285])\n",
      "Train macro f1: 0.7718432649084743, Val macro f1: 0.5090840071235446\n",
      "Epoch 8/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 906it [00:44, 20.55it/s]\n",
      "Epoch:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [08:22<00:54, 54.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor([0.0288], grad_fn=<DivBackward0>), Val loss: tensor([0.1391])\n",
      "Train macro f1: 0.8018852724784512, Val macro f1: 0.49414300920914944\n",
      "Epoch 9/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 906it [00:47, 18.91it/s]\n",
      "Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [09:14<00:00, 55.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor([0.0343], grad_fn=<DivBackward0>), Val loss: tensor([0.0996])\n",
      "Train macro f1: 0.8237810294876755, Val macro f1: 0.5400539278465898\n",
      "Epoch 10/10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTM_CRF(len(word_to_idx), 768, 512, len(tag_to_ix), torch.tensor(bert_embedding_mat).to(device), tag_to_ix['START_TAG'], tag_to_ix['END_TAG'], tag_to_ix, device=device).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "EPOCHS = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_macro_f1 = []\n",
    "val_macro_f1 = []\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), desc='Epoch'):\n",
    "    loss = 0\n",
    "    f1 = 0\n",
    "    for i, case in tqdm(enumerate(train_data), desc=f'Epoch {epoch + 1}/{EPOCHS}'):\n",
    "        sentence = prepare_sequence(train_data[case]['text'].split(' '), word_to_idx)\n",
    "        tags = prepare_sequence(train_data[case]['labels'], tag_to_ix)\n",
    "        sentence = sentence.to(device)\n",
    "        tags = tags.to(device)\n",
    "        model.train(True)\n",
    "        model.zero_grad()\n",
    "        loss = model.neg_log_likelihood(sentence, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss += loss.item()\n",
    "        with torch.no_grad():\n",
    "            preds = model(sentence)[1]\n",
    "            tags = tags.cpu().numpy()\n",
    "            f1 += f1_score(tags, preds, average='macro')\n",
    "    \n",
    "    train_loss.append(loss/len(train_data))\n",
    "    train_macro_f1.append(f1/len(train_data))\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        f1 = 0\n",
    "        for case in val_data:\n",
    "            sentence = prepare_sequence(val_data[case]['text'].split(' '), word_to_idx)\n",
    "            tags = prepare_sequence(val_data[case]['labels'], tag_to_ix)\n",
    "            setence = sentence.to(device)\n",
    "            tags = tags.to(device)\n",
    "            loss = model.neg_log_likelihood(sentence, tags)\n",
    "            loss += loss.item()\n",
    "            preds = model(sentence)[1]\n",
    "            f1 += f1_score(tags.cpu().numpy(), preds, average='macro')\n",
    "        val_macro_f1.append(f1/len(val_data))\n",
    "        val_loss.append(loss/len(val_data))\n",
    "\n",
    "    print()\n",
    "    print(f'Train loss: {train_loss[-1]}, Val loss: {val_loss[-1]}')\n",
    "    print(f'Train macro f1: {train_macro_f1[-1]}, Val macro f1: {val_macro_f1[-1]}')\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'Non Trainable Embeddings/Bert/bilstm_crf_bert.pt')\n",
    "pickle.dump(train_loss, open('Non Trainable Embeddings/Bert/train_loss.pkl', 'wb'))\n",
    "pickle.dump(val_loss, open('Non Trainable Embeddings/Bert/val_loss.pkl', 'wb'))\n",
    "pickle.dump(train_macro_f1, open('Non Trainable Embeddings/Bert/train_f1.pkl', 'wb'))\n",
    "pickle.dump(val_macro_f1, open('Non Trainable Embeddings/Bert/val_f1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
