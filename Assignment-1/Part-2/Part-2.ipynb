{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "C:\\Users\\Dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\pipelines\\text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from utils import emotion_scores\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLM:\n",
    "    def __init__(self):\n",
    "        self.vocab = set()\n",
    "        self.bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.unigram_counts = defaultdict(int)\n",
    "        self.bigram_probs = None\n",
    "        self.beta_values = None\n",
    "        self.emotion_dict = {'sadness': 0, 'joy': 1, 'love': 2, 'anger': 3, 'fear': 4, 'surprise': 5}\n",
    "        self.sentence_emotions = []\n",
    "        self.bigram_sentences = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    def learn_from_dataset(self, dataset):\n",
    "        for x, sentence in enumerate(dataset):\n",
    "            emotion = emotion_scores(sentence)\n",
    "            self.sentence_emotions.append(emotion)\n",
    "            tokens = sentence.split()                        \n",
    "            for i in range(len(tokens) - 1):\n",
    "                word1, word2 = tokens[i], tokens[i + 1]\n",
    "                self.vocab.add(word1)\n",
    "                self.vocab.add(word2)\n",
    "                self.bigram_counts[word1][word2] += 1\n",
    "                self.bigram_sentences[word1][word2].append(x)\n",
    "                self.unigram_counts[word1] += 1                            \n",
    "                \n",
    "        self.vocab = list(self.vocab)\n",
    "        print(f\"Vocabulary size: {len(self.vocab)}\")\n",
    "\n",
    "    def calculate_beta_values_sentence(self):  \n",
    "        num_words = len(self.vocab)\n",
    "        self.beta_values_sentence = np.zeros((num_words, num_words, 6))\n",
    "\n",
    "        for i, word1 in tqdm(enumerate(self.vocab)):\n",
    "            if word1 not in self.bigram_counts.keys():\n",
    "                continue\n",
    "            for j, word2 in enumerate(self.vocab):\n",
    "                if word2 not in self.bigram_counts[word1].keys():\n",
    "                    continue\n",
    "                self.beta_values_sentence[i][j] = np.array([np.mean([self.sentence_emotions[sentence][k]['score'] for sentence in self.bigram_sentences[word1][word2]]) for k in range(6)])\n",
    "                \n",
    "\n",
    "    def calculate_beta_values(self):\n",
    "            num_words = len(self.vocab)\n",
    "            self.beta_values = np.zeros((num_words, num_words, 6))\n",
    "            for i, word1 in tqdm(enumerate(self.vocab)):\n",
    "                if word1 not in self.bigram_counts.keys():\n",
    "                    continue\n",
    "                for j, word2 in enumerate(self.vocab):\n",
    "                    if word2 not in self.bigram_counts[word1].keys():\n",
    "                        continue\n",
    "                    emotions = emotion_scores(word1 + \" \" + word2)\n",
    "                    self.beta_values[i][j] = np.array([emotions[k]['score'] for k in range(6)])\n",
    "                    \n",
    "    def calculate_bigram_probs(self):\n",
    "        num_words = len(self.vocab)\n",
    "        self.bigram_probs = np.zeros((num_words, num_words))        \n",
    "        \n",
    "        for i, word1 in tqdm(enumerate(self.vocab)):\n",
    "            for j, word2 in enumerate(self.vocab):\n",
    "                if self.unigram_counts[word1] > 0:\n",
    "                    self.bigram_probs[i][j] = float(self.bigram_counts[word1][word2]) / float(self.unigram_counts[word1])\n",
    "\n",
    "                    \n",
    "    def calculate_bigram_probs_laplace(self):\n",
    "        num_words = len(self.vocab)\n",
    "        self.bigram_probs = np.zeros((num_words, num_words))\n",
    "\n",
    "        for i, word1 in enumerate(self.vocab):\n",
    "            for j, word2 in enumerate(self.vocab):\n",
    "                self.bigram_probs[i][j] = (self.bigram_counts[word1][word2] + 1) / (self.unigram_counts[word1] + num_words)\n",
    "                \n",
    "    \n",
    "    def calculate_bigram_probs_kneser_ney(self, discount=0.75):\n",
    "        num_words = len(self.vocab)\n",
    "        self.bigram_probs = np.zeros((num_words, num_words))\n",
    "\n",
    "        continuation_counts = defaultdict(set)\n",
    "   \n",
    "        for word1, word2_dict in self.bigram_counts.items():\n",
    "            for word2 in word2_dict:\n",
    "                continuation_counts[word2].add(word1)\n",
    "\n",
    "        total_continuations = {word2: len(word1s) for word2, word1s in continuation_counts.items()}\n",
    "\n",
    "        for i, word1 in tqdm(enumerate(self.vocab)):\n",
    "            sum_adjusted_counts = sum(max(self.bigram_counts[word1][word2] - discount, 0) for word2 in self.vocab)\n",
    "            for j, word2 in enumerate(self.vocab):\n",
    "                if word2 in total_continuations.keys():\n",
    "                    adjusted_count = max(self.bigram_counts[word1][word2] - discount, 0)\n",
    "                    continuation_prob = total_continuations[word2] / sum(total_continuations.values()) if sum(total_continuations.values()) > 0 else 0\n",
    "                    lower_order_weight = (discount * continuation_prob) / self.unigram_counts[word1] if self.unigram_counts[word1] > 0 else 0\n",
    "\n",
    "                    self.bigram_probs[i][j] = adjusted_count / self.unigram_counts[word1] + lower_order_weight if self.unigram_counts[word1] > 0 else 0\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('../Dataset/corpus.txt')\n",
    "dataset = []\n",
    "for i in corpus.readlines():\n",
    "    dataset.append('<SOS> ' + i + ' <EOS>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5431\n"
     ]
    }
   ],
   "source": [
    "model = BigramLM()\n",
    "model.learn_from_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model.vocab, open('Checkpoints/vocab.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5431it [00:06, 890.43it/s] \n"
     ]
    }
   ],
   "source": [
    "model.calculate_beta_values_sentence()\n",
    "pickle.dump(model.beta_values_sentence, open('Checkpoints/beta_values_sentence.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5431it [08:38, 10.47it/s]\n"
     ]
    }
   ],
   "source": [
    "model.calculate_beta_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model.beta_values, open('Checkpoints/beta_values.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5431it [00:16, 321.77it/s]\n"
     ]
    }
   ],
   "source": [
    "model.calculate_bigram_probs()\n",
    "pickle.dump(model.bigram_probs, open('Checkpoints/bigram_probs.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.calculate_bigram_probs_kneser_ney()\n",
    "pickle.dump(model.bigram_probs, open('Checkpoints/bigram_probs_kneser.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.calculate_bigram_probs_laplace()\n",
    "pickle.dump(model.bigram_probs, open('Checkpoints/bigram_probs_laplace.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
