{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical \n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import json\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from keras.callbacks import Callback\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = json.load(open('../Dataset/BIO_Tagged/ATE_train.json'))\n",
    "val_data = json.load(open('../Dataset/BIO_Tagged/ATE_val.json'))\n",
    "test_data = json.load(open('../Dataset/BIO_Tagged/ATE_test.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = pickle.load(open('Word Embeddings/word2vec.pkl', 'rb'))\n",
    "glove = pickle.load(open('Word Embeddings/glove.pkl', 'rb'))\n",
    "bert = pickle.load(open('Word Embeddings/bert.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_to_idx = {}\n",
    "\n",
    "# for case in train_data:\n",
    "#     for text in train_data[case]['text'].split(' '):\n",
    "#         if text not in word_to_idx:\n",
    "#             word_to_idx[text] = len(word_to_idx)\n",
    "\n",
    "# for case in test_data:\n",
    "#     for text in test_data[case]['text'].split(' '):\n",
    "#         if text not in word_to_idx:\n",
    "#             word_to_idx[text] = len(word_to_idx)\n",
    "\n",
    "# for case in val_data:\n",
    "#     for text in val_data[case]['text'].split(' '):\n",
    "#         if text not in word_to_idx:\n",
    "#             word_to_idx[text] = len(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag_to_ix = {}\n",
    "\n",
    "# for case in train_data:\n",
    "#     for tag in train_data[case]['labels']:\n",
    "#         if tag not in tag_to_ix:\n",
    "#             tag_to_ix[tag] = len(tag_to_ix)\n",
    "\n",
    "# for case in test_data:\n",
    "#     for tag in test_data[case]['labels']:\n",
    "#         if tag not in tag_to_ix:\n",
    "#             tag_to_ix[tag] = len(tag_to_ix)\n",
    "\n",
    "# for case in val_data:\n",
    "#     for tag in val_data[case]['labels']:\n",
    "#         if tag not in tag_to_ix:\n",
    "#             tag_to_ix[tag] = len(tag_to_ix)\n",
    "\n",
    "# tag_to_ix['START_TAG'] = len(tag_to_ix)\n",
    "# tag_to_ix['END_TAG'] = len(tag_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_mat = np.zeros((len(word_to_idx), 768))\n",
    "\n",
    "# for word, idx in tqdm(word_to_idx.items()):\n",
    "#     try:\n",
    "#         tokens = tokenizer.batch_encode_plus([word], return_tensors='pt', add_special_tokens=False)\n",
    "#     except:\n",
    "#         tokens = tokenizer.batch_encode_plus(['unk'], return_tensors='pt', add_special_tokens=False)\n",
    "#         continue\n",
    "#     embeddings = None\n",
    "#     with torch.no_grad():\n",
    "#         try:\n",
    "#             outputs = model(**tokens)\n",
    "#             embeddings = outputs.last_hidden_state\n",
    "#         except:\n",
    "#             tokens = tokenizer.batch_encode_plus(['unk'], return_tensors='pt', add_special_tokens=False)\n",
    "#             outputs = model(**tokens)\n",
    "#             embeddings = outputs.last_hidden_state\n",
    "#     embeddings = embeddings.squeeze(0)\n",
    "#     word_embeddings = embeddings.mean(dim = 0)\n",
    "#     embedding_mat[idx] = word_embeddings.squeeze(0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(embedding_mat, open('Word Embeddings/bert.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_embedding_matrix(model, word_index, embedding_dim):\n",
    "#     embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "#     for word, i in word_index.items():\n",
    "#         if word in model.wv:\n",
    "#             embedding_matrix[i] = model.wv[word]\n",
    "#         # Else, you can choose to initialize randomly or use a special token here\n",
    "#     return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(train_data, val_data, test_data, embedding_type):\n",
    "    # Combine all datasets for consistent tokenization and vocabulary creation\n",
    "    all_data = {**train_data, **val_data, **test_data}\n",
    "        \n",
    "    # Extract texts and labels from the combined data\n",
    "    texts = [item[\"text\"] for item in all_data.values()]\n",
    "    labels = [item[\"labels\"] for item in all_data.values()]\n",
    "    \n",
    "    word_tokenizer = Tokenizer()                      \n",
    "    word_tokenizer.fit_on_texts(texts)                    \n",
    "    \n",
    "    train_sequences = word_tokenizer.texts_to_sequences([item[\"text\"] for item in train_data.values()])\n",
    "    val_sequences = word_tokenizer.texts_to_sequences([item[\"text\"] for item in val_data.values()])\n",
    "    test_sequences = word_tokenizer.texts_to_sequences([item[\"text\"] for item in test_data.values()])\n",
    " \n",
    "    max_sequence_length = 50 \n",
    "    vocab_size = len(word_tokenizer.word_index) + 1  # Adding 1 for the zero-padding\n",
    "\n",
    "    X_train = pad_sequences(train_sequences, maxlen=max_sequence_length)\n",
    "    X_val = pad_sequences(val_sequences, maxlen=max_sequence_length)\n",
    "    X_test = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "    tag_tokenizer = Tokenizer()\n",
    "    tag_tokenizer.fit_on_texts(labels)\n",
    "\n",
    "    train_labels = tag_tokenizer.texts_to_sequences([item[\"labels\"] for item in train_data.values()])\n",
    "    val_labels = tag_tokenizer.texts_to_sequences([item[\"labels\"] for item in val_data.values()])\n",
    "    test_labels = tag_tokenizer.texts_to_sequences([item[\"labels\"] for item in test_data.values()])\n",
    "\n",
    "    max_label_length = 50    \n",
    "    num_classes = len(tag_tokenizer.word_index) + 1  # Adding 1 for the zero-padding\n",
    "\n",
    "    Y_train = pad_sequences(train_labels, maxlen=max_label_length)\n",
    "    Y_val = pad_sequences(val_labels, maxlen=max_label_length)\n",
    "    Y_test = pad_sequences(test_labels, maxlen=max_label_length)    \n",
    "\n",
    "    try:\n",
    "        embedding_dim = embedding_type.vector_size\n",
    "    except AttributeError:\n",
    "        embedding_dim = 768\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_tokenizer.word_index) + 1, embedding_dim))\n",
    "    word2id = word_tokenizer.word_index\n",
    "\n",
    "    for word, index in word2id.items():\n",
    "        try:\n",
    "            embedding_matrix[index, :] = embedding_type[word]\n",
    "        except IndexError:\n",
    "            embedding_matrix[index, :] = embedding_type[index]\n",
    "        except KeyError:\n",
    "            pass       \n",
    "\n",
    "    Y_train = to_categorical(Y_train)\n",
    "    Y_val = to_categorical(Y_val)\n",
    "    Y_test = to_categorical(Y_test)    \n",
    "\n",
    "    return X_train, X_val, X_test, Y_train, Y_val, Y_test, vocab_size, embedding_dim, max_sequence_length, num_classes, embedding_matrix, word_tokenizer, tag_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['word2vec'] = {}\n",
    "data['glove'] = {}\n",
    "data['bert'] = {}\n",
    "\n",
    "data['word2vec']['X_train'], data['word2vec']['X_val'], data['word2vec']['X_test'], data['word2vec']['Y_train'], data['word2vec']['Y_val'], data['word2vec']['Y_test'], data['word2vec']['vocab_size'], data['word2vec']['embedding_dim'], data['word2vec']['max_sequence_length'], data['word2vec']['num_classes'], data['word2vec']['embedding_matrix'], word_tokenizer, tag_tokenizer = prepareData(train_data, val_data, test_data, word2vec)\n",
    "data['glove']['X_train'], data['glove']['X_val'], data['glove']['X_test'], data['glove']['Y_train'], data['glove']['Y_val'], data['glove']['Y_test'], data['glove']['vocab_size'], data['glove']['embedding_dim'], data['glove']['max_sequence_length'], data['glove']['num_classes'], data['glove']['embedding_matrix'], word_tokenizer, tag_tokenizer = prepareData(train_data, val_data, test_data, glove)\n",
    "data['bert']['X_train'], data['bert']['X_val'], data['bert']['X_test'], data['bert']['Y_train'], data['bert']['Y_val'], data['bert']['Y_test'], data['bert']['vocab_size'], data['bert']['embedding_dim'], data['bert']['max_sequence_length'], data['bert']['num_classes'], data['bert']['embedding_matrix'], word_tokenizer, tag_tokenizer = prepareData(train_data, val_data, test_data, bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MacroF1ScoreCallback(Callback):\n",
    "    def __init__(self, train_data, val_data):\n",
    "        super().__init__()\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.train_f1s = []\n",
    "        self.val_f1s = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        train_pred = np.argmax(self.model.predict(self.train_data[0]), axis=-1)\n",
    "        train_true = np.argmax(self.train_data[1], axis=-1)\n",
    "        train_f1 = f1_score(train_true.flatten(), train_pred.flatten(), average='macro')\n",
    "\n",
    "        val_pred = np.argmax(self.model.predict(self.val_data[0]), axis=-1)\n",
    "        val_true = np.argmax(self.val_data[1], axis=-1)\n",
    "        val_f1 = f1_score(val_true.flatten(), val_pred.flatten(), average='macro')\n",
    "\n",
    "        self.train_f1s.append(train_f1)\n",
    "        self.val_f1s.append(val_f1)\n",
    "        print(f'Epoch {epoch + 1} - Train Macro-F1: {train_f1:.4f} - Val Macro-F1: {val_f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VANILLA RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainRNN(embedding):\n",
    "    X_train, Y_train, X_val, Y_val, embedding_matrix = data[embedding]['X_train'], data[embedding]['Y_train'], data[embedding]['X_val'], data[embedding]['Y_val'], data[embedding]['embedding_matrix']\n",
    "    \n",
    "    NUM_CLASSES = data[embedding]['num_classes']\n",
    "    VOCABULARY_SIZE = data[embedding]['vocab_size']\n",
    "    EMBEDDING_SIZE = data[embedding]['embedding_dim']\n",
    "    MAX_SEQUENCE_LENGTH = data[embedding]['max_sequence_length']\n",
    "\n",
    "    rnn_model = Sequential()\n",
    "    rnn_model.add(Embedding(input_dim=VOCABULARY_SIZE,\n",
    "                            output_dim=EMBEDDING_SIZE,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=True))\n",
    "\n",
    "    rnn_model.add(SimpleRNN(64, return_sequences=True))\n",
    "\n",
    "    rnn_model.add(TimeDistributed(Dense(NUM_CLASSES, activation='softmax')))\n",
    "\n",
    "    rnn_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['acc'])\n",
    "\n",
    "    macro_f1_callback = MacroF1ScoreCallback(train_data=(X_train, Y_train), val_data=(X_val, Y_val))\n",
    "\n",
    "    macro_f1_scores = rnn_model.fit(X_train, Y_train, batch_size=128, epochs=50, validation_data=(X_val, Y_val), callbacks=[macro_f1_callback])\n",
    "\n",
    "    rnn_model.save(f'Models/t2_rnn_{embedding}.h5')\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(f'Loss - {embedding} RNN')\n",
    "    plt.plot(macro_f1_scores.history['loss'], label='Training Loss')\n",
    "    plt.plot(macro_f1_scores.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(f'Macro F1 - {embedding} RNN')\n",
    "    plt.plot(macro_f1_callback.train_f1s, label='Training Macro-F1')\n",
    "    plt.plot(macro_f1_callback.val_f1s, label='Validation Macro-F1')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Macro-F1 Score')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRNN('word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRNN('glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRNN('bert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLSTM(embedding):\n",
    "    X_train, Y_train, X_val, Y_val, embedding_matrix = data[embedding]['X_train'], data[embedding]['Y_train'], data[embedding]['X_val'], data[embedding]['Y_val'], data[embedding]['embedding_matrix']\n",
    "    \n",
    "    NUM_CLASSES = data[embedding]['num_classes']\n",
    "    VOCABULARY_SIZE = data[embedding]['vocab_size']\n",
    "    EMBEDDING_SIZE = data[embedding]['embedding_dim']\n",
    "    MAX_SEQUENCE_LENGTH = data[embedding]['max_sequence_length']\n",
    "\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(Embedding(input_dim=VOCABULARY_SIZE,\n",
    "                            output_dim=EMBEDDING_SIZE,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=True))\n",
    "\n",
    "    lstm_model.add(LSTM(64, return_sequences=True))\n",
    "\n",
    "    lstm_model.add(TimeDistributed(Dense(NUM_CLASSES, activation='softmax')))\n",
    "\n",
    "    lstm_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['acc'])\n",
    "\n",
    "    macro_f1_callback = MacroF1ScoreCallback(train_data=(X_train, Y_train), val_data=(X_val, Y_val))\n",
    "\n",
    "    macro_f1_scores = lstm_model.fit(X_train, Y_train, batch_size=128, epochs=50, validation_data=(X_val, Y_val), callbacks=[macro_f1_callback])\n",
    "\n",
    "    lstm_model.save(f'Models/t2_lstm_{embedding}.h5')\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(f'Loss - {embedding} LSTM')\n",
    "    plt.plot(macro_f1_scores.history['loss'], label='Training Loss')\n",
    "    plt.plot(macro_f1_scores.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(f'Macro F1 - {embedding} LSTM')\n",
    "    plt.plot(macro_f1_callback.train_f1s, label='Training Macro-F1')\n",
    "    plt.plot(macro_f1_callback.val_f1s, label='Validation Macro-F1')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Macro-F1 Score')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLSTM('word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLSTM('glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLSTM('bert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGRU(embedding):\n",
    "    X_train, Y_train, X_val, Y_val, embedding_matrix = data[embedding]['X_train'], data[embedding]['Y_train'], data[embedding]['X_val'], data[embedding]['Y_val'], data[embedding]['embedding_matrix']\n",
    "    \n",
    "    NUM_CLASSES = data[embedding]['num_classes']\n",
    "    VOCABULARY_SIZE = data[embedding]['vocab_size']\n",
    "    EMBEDDING_SIZE = data[embedding]['embedding_dim']\n",
    "    MAX_SEQUENCE_LENGTH = data[embedding]['max_sequence_length']\n",
    "\n",
    "    gru_model = Sequential()\n",
    "    gru_model.add(Embedding(input_dim=VOCABULARY_SIZE,\n",
    "                            output_dim=EMBEDDING_SIZE,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=True))\n",
    "\n",
    "    gru_model.add(GRU(64, return_sequences=True))\n",
    "\n",
    "    gru_model.add(TimeDistributed(Dense(NUM_CLASSES, activation='softmax')))\n",
    "\n",
    "    gru_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['acc'])\n",
    "\n",
    "    macro_f1_callback = MacroF1ScoreCallback(train_data=(X_train, Y_train), val_data=(X_val, Y_val))\n",
    "\n",
    "    macro_f1_scores = gru_model.fit(X_train, Y_train, batch_size=128, epochs=50, validation_data=(X_val, Y_val), callbacks=[macro_f1_callback])\n",
    "\n",
    "    gru_model.save(f'Models/t2_gru_{embedding}.h5')\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(f'Loss - {embedding} GRU')\n",
    "    plt.plot(macro_f1_scores.history['loss'], label='Training Loss')\n",
    "    plt.plot(macro_f1_scores.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(f'Macro F1 - {embedding} GRU')\n",
    "    plt.plot(macro_f1_callback.train_f1s, label='Training Macro-F1')\n",
    "    plt.plot(macro_f1_callback.val_f1s, label='Validation Macro-F1')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Macro-F1 Score')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGRU('word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGRU('glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGRU('bert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aspect_terms(test_pred, X_test):\n",
    "    aspect_terms = []\n",
    "    for i in range(len(test_pred)):\n",
    "        aspect_term = []\n",
    "        for j in range(len(test_pred[i])):\n",
    "            if test_pred[i][j] == 1:\n",
    "                aspect_term.append(word_tokenizer.index_word[X_test[i][j]])\n",
    "        aspect_terms.append(aspect_term)\n",
    "    return aspect_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, Y_test):\n",
    "    Y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "    Y_true = np.argmax(Y_test, axis=-1)\n",
    "\n",
    "    aspect_terms = get_aspect_terms(Y_pred, X_test)\n",
    "\n",
    "    print(f'Macro F1 Score: {f1_score(Y_true.flatten(), Y_pred.flatten(), average=\"macro\"):.4f}')\n",
    "    print(f'Accuracy: {accuracy_score(Y_true.flatten(), Y_pred.flatten())*100:.2f}%')\n",
    "\n",
    "    return aspect_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_word2vec = load_model('Models/t2_rnn_word2vec.h5')\n",
    "rnn_glove = load_model('Models/t2_rnn_glove.h5')\n",
    "rnn_bert = load_model('Models/t2_rnn_bert.h5')\n",
    "\n",
    "lstm_word2vec = load_model('Models/t2_lstm_word2vec.h5')\n",
    "lstm_glove = load_model('Models/t2_lstm_glove.h5')\n",
    "lstm_bert = load_model('Models/t2_lstm_bert.h5')\n",
    "\n",
    "gru_word2vec = load_model('Models/t2_gru_word2vec.h5')\n",
    "gru_glove = load_model('Models/t2_gru_glove.h5')\n",
    "gru_bert = load_model('Models/t2_gru_bert.h5')\n",
    "\n",
    "\n",
    "print(\"RNN Word2Vec\")\n",
    "rnn_word2vec_aspect_terms = evaluate_model(rnn_word2vec, data['word2vec']['X_test'], data['word2vec']['Y_test'])\n",
    "\n",
    "print()\n",
    "print(\"RNN Glove\")\n",
    "rnn_glove_aspect_terms = evaluate_model(rnn_glove, data['glove']['X_test'], data['glove']['Y_test'])\n",
    "\n",
    "print()\n",
    "print(\"RNN bert\")\n",
    "rnn_bert_aspect_terms = evaluate_model(rnn_bert, data['bert']['X_test'], data['bert']['Y_test'])\n",
    "\n",
    "print()\n",
    "print(\"LSTM Word2Vec\")\n",
    "lstm_word2vec_aspect_terms = evaluate_model(lstm_word2vec, data['word2vec']['X_test'], data['word2vec']['Y_test'])\n",
    "\n",
    "print()\n",
    "print(\"LSTM Glove\")\n",
    "lstm_glove_aspect_terms = evaluate_model(lstm_glove, data['glove']['X_test'], data['glove']['Y_test'])\n",
    "\n",
    "print()\n",
    "print(\"LSTM bert\")\n",
    "lstm_bert_aspect_terms = evaluate_model(lstm_bert, data['bert']['X_test'], data['bert']['Y_test'])\n",
    "\n",
    "print()\n",
    "print(\"GRU Word2Vec\")\n",
    "gru_word2vec_aspect_terms = evaluate_model(gru_word2vec, data['word2vec']['X_test'], data['word2vec']['Y_test'])\n",
    "\n",
    "print()\n",
    "print(\"GRU Glove\")\n",
    "gru_glove_aspect_terms = evaluate_model(gru_glove, data['glove']['X_test'], data['glove']['Y_test'])\n",
    "\n",
    "print()\n",
    "print(\"GRU bert\")\n",
    "gru_bert_aspect_terms = evaluate_model(gru_bert, data['bert']['X_test'], data['bert']['Y_test'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
